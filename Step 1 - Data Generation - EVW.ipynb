{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this variable yourself.\n",
    "running_on_colab = False\n",
    "# Store data as reduced density matrix `rho` or eigenvector tuple `EVW`.\n",
    "rho_or_EVW = 'EVW'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning of Many Body Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Use exact diagonalization to obtain a few eigenstates near energy $E = 0$ from the Heisenberg model with a\n",
    "random field, \n",
    "\n",
    "\\begin{equation}\n",
    "    H = J \\sum_i \\vec{S}_{i} \\cdot \\vec{S}_{i+1} - \\sum_i h_i S^z_i\n",
    "\\end{equation}\n",
    "\n",
    ", where the values of the field $ h_i \\in [-W, W] $ are chosen from a uniform random distribution with a \"disorder strength\" $W$ (with moderate system sizes $L \\approx 12$). \n",
    "\n",
    "The exciting property of this model is that it is believed to undergo a phase transition from an extended phase (small $W$) to a localized phase (large $W$). \n",
    "\n",
    "We will use ML to detect this transition: Pick a number of eigenstates that are near energy $E = 0$ and obtain the reduced density matrices $\\rho^A$, where $A$ is a region of $n$ consecutive spins (a few hundred to thousands eigenstates for different disorder realizations). \n",
    "\n",
    "Now use the density matrices for $W = 0.5 J$ and $W = 8.0 J$ to train a neural network (just interpret the entries of $\\rho^A$ as an image with $2^n \\times 2^n$ pixel). \n",
    "Then use this network and study the output of the neural network for different $W$. \n",
    "\n",
    "How does the results depend on system size $L$ and block size $n$? \n",
    "At which $W_c$ do you expect the transition to occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Wikipedia:  \n",
    "[Many body localization](https://en.wikipedia.org/wiki/Many_body_localization)  \n",
    "[Localization protected quantum order](https://en.wikipedia.org/wiki/Localization_protected_quantum_order)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Many-body localization (MBL) is a dynamical phenomenon which leads to the breakdown of equilibrium statistical mechanics in isolated many-body systems. Such systems never reach local thermal equilibrium, and retain local memory of their initial conditions for infinite times.\n",
    "\n",
    "MBL was first proposed by P.W. Anderson in 1958 as a possibility that could arise in strongly disordered quantum systems. The basic idea was that if particles all live in a random energy landscape, then any rearrangement of particles would change the energy of the system. Since energy is a conserved quantity in quantum mechanics, such a process can only be virtual and cannot lead to any transport of particle number or energy.  \n",
    "\n",
    "The process of thermalization erases local memory of the initial conditions. In textbooks, thermalization is ensured by coupling the system to an external environment or \"reservoir,\" with which the system can exchange energy. What happens if the system is isolated from the environment, and evolves according to its own Schrödinger equation? Does the system still thermalize?\n",
    "\n",
    "Quantum mechanical time evolution is unitary and formally preserves all information about the initial condition in the quantum state at all times.\n",
    "\n",
    "This question can be formalized by considering the quantum mechanical density matrix ρ of the system. If the system is divided into a subregion A (the region being probed) and its complement B (everything else), then all information that can be extracted by measurements made on A alone is encoded in the reduced density matrix $\\rho_A = Tr_B (\\rho(t))$. If in the long time limit $\\rho_A(t)$ approaches a thermal density matrix at a temperature set by the energy density in the state, then the system has \"thermalized,\" and no local information about the initial condition can be extracted from local measurements. This process of \"quantum thermalization\" may be understood in terms of B acting as a reservoir for A. In this perspective, the entanglement entropy $ S = - Tr \\rho_A log \\rho_A $ of a thermalizing system in a pure state plays the role of thermal entropy. Thermalizing systems therefore generically have extensive or \"volume law\" entanglement entropy at any non-zero temperature.\n",
    "\n",
    "In contrast, if $\\rho_A(t)$ fails to approach a thermal density matrix even in the long time limit, and remains instead close to its initial condition $\\rho_A(0)$, then the system retains forever a memory of its initial condition in local observables. This latter possibility is referred to as \"many body localization,\" and involves B failing to act as a reservoir for A. Eigenstates of systems exhibiting MBL do not obey the ETH, and generically follow an \"area law\" for entanglement entropy (i.e. the entanglement entropy scales with the surface area of subregion A).\n",
    "\n",
    "In thermalizing systems, energy eigenstates have volume law entanglement entropy. In MBL systems, energy eigenstates have area law entanglement entropy.\n",
    "\n",
    "In thermalizing systems, entanglement entropy grows as a power law in time starting from low entanglement initial conditions. In MBL systems, entanglement entropy grows logarithmically in time starting from low entanglement initial conditions.\n",
    "\n",
    "In thermalizing systems, the dynamics of out-of-time-ordered correlators forms a linear light cone which reflects the ballistic propagation of information. In MBL systems, the light cone is logarithmic.\n",
    "\n",
    "What's more, while individual eigenstates aren't themselves experimentally accessible, order in eigenstates nevertheless has measurable dynamical signatures. The eigenspectrum properties change in a singular fashion as the system transitions between from one type of MBL phase to another, or from an MBL phase to a thermal one---again with measurable dynamical signatures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['running_on_colab'] = str(running_on_colab)\n",
    "# running_on_colab = (os.getenv('running_on_colab', 'False') == 'True')\n",
    "\n",
    "if running_on_colab:\n",
    "    data_root             = 'drive/MyDrive/Colab Data/MBL/'\n",
    "    sys.path.append(data_root)\n",
    "else:\n",
    "    data_root             = './'\n",
    "\n",
    "# Store data as reduced density matrix `rho` or eigenvector tuple `EVW`.\n",
    "os.environ['rho_or_EVW'] = str(rho_or_EVW)\n",
    "# running_on_colab = (os.getenv('rho_or_EVW', 'EVW') == 'rho')\n",
    "\n",
    "from file_io import *\n",
    "from data_gen import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "dpi = 100\n",
    "fig_w = 1280\n",
    "fig_h = 640\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if running_on_colab:\n",
    "    !cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if running_on_colab:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if running_on_colab:\n",
    "    !pip install pytorch_lightning==0.7.6 torchsummary==1.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Test execution time of Exact Diagonalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Test solving a single Hamiltonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "L = 10\n",
    "W = 0.5\n",
    "J = 1\n",
    "periodic = False\n",
    "num_Hs = 10\n",
    "\n",
    "H = build_H(L, W, J, periodic)\n",
    "E, V = ED(H.toarray())\n",
    "print('2^L: {}'.format(2**L))\n",
    "print('#eigenvalues: {}'.format(len(E)))\n",
    "print('E.shape: {}'.format(E.shape))\n",
    "print('V.shape: {}'.format(V.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Solve using numpy's dense solver\n",
    "E, V = ED(H.toarray())\n",
    "E0, V0 = select_N_eigenvalues(E, V, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Solve using scipy's sparse solver instead.\n",
    "E1, V1 = ED_sparse(H, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Solve using numpy's dense solver\n",
    "E, V = ED(H.toarray())\n",
    "E0, V0 = select_N_eigenvalues(E, V, 20)\n",
    "\n",
    "# Solve using scipy's sparse solver instead.\n",
    "E1, V1 = ED_sparse(H, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check eigenvalues are sorted correctly, and eigenvectors are selected along the correct axis.\n",
    "V0_norm = []\n",
    "V1_norm = []\n",
    "for i in range(len(E0)):\n",
    "    V0_norm.append(np.linalg.norm(V0[:,i]))\n",
    "    V1_norm.append(np.linalg.norm(V1[:,i]))\n",
    "\n",
    "for E0i, E1i, V0i, V1i in zip(E0, E1, V0_norm, V1_norm):\n",
    "    print('Numpy: {:+.12f} | Numpy norm: {:.16f}'.format(E0i, V0i))\n",
    "    print('Scipy: {:+.12f} | Scipy norm: {:.16f}'.format(E1i, V1i))\n",
    "\n",
    "assert np.allclose(V0i, V1i), 'Eigenvectors evaluated using Numpy and Scipy should be identical.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Test solving multiple Hamiltonians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Hs = build_Hs(L, W, J, periodic, num_Hs)\n",
    "Es, Vs = EDs(Hs)\n",
    "E0s, V0s = EDs_sparse(Hs, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Test execution time, numpy vs scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test execution time: numpy.\n",
    "J  = 1                      # Always = 1\n",
    "Ws = [8]                    # Disorder strength W.\n",
    "Ls = list(range(8,12))      # System size L.\n",
    "ns = [1]*len(Ls)            # Number of samples for each L.\n",
    "ps = [False]                # Periodic or not.\n",
    "et = []                     # Execution time.\n",
    "num_EV = 20                 # Number of eigenvalues near zero to save.\n",
    "\n",
    "for L, num_Hs in zip(Ls, ns):\n",
    "    for W in Ws:\n",
    "        for p in ps:\n",
    "            start_time = time.time()\n",
    "            batch_generate_ED_data(L, W, J, p, num_Hs, num_EV, save_data=False, npsp='np')\n",
    "            exec_time = time.time() - start_time\n",
    "            et.append(exec_time)\n",
    "            print('Computed: L={:02d} | W={:.2f} | periodic={: <5}. Execution took {: 8.2f}s or {: 6.2f}min'.format(L, W, str(p), exec_time, exec_time/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(fig_w/dpi,fig_h/dpi), dpi=dpi, squeeze=False)\n",
    "\n",
    "base = 10\n",
    "axes[0,0].plot(Ls, et)\n",
    "axes[0,1].plot(np.array(Ls), np.log(et) / np.log(base))\n",
    "\n",
    "axes[0,0].set_title('Numpy Execution time vs System size')\n",
    "axes[0,1].set_title('Log(Numpy Execution time) vs System size')\n",
    "axes[0,0].set_ylabel('Execution time')\n",
    "axes[0,1].set_ylabel('Log(Execution time)')\n",
    "\n",
    "for axe in axes:\n",
    "    for ax in axe:\n",
    "        ax.set_xlabel('System size L')\n",
    "        # ax.legend(loc='best')\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test execution time: scipy.\n",
    "J  = 1                      # Always = 1\n",
    "Ws = [8]                    # Disorder strength W.\n",
    "Ls = list(range(8,16))      # System size L.\n",
    "ns = [1]*len(Ls)            # Number of samples for each L.\n",
    "ps = [False]                # Periodic or not.\n",
    "et = []                     # Execution time.\n",
    "num_EV = 20                 # Number of eigenvalues near zero to save.\n",
    "\n",
    "for L, num_Hs in zip(Ls, ns):\n",
    "    for W in Ws:\n",
    "        for p in ps:\n",
    "            start_time = time.time()\n",
    "            batch_generate_ED_data(L, W, J, p, num_Hs, num_EV, save_data=False, npsp='sp')\n",
    "            exec_time = time.time() - start_time\n",
    "            et.append(exec_time)\n",
    "            print('Computed: L={:02d} | W={:.2f} | periodic={: <5}. Execution took {: 8.2f}s or {: 6.2f}min'.format(L, W, str(p), exec_time, exec_time/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(fig_w/dpi,fig_h/dpi), dpi=dpi, squeeze=False)\n",
    "\n",
    "base = 10\n",
    "axes[0,0].plot(Ls, et)\n",
    "axes[0,1].plot(np.array(Ls), np.log(et) / np.log(base))\n",
    "\n",
    "axes[0,0].set_title('SciPy Execution time vs System size')\n",
    "axes[0,1].set_title('Log(SciPy Execution time) vs System size')\n",
    "axes[0,0].set_ylabel('Execution time')\n",
    "axes[0,1].set_ylabel('Log(Execution time)')\n",
    "\n",
    "for axe in axes:\n",
    "    for ax in axe:\n",
    "        ax.set_xlabel('System size L')\n",
    "        # ax.legend(loc='best')\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Sample parameters.\n",
    "J  = 1                                 # Always = 1\n",
    "Ws = [0.5, 8] * 10 + [i/2 for i in range(1*2, 10*2)] # Disorder strength W.\n",
    "Ls = [   8,   9,  10,  11, 12, 13, 14] # System size L.\n",
    "ns = [1000, 500, 250, 100, 50, 20, 10] # Number of samples for each L.\n",
    "ps = [False, True]                     # Periodic or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test file size. It's not feasible to store ALL eigenvectors.\n",
    "J  = 1             # Always = 1\n",
    "Ws = [0.5, 8] * 10 + [i/2 for i in range(1*2, 10*2)] # Disorder strength W.\n",
    "Ls = [   8]        # System size L.\n",
    "ns = [1000]        # Number of samples for each L.\n",
    "ps = [False, True] # Periodic or not.\n",
    "\n",
    "fs = 0\n",
    "for L, num_Hs in zip(Ls, ns):\n",
    "    for W in Ws:\n",
    "        for p in ps:\n",
    "            start_time = time.time()\n",
    "            # batch_generate_ED_data(L, W, J, p, num_Hs)\n",
    "            fs += 1\n",
    "            exec_time = time.time() - start_time\n",
    "            # print('Computed: L={:02d} | W={:.2f} | periodic={: <5}. Execution took {: 8.2f}s or {: 6.2f}min'.format(L, W, str(p), exec_time, exec_time/60))\n",
    "print('Eigenvectors `Vs` dominates the file size. Assume 1000 samples are generated for each W, for around 20 Ws:')\n",
    "print('For L=8, each Es file is 2MB, Hs is 23MB, Vs is 1000MB.')\n",
    "print('Estimate file size of each run is thus {}MB'.format(fs*(2+23+1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('The full eigenvectors `Vs` dominates the file size.')\n",
    "for i in range(5):\n",
    "    print('For L = {:2d}, Vs is {:3d} MB.'.format(8+i, 4**i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Visualize eigenvectors for different W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num = 4\n",
    "num_plots = num * num # Number of rho_A to display.\n",
    "base_samples = num_plots // 2 * 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test drawing images of eigenvectors.\n",
    "J  = 1                      # Always = 1\n",
    "L  = 10                     # System size L.\n",
    "Ls = [L]                    # System sizes Ls.\n",
    "ps = [False]                # Periodic or not.\n",
    "num_EV = 1                  # Number of eigenvalues near zero to save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Training data.\n",
    "Ws_main = [0.5, 8]          # Disorder strength W.\n",
    "Hs = [base_samples]         # Number of samples per L per W.\n",
    "et = []                     # Execution time.\n",
    "EVWs_main = []\n",
    "\n",
    "for L, num_Hs in zip(Ls, Hs):\n",
    "    for p in ps:\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        EVWsi = batch_gen_EVW_data_main(L, Ws_main, J, p, num_Hs, num_EV, save_data=False)\n",
    "        EVWs_main.extend(EVWsi)\n",
    "\n",
    "        exec_time = time.time() - start_time\n",
    "        et.append(exec_time)\n",
    "        print('Computed: L={:02d} | periodic={: <5}.'.format(L, str(p)))\n",
    "        print('Execution took {: 8.2f}s or {: 6.2f}min.'.format(exec_time, exec_time/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Random data.\n",
    "# Ws_rand = np.random.uniform(0.1, high=9.9, size=(2 * base_samples,)) # Disorder strength W.\n",
    "# Hs = [1]                    # Number of samples per L per W.\n",
    "# et = []                     # Execution time.\n",
    "# EVWs_rand = []\n",
    "\n",
    "# for L, num_Hs in zip(Ls, Hs):\n",
    "#     for p in ps:\n",
    "\n",
    "#         start_time = time.time()\n",
    "\n",
    "#         EVWsi = batch_gen_EVW_data_rand(L, Ws_rand, J, p, num_Hs, num_EV, save_data=False)\n",
    "#         EVWs_rand.extend(EVWsi)\n",
    "\n",
    "#         exec_time = time.time() - start_time\n",
    "#         et.append(exec_time)\n",
    "#         print('Computed: L={:02d} | periodic={: <5}.'.format(L, str(p)))\n",
    "#         print('Execution took {: 8.2f}s or {: 6.2f}min.'.format(exec_time, exec_time/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(len(EVWs_main))       # = base_samples * len(Ws) * num_EV\n",
    "print(len(EVWs_main[0]))    # = 3\n",
    "print(len(EVWs_main[0][1])) # = 2**L\n",
    "\n",
    "# print(len(EVWs_rand))       # = base_samples * len(Ws) * num_EV\n",
    "# print(len(EVWs_rand[0]))    # = 3\n",
    "# print(len(EVWs_rand[0][1])) # = 2**L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Wext_Vs_main = []\n",
    "Wloc_Vs_main = []\n",
    "\n",
    "for E, V, W in EVWs_main:\n",
    "\n",
    "    if W == 0.5:\n",
    "        Wext_Vs_main.append(V)\n",
    "    elif W == 8:\n",
    "        Wloc_Vs_main.append(V)\n",
    "\n",
    "Wext_Vs_main = np.array(Wext_Vs_main)\n",
    "Wloc_Vs_main = np.array(Wloc_Vs_main)\n",
    "print(Wext_Vs_main.shape)\n",
    "print(Wloc_Vs_main.shape)\n",
    "\n",
    "# Reshape due to `num_EV`.\n",
    "Wext_Vs_main = Wext_Vs_main.reshape(-1, 2**L)\n",
    "Wloc_Vs_main = Wloc_Vs_main.reshape(-1, 2**L)\n",
    "print(Wext_Vs_main.shape)\n",
    "print(Wloc_Vs_main.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Wext_Vs_rand = []\n",
    "# Wloc_Vs_rand = []\n",
    "\n",
    "# for E, V, W in EVWs_rand:\n",
    "\n",
    "#     if W <= 3.0:\n",
    "#         Wext_Vs_rand.append(V)\n",
    "#     else:\n",
    "#         Wloc_Vs_rand.append(V)\n",
    "\n",
    "# Wext_Vs_rand = np.array(Wext_Vs_rand)\n",
    "# Wloc_Vs_rand = np.array(Wloc_Vs_rand)\n",
    "# print(Wext_Vs_rand.shape)\n",
    "# print(Wloc_Vs_rand.shape)\n",
    "\n",
    "# # Reshape due to `num_EV`.\n",
    "# Wext_Vs_rand = Wext_Vs_rand.reshape(-1, 2**L)\n",
    "# Wloc_Vs_rand = Wloc_Vs_rand.reshape(-1, 2**L)\n",
    "# print(Wext_Vs_rand.shape)\n",
    "# print(Wloc_Vs_rand.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(fig_w/dpi,fig_h/dpi), dpi=dpi, squeeze=False)\n",
    "fig.suptitle('Visualize training eigenvectors ($W=0.5,W=8.0$)', fontsize=16)\n",
    "\n",
    "axes[0,0].imshow(np.abs(Wext_Vs_main), vmin=0, vmax=1e-1)\n",
    "axes[0,0].set_title('Extended phase ($W=0.5$)')\n",
    "axes[0,0].annotate('W={:3.1f}'.format(0.5), (0.5,0.5), xycoords='axes fraction', ha='center', color='w', fontsize=14)\n",
    "\n",
    "axes[0,1].imshow(np.abs(Wloc_Vs_main), vmin=0, vmax=1e-1)\n",
    "axes[0,1].set_title('Localized phase ($W=8.0$)')\n",
    "axes[0,1].annotate('W={:3.1f}'.format(8.0), (0.5,0.5), xycoords='axes fraction', ha='center', color='w', fontsize=14)\n",
    "\n",
    "for axe in axes:\n",
    "    for ax in axe:\n",
    "        # ax.legend(loc='best')\n",
    "        ax.xaxis.set_ticklabels([])\n",
    "        ax.yaxis.set_ticklabels([])\n",
    "        # ax.xaxis.set_visible(False)\n",
    "        # ax.yaxis.set_visible(False)\n",
    "        ax.set_xlabel('Eigenvector coefficients')\n",
    "        ax.set_ylabel('Eigenvector samples')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch generate data (execution part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Estimated runtime to generate 10,000 samples per W:')\n",
    "for i in range(7):\n",
    "    print('For L = {:2d}, est runtime {:3d} min or {: 2.2f} hrs.'.format(8+i, 7 * 2**i, 7 * 2**i / 60))\n",
    "print(' ')\n",
    "\n",
    "print('Estimated runtime to generate 100,000 samples per W:')\n",
    "for i in range(7):\n",
    "    print('For L = {:2d}, est runtime {:4d} min or {:5.2f} hrs.'.format(8+i, 70 * 2**i, 70 * 2**i / 60))\n",
    "print(' ')\n",
    "\n",
    "print('It will therefore be safer to break down generation to segments of 1-hr:')\n",
    "for i in range(7):\n",
    "    print('For L = {:2d}, separate execution into {:2d} batches with {:6d} samples each.'.format(8+i, 2**i, 100000 // 2**i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Batch generate eigenvectors.\n",
    "\n",
    "k = 5\n",
    "batches = 10\n",
    "batch_resume = 2\n",
    "base_sample = 10000 // k // batches # Divide by k (num_EV)\n",
    "rand_sample = 100           # Samples per random W.\n",
    "Ws_main = [0.5, 8]          # Disorder strength W.\n",
    "Ws_rand = np.random.uniform(0.1, high=7.9, size=(2 * base_sample // rand_sample,))\n",
    "J  = 1                      # Always = 1\n",
    "Ls = list(range(8,13,1))    # System sizes L.\n",
    "ps = [False, True]          # Periodic or not.\n",
    "et = []                     # Execution time.\n",
    "Hs_main = [base_sample]*len(Ls) # Number of samples per L per W.\n",
    "Hs_rand = [rand_sample]*len(Ls) # Number of samples per L per W.\n",
    "num_EVs = [k]               # Number of eigenvalues near zero to save.\n",
    "\n",
    "for i in range(batches):\n",
    "\n",
    "    if i < batch_resume:\n",
    "        tqdm.write('{} | Processing batch {:03d} of {:d}:'.format(dt(), i+1, batches)) #, flush=True)\n",
    "        tqdm.write('{} | Batch {:03d} skipped.'.format(dt(), i+1, batches)) #, flush=True)\n",
    "        tqdm.write(' ') #, flush=True)\n",
    "        continue\n",
    "\n",
    "    tqdm.write('='*60) #, flush=True)\n",
    "    tqdm.write('{} | Processing batch {:03d} of {:d}:'.format(dt(), i+1, batches)) #, flush=True)\n",
    "    tqdm.write(' ') #, flush=True)\n",
    "    for L, num_Hs_m, num_Hs_r in zip(Ls, Hs_main, Hs_rand):\n",
    "\n",
    "        tqdm.write('='*40) #, flush=True)\n",
    "        for num_EV in num_EVs:\n",
    "            for p in ps:\n",
    "                start_time = time.time()\n",
    "\n",
    "                tqdm.write('{} | Generating training data for L={:02d} | num_EV={} | periodic={: <5}...'.format(dt(), L, num_EV, str(p))) #, flush=True)\n",
    "                batch_gen_EVW_data_main(L, Ws_main, J, p, num_Hs_m, num_EV, save_data=True)\n",
    "                tqdm.write('{} | Generating random data for L={:02d} | num_EV={} | periodic={: <5}...'.format(dt(), L, num_EV, str(p))) #, flush=True)\n",
    "                batch_gen_EVW_data_rand(L, Ws_rand, J, p, num_Hs_r, num_EV, save_data=True)\n",
    "\n",
    "                exec_time = time.time() - start_time\n",
    "                et.append(exec_time)\n",
    "                tqdm.write('{} | Computed: L={:02d} | num_EV={} | periodic={: <5}.'.format(dt(), L, num_EV, str(p))) #, flush=True)\n",
    "                tqdm.write('{} | Execution took {: 8.2f}s or {: 6.2f}min.'.format(dt(), exec_time, exec_time/60)) #, flush=True)\n",
    "                tqdm.write(' ') #, flush=True)\n",
    "\n",
    "        tqdm.write('='*40) #, flush=True)\n",
    "        tqdm.write(' ') #, flush=True)\n",
    "\n",
    "    tqdm.write('{} | Batch {:03d} of {:d} completed.'.format(dt(), i+1, batches)) #, flush=True)\n",
    "    tqdm.write('='*60) #, flush=True)\n",
    "    tqdm.write(' ') #, flush=True)\n",
    "\n",
    "    if check_shutdown_signal():\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit",
   "language": "python",
   "name": "python37264bit22d8f94fb4124d8cb7bc86dc616da5cb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
