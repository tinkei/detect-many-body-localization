{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_colab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 13*: Machine Learning of Many Body Localization (Exact diagonalization + Machine Learning)\n",
    "\n",
    "Use exact diagonalization to obtain all eigenstates of the the Heisenberg model with a\n",
    "random field, \n",
    "\n",
    "\\begin{equation}\n",
    "    H = J \\sum_i \\vec{S}_{i} \\cdot \\vec{S}_{i+1} - \\sum_i h_i S^z_i\n",
    "\\end{equation}\n",
    "\n",
    ", where the values of the field $ h_i \\in [-W, W] $ are chosen from a uniform random distribution with a \"disorder strength\" $W$ (Use moderate system sizes $L = [10, 12]$). \n",
    "\n",
    "The exciting property of this model is that it is believed to undergo a phase transition from an extended phase (small $W$) to a localized phase (large $W$). \n",
    "\n",
    "We will use ML to detect this transition: Pick a number of eigenstates that are near energy $E = 0$ and obtain the reduced density matrices $\\rho^A$, where $A$ is a region of $n$ consecutive spins (a few hundred to thousands eigenstates for different disorder realizations). \n",
    "\n",
    "Now use the density matrices for $W = 0.5 J$ and $W = 8.0 J$ to train a neural network (just interpret the entries of $\\rho^A$ as an image with $2^n \\times 2^n$ pixel). Then use this network and study the output of the neural network for different $W$. \n",
    "\n",
    "How does the results depend on system size $L$ and block size $n$? At which $W_c$ do you expect the\n",
    "transition to occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Author: Tin Kei CHENG_  \n",
    "_TUM, 2021_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Wikipedia:  \n",
    "[Many body localization](https://en.wikipedia.org/wiki/Many_body_localization)  \n",
    "[Localization protected quantum order](https://en.wikipedia.org/wiki/Localization_protected_quantum_order)  \n",
    "\n",
    "Many-body localization (MBL) is a dynamical phenomenon which leads to the breakdown of equilibrium statistical mechanics in isolated many-body systems. Such systems never reach local thermal equilibrium, and retain local memory of their initial conditions for infinite times.\n",
    "\n",
    "MBL was first proposed by P.W. Anderson in 1958 as a possibility that could arise in strongly disordered quantum systems. The basic idea was that if particles all live in a random energy landscape, then any rearrangement of particles would change the energy of the system. Since energy is a conserved quantity in quantum mechanics, such a process can only be virtual and cannot lead to any transport of particle number or energy.  \n",
    "\n",
    "The process of thermalization erases local memory of the initial conditions. In textbooks, thermalization is ensured by coupling the system to an external environment or \"reservoir,\" with which the system can exchange energy. What happens if the system is isolated from the environment, and evolves according to its own Schrödinger equation? Does the system still thermalize?\n",
    "\n",
    "Quantum mechanical time evolution is unitary and formally preserves all information about the initial condition in the quantum state at all times.\n",
    "\n",
    "This question can be formalized by considering the quantum mechanical density matrix ρ of the system. If the system is divided into a subregion A (the region being probed) and its complement B (everything else), then all information that can be extracted by measurements made on A alone is encoded in the reduced density matrix $\\rho_A = Tr_B (\\rho(t))$. If in the long time limit $\\rho_A(t)$ approaches a thermal density matrix at a temperature set by the energy density in the state, then the system has \"thermalized,\" and no local information about the initial condition can be extracted from local measurements. This process of \"quantum thermalization\" may be understood in terms of B acting as a reservoir for A. In this perspective, the entanglement entropy $ S = - Tr \\rho_A log \\rho_A $ of a thermalizing system in a pure state plays the role of thermal entropy. Thermalizing systems therefore generically have extensive or \"volume law\" entanglement entropy at any non-zero temperature.\n",
    "\n",
    "In contrast, if $\\rho_A(t)$ fails to approach a thermal density matrix even in the long time limit, and remains instead close to its initial condition $\\rho_A(0)$, then the system retains forever a memory of its initial condition in local observables. This latter possibility is referred to as \"many body localization,\" and involves B failing to act as a reservoir for A. Eigenstates of systems exhibiting MBL do not obey the ETH, and generically follow an \"area law\" for entanglement entropy (i.e. the entanglement entropy scales with the surface area of subregion A).\n",
    "\n",
    "In thermalizing systems, energy eigenstates have volume law entanglement entropy. In MBL systems, energy eigenstates have area law entanglement entropy.\n",
    "\n",
    "In thermalizing systems, entanglement entropy grows as a power law in time starting from low entanglement initial conditions. In MBL systems, entanglement entropy grows logarithmically in time starting from low entanglement initial conditions.\n",
    "\n",
    "In thermalizing systems, the dynamics of out-of-time-ordered correlators forms a linear light cone which reflects the ballistic propagation of information. In MBL systems, the light cone is logarithmic.\n",
    "\n",
    "What's more, while individual eigenstates aren't themselves experimentally accessible, order in eigenstates nevertheless has measurable dynamical signatures. The eigenspectrum properties change in a singular fashion as the system transitions between from one type of MBL phase to another, or from an MBL phase to a thermal one---again with measurable dynamical signatures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numba import jit, njit # Set \"nopython\" mode for best performance, equivalent to @njit # cache=True, parallel=True\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "tz = pytz.timezone('Europe/Berlin')\n",
    "\n",
    "import scipy\n",
    "import scipy.linalg\n",
    "import scipy.sparse.linalg\n",
    "\n",
    "from scipy.sparse import csr_matrix, kron, identity\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.linalg import svd\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "dpi = 100\n",
    "fig_w = 1280\n",
    "fig_h = 640\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if in_colab:\n",
    "    ED_data_dir = 'drive/MyDrive/Colab Data/CMMP/ED_data'\n",
    "    rho_train_data_dir = 'drive/MyDrive/Colab Data/CMMP/rho_train_data'\n",
    "    rho_valid_data_dir = 'drive/MyDrive/Colab Data/CMMP/rho_valid_data'\n",
    "    model_dir  = 'drive/MyDrive/Colab Data/CMMP/models'\n",
    "    signal_dir = 'drive/MyDrive/Colab Data/CMMP'\n",
    "else:\n",
    "    ED_data_dir = 'ED_data'\n",
    "    rho_train_data_dir = 'rho_train_data'\n",
    "    rho_valid_data_dir = 'rho_valid_data'\n",
    "    model_dir  = 'models'\n",
    "    signal_dir = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if in_colab:\n",
    "    !cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if in_colab:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# if in_colab:\n",
    "#     !pip install pytorch_lightning==0.7.6 torchsummary==1.5.1\n",
    "#     !pip install torch==1.4.0+cu92 torchvision==0.5.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pip install torch==1.4.0+cu92 torchsummary==1.5.1 torchvision==0.5.0+cu92 pytorch-lightning==0.7.6  -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms\n",
    "\n",
    "# import pytorch_lightning as pl\n",
    "# from pytorch_lightning import Trainer, seed_everything\n",
    "# from pytorch_lightning.callbacks import EarlyStopping\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "# # from torchsummary import summary\n",
    "# # help(summary)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # device = \"cpu\"\n",
    "# print(device)\n",
    "# # python -c \"import torch; print(torch.__version__)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     10,
     32,
     56,
     82,
     115
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def dt():\n",
    "    return datetime.now(tz=tz).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def dict_to_str(_dict):\n",
    "    \n",
    "    od = OrderedDict(sorted(_dict.items())) # Sort keys.\n",
    "    s = json.dumps(od) # Turn dict into str.\n",
    "    s = s[1:-1].replace('\\\"', '').replace(' ', '') # Replace some special characeters.\n",
    "    s = ''.join(x if x.isalnum() else ('=' if x == ':' else '_') for x in s) # Replace all remaining special characters.\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def save_cache(obj, obj_name, obj_params, cache_dir='cache'):\n",
    "    \"\"\"Cache an object, together with the parameters used to generate it.\n",
    "    For `obj_params`, try not to use nested dict or with complicated objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj : object\n",
    "        An `object` you want to cache.\n",
    "    obj_name : str\n",
    "        A unique name you give to this object.\n",
    "    obj_params : dict\n",
    "        A `dict` of all parameters necessary to generate this object.\n",
    "    cache_dir : str, optional\n",
    "        Directory where the cache is located.\n",
    "    \"\"\"\n",
    "\n",
    "    param_str = dict_to_str(obj_params)\n",
    "    os.makedirs(os.path.join(cache_dir, obj_name), exist_ok=True)\n",
    "    with gzip.open(os.path.join(cache_dir, obj_name, param_str + '.pkl.gz'), 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_cache(obj_name, obj_params, cache_dir='cache'):\n",
    "    \"\"\"Check if the object is cached. If not, return None.\n",
    "    For `obj_params`, try not to use nested dict or with complicated objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj_name : str\n",
    "        A unique name you give to this object.\n",
    "    obj_params : dict\n",
    "        A `dict` of all parameters necessary to generate this object.\n",
    "    cache_dir : str, optional\n",
    "        Directory where the cache is located.\n",
    "    \"\"\"\n",
    "\n",
    "    param_str = dict_to_str(obj_params)\n",
    "    os.makedirs(os.path.join(cache_dir, obj_name), exist_ok=True)\n",
    "    if os.path.isfile(os.path.join(cache_dir, obj_name, param_str + '.pkl.gz')):\n",
    "        with gzip.open(os.path.join(cache_dir, obj_name, param_str + '.pkl.gz'), 'rb') as handle:\n",
    "            obj = pickle.load(handle)\n",
    "            return obj\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def check_shutdown_signal(signal_dir=signal_dir):\n",
    "    \"\"\"To gracefully stop generating data by making sure a loop is completed, this function will read a text file in a directory for the value `1`.\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    shutdown : bool\n",
    "        Shutdown signal detected.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(os.path.join(signal_dir), exist_ok=True)\n",
    "    if os.path.isfile(os.path.join(signal_dir, 'shutdown_signal.txt')):\n",
    "        with open(os.path.join(signal_dir, 'shutdown_signal.txt')) as f:\n",
    "            lines = f.readlines()\n",
    "        if lines is not None and len(lines) > 0:\n",
    "            lines = [x.strip() for x in lines]\n",
    "            if lines[0] == '1':\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "@njit\n",
    "def is_sorted(arr):\n",
    "    return np.all(arr[:-1] <= arr[1:])\n",
    "\n",
    "\n",
    "def save_ED(obj, obj_name, L, W, periodic, data_dir=ED_data_dir):\n",
    "    \"\"\"Save a list of exact diagonalization results, organized by the parameters used to generate them.\n",
    "    For `obj_params`, try not to use nested dict or with complicated objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj : list\n",
    "        A list of lists, where each reduced density matrix is paired with its disorder strength W.\n",
    "        Number of data must be a multiple of 10.\n",
    "    obj_name : str\n",
    "        A unique name you give to this object. Call it `rho_A`.\n",
    "    L : int\n",
    "        System size.\n",
    "    W : float\n",
    "        Disorder strength.\n",
    "    periodic : bool\n",
    "        Whether the Hamiltonian is periodic.\n",
    "    data_dir : str, optional\n",
    "        Directory where the data is saved.\n",
    "    \"\"\"\n",
    "\n",
    "    directory = os.path.join(data_dir, obj_name, 'L={:02d}'.format(L), 'W={:.2f}'.format(W), 'periodic={}'.format(periodic))\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Check if file exists, and increment suffix.\n",
    "    i = 0\n",
    "    while os.path.exists(os.path.join(directory, obj_name + '-{:09d}.pkl.gz'.format(i))):\n",
    "        i += 1\n",
    "\n",
    "    with gzip.open(os.path.join(directory, obj_name + '-{:09d}.pkl.gz'.format(i)), 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_ED(obj_name, L, W, periodic, data_dir=ED_data_dir):\n",
    "    \"\"\"Check if the object is cached. If not, return None.\n",
    "    For `obj_params`, try not to use nested dict or with complicated objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj_name : str\n",
    "        A unique name you give to this object. Call it `rho_A`.\n",
    "    L : int\n",
    "        System size.\n",
    "    W : float\n",
    "        Disorder strength.\n",
    "    periodic : bool\n",
    "        Whether the Hamiltonian is periodic.\n",
    "    data_dir : str, optional\n",
    "        Directory where the data is saved.\n",
    "    \"\"\"\n",
    "\n",
    "    raise NotImplementedError('Function not implemented.')\n",
    "\n",
    "    param_str = dict_to_str(obj_params)\n",
    "    os.makedirs(os.path.join(data_dir, obj_name, 'L={:2d}'.format(L), 'W={:.2d}'.format(W)), exist_ok=True)\n",
    "    if os.path.isfile(os.path.join(data_dir, obj_name, 'L={:2d}'.format(L), 'W={:.2d}'.format(W), param_str + '.pkl.gz')):\n",
    "        with gzip.open(os.path.join(data_dir, obj_name, 'L={:2d}'.format(L), 'W={:.2d}'.format(W), param_str + '.pkl.gz'), 'rb') as handle:\n",
    "            obj = pickle.load(handle)\n",
    "            return obj\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     35
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def save_rho_train(obj, obj_name, L, n, periodic, num_EV, data_dir=rho_train_data_dir):\n",
    "    \"\"\"Save a list of reduced density matrices with W = {0.5, 8}.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj : list\n",
    "        A list of lists, where each reduced density matrix is paired with its disorder strength W.\n",
    "        i.e. obj[i][0] is a 2D numpy.ndarray of the reduced density matrix, and obj[i][1] is the disorder strength used to generate it.\n",
    "        Number of data must be a multiple of 10.\n",
    "    obj_name : str\n",
    "        A name you give to this object. Call it `rho_A`.\n",
    "    L : int\n",
    "        System size.\n",
    "    n : int\n",
    "        Number of consecutive spins sampled.\n",
    "    periodic : bool\n",
    "        Whether the Hamiltonian is periodic.\n",
    "    num_EV : int\n",
    "        Number of eigenvalues around zero being sampled.\n",
    "    data_dir : str, optional\n",
    "        Directory where the data is saved.\n",
    "    \"\"\"\n",
    "\n",
    "    directory = os.path.join(data_dir, 'L={:02d}'.format(L), 'n={:02d}'.format(n), 'periodic={}'.format(periodic), 'num_EV={}'.format(num_EV))\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Check if file exists, and increment suffix.\n",
    "    i = 0\n",
    "    while os.path.exists(os.path.join(directory, obj_name + '-{:09d}.pkl.gz'.format(i))):\n",
    "        i += 1\n",
    "\n",
    "    with gzip.open(os.path.join(directory, obj_name + '-{:09d}.pkl.gz'.format(i)), 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def save_rho_valid(obj, obj_name, L, n, periodic, num_EV, data_dir=rho_valid_data_dir):\n",
    "    \"\"\"Save a list of reduced density matrices with random W != {0.5, 8}.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj : list\n",
    "        A list of lists, where each reduced density matrix is paired with its disorder strength W.\n",
    "        i.e. obj[i][0] is a 2D numpy.ndarray of the reduced density matrix, and obj[i][1] is the disorder strength used to generate it.\n",
    "        Number of data must be a multiple of 10.\n",
    "    obj_name : str\n",
    "        A name you give to this object. Call it `rho_A`.\n",
    "    L : int\n",
    "        System size.\n",
    "    n : int\n",
    "        Number of consecutive spins sampled.\n",
    "    periodic : bool\n",
    "        Whether the Hamiltonian is periodic.\n",
    "    num_EV : int\n",
    "        Number of eigenvalues around zero being sampled.\n",
    "    data_dir : str, optional\n",
    "        Directory where the data is saved.\n",
    "    \"\"\"\n",
    "\n",
    "    directory = os.path.join(data_dir, 'L={:02d}'.format(L), 'n={:02d}'.format(n), 'periodic={}'.format(periodic), 'num_EV={}'.format(num_EV))\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Check if file exists, and increment suffix.\n",
    "    i = 0\n",
    "    while os.path.exists(os.path.join(directory, obj_name + '-{:09d}.pkl.gz'.format(i))):\n",
    "        i += 1\n",
    "\n",
    "    with gzip.open(os.path.join(directory, obj_name + '-{:09d}.pkl.gz'.format(i)), 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     40
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_rho_train(obj_name, L, n, periodic, num_EV, data_dir=rho_train_data_dir):\n",
    "    \"\"\"Load a list of reduced density matrices with W = {0.5, 8}.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj_name : str\n",
    "        A name you give to this object. Call it `rho_A`.\n",
    "    L : int\n",
    "        System size.\n",
    "    n : int\n",
    "        Number of consecutive spins sampled.\n",
    "    periodic : bool\n",
    "        Whether the Hamiltonian is periodic.\n",
    "    num_EV : int\n",
    "        Number of eigenvalues around zero being sampled.\n",
    "    data_dir : str, optional\n",
    "        Directory where the data is saved.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    obj : list\n",
    "        A list of lists, where each reduced density matrix is paired with its disorder strength W.\n",
    "        i.e. obj[i][0] is a 2D numpy.ndarray of the reduced density matrix, and obj[i][1] is the disorder strength used to generate it.\n",
    "        Number of data must be a multiple of 10.\n",
    "    \"\"\"\n",
    "\n",
    "    directory = os.path.join(data_dir, 'L={:02d}'.format(L), 'n={:02d}'.format(n), 'periodic={}'.format(periodic), 'num_EV={}'.format(num_EV))\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Check if file exists, load the file, and increment suffix.\n",
    "    i = 0\n",
    "    data = []\n",
    "    while os.path.exists(os.path.join(directory, obj_name + '-{:09d}.pkl.gz'.format(i))):\n",
    "        with gzip.open(os.path.join(directory, obj_name + '-{:09d}.pkl.gz'.format(i)), 'rb') as handle:\n",
    "            data = data + pickle.load(handle)\n",
    "        i += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_rho_valid(obj_name, L, n, periodic, num_EV, data_dir=rho_valid_data_dir):\n",
    "    \"\"\"Load a list of reduced density matrices with random W != {0.5, 8}.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj_name : str\n",
    "        A name you give to this object. Call it `rho_A`.\n",
    "    L : int\n",
    "        System size.\n",
    "    n : int\n",
    "        Number of consecutive spins sampled.\n",
    "    periodic : bool\n",
    "        Whether the Hamiltonian is periodic.\n",
    "    num_EV : int\n",
    "        Number of eigenvalues around zero being sampled.\n",
    "    data_dir : str, optional\n",
    "        Directory where the data is saved.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    obj : list\n",
    "        A list of lists, where each reduced density matrix is paired with its disorder strength W.\n",
    "        i.e. obj[i][0] is a 2D numpy.ndarray of the reduced density matrix, and obj[i][1] is the disorder strength used to generate it.\n",
    "        Number of data must be a multiple of 10.\n",
    "    \"\"\"\n",
    "\n",
    "    directory = os.path.join(data_dir, 'L={:02d}'.format(L), 'n={:02d}'.format(n), 'periodic={}'.format(periodic), 'num_EV={}'.format(num_EV))\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Check if file exists, load the file, and increment suffix.\n",
    "    i = 0\n",
    "    data = []\n",
    "    while os.path.exists(os.path.join(directory, obj_name + '-{:09d}.pkl.gz'.format(i))):\n",
    "        with gzip.open(os.path.join(directory, obj_name + '-{:09d}.pkl.gz'.format(i)), 'rb') as handle:\n",
    "            data = data + pickle.load(handle)\n",
    "        i += 1\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     18,
     28
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, file_name, L, n, periodic, num_EV, directory=model_dir):\n",
    "    \"\"\"Save model as pickle\"\"\"\n",
    "\n",
    "    model = model.cpu()\n",
    "    model_dict = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"hparams\": model.hparams\n",
    "    }\n",
    "\n",
    "    directory = os.path.join(directory, 'L={:02d}'.format(L), 'n={:02d}'.format(n), 'periodic={}'.format(periodic), 'num_EV={}'.format(num_EV))\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    model_path = os.path.join(directory, file_name)\n",
    "    with gzip.open(model_path, 'wb', 4) as handle:\n",
    "        pickle.dump(model_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return model_path\n",
    "\n",
    "\n",
    "def load_model(file_name, L, n, periodic, num_EV, directory=model_dir):\n",
    "\n",
    "    directory = os.path.join(directory, 'L={:02d}'.format(L), 'n={:02d}'.format(n), 'periodic={}'.format(periodic), 'num_EV={}'.format(num_EV))\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    model_path = os.path.join(directory, file_name)\n",
    "    with gzip.open(model_path, 'rb') as fp:\n",
    "        return pickle.load(fp)[\"state_dict\"]\n",
    "\n",
    "\n",
    "def model_exists(file_name, L, n, periodic, num_EV, directory=model_dir):\n",
    "\n",
    "    directory = os.path.join(directory, 'L={:02d}'.format(L), 'n={:02d}'.format(n), 'periodic={}'.format(periodic), 'num_EV={}'.format(num_EV))\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    model_path = os.path.join(directory, file_name)\n",
    "    return os.path.exists(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Build Hamiltonian and Exact Diagonalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def get_h(L, W):\n",
    "    h = np.random.uniform(-W, W, L)\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_si_list(L):\n",
    "\n",
    "    # Get single site operaors.\n",
    "    sx = csr_matrix(np.array([[0.,  1. ], [1. ,  0.]]))\n",
    "    sy = csr_matrix(np.array([[0., -1.j], [1.j,  0.]]))\n",
    "    sz = csr_matrix(np.array([[1.,  0. ], [0. , -1.]]))\n",
    "    id = csr_matrix(np.eye(2))\n",
    "\n",
    "    # ========================================\n",
    "    # Start cached area: si_list.\n",
    "    # ========================================\n",
    "\n",
    "    obj_params = {'L': L}\n",
    "\n",
    "    sx_list = load_cache('sx_list', obj_params)\n",
    "    sy_list = load_cache('sy_list', obj_params)\n",
    "    sz_list = load_cache('sz_list', obj_params)\n",
    "\n",
    "    if sx_list is None or sy_list is None or sz_list is None:\n",
    "\n",
    "        # print('Cache not found for `si_list`. Generate from scratch.')\n",
    "\n",
    "        sx_list = []  # sx_list[i] = kron([id, id, ..., id, sx, id, .... id])\n",
    "        sy_list = []\n",
    "        sz_list = []\n",
    "\n",
    "        for i_site in range(L):\n",
    "\n",
    "            x_ops = [id] * L\n",
    "            y_ops = [id] * L\n",
    "            z_ops = [id] * L\n",
    "            x_ops[i_site] = sx\n",
    "            y_ops[i_site] = sy\n",
    "            z_ops[i_site] = sz\n",
    "\n",
    "            X = x_ops[0]\n",
    "            Y = y_ops[0]\n",
    "            Z = z_ops[0]\n",
    "            for j in range(1, L):\n",
    "                X = kron(X, x_ops[j], 'csr')\n",
    "                Y = kron(Y, y_ops[j], 'csr')\n",
    "                Z = kron(Z, z_ops[j], 'csr')\n",
    "            sx_list.append(X)\n",
    "            sy_list.append(Y)\n",
    "            sz_list.append(Z)\n",
    "\n",
    "        save_cache(sx_list, 'sx_list', obj_params)\n",
    "        save_cache(sy_list, 'sy_list', obj_params)\n",
    "        save_cache(sz_list, 'sz_list', obj_params)\n",
    "\n",
    "    # else:\n",
    "\n",
    "    #     print('Cache found for `si_list`. Load from cache.')\n",
    "\n",
    "    # ========================================\n",
    "    # End cached area: si_list.\n",
    "    # ========================================\n",
    "\n",
    "    return sx_list, sy_list, sz_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_H_ii(L, periodic):\n",
    "\n",
    "    sx_list, sy_list, sz_list = build_si_list(L)\n",
    "\n",
    "    # ========================================\n",
    "    # Start cached area: H_ii.\n",
    "    # ========================================\n",
    "    \n",
    "    obj_params = {'L': L, 'periodic': periodic}\n",
    "\n",
    "    H_xx = load_cache('H_xx', obj_params)\n",
    "    H_yy = load_cache('H_yy', obj_params)\n",
    "    H_zz = load_cache('H_zz', obj_params)\n",
    "\n",
    "    if H_xx is None or H_yy is None or H_zz is None:\n",
    "\n",
    "        # print('Cache not found for `H_ii`. Generate from scratch.')\n",
    "\n",
    "        H_xx = csr_matrix((2**L, 2**L))\n",
    "        H_yy = csr_matrix((2**L, 2**L))\n",
    "        H_zz = csr_matrix((2**L, 2**L))\n",
    "\n",
    "        for i in range(L if periodic else L - 1):\n",
    "            H_xx = H_xx + sx_list[i] * sx_list[(i + 1) % L]\n",
    "            H_yy = H_yy + sy_list[i] * sy_list[(i + 1) % L]\n",
    "            H_zz = H_zz + sz_list[i] * sz_list[(i + 1) % L]\n",
    "\n",
    "        save_cache(H_xx, 'H_xx', obj_params)\n",
    "        save_cache(H_yy, 'H_yy', obj_params)\n",
    "        save_cache(H_zz, 'H_zz', obj_params)\n",
    "\n",
    "    # else:\n",
    "\n",
    "    #     print('Cache found for `H_ii`. Load from cache.')\n",
    "\n",
    "    # ========================================\n",
    "    # End cached area: H_ii.\n",
    "    # ========================================\n",
    "\n",
    "    return H_xx, H_yy, H_zz, sx_list, sy_list, sz_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     15
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_H(L, W, J, periodic=False):\n",
    "\n",
    "    H_xx, H_yy, H_zz, sx_list, sy_list, sz_list = build_H_ii(L, periodic)\n",
    "\n",
    "    # H_z is not cached due to randomness.\n",
    "    H_z  = csr_matrix((2**L, 2**L))\n",
    "    h    = get_h(L, W)\n",
    "\n",
    "    for i in range(L):\n",
    "        H_z = H_z + h[i] * sz_list[i]\n",
    "\n",
    "    H = J * (H_xx + H_yy + H_zz) - H_z\n",
    "\n",
    "    return H\n",
    "\n",
    "def build_Hs(L, W, J, periodic=False, num_Hs=1000):\n",
    "\n",
    "    H_xx, H_yy, H_zz, sx_list, sy_list, sz_list = build_H_ii(L, periodic)\n",
    "\n",
    "    Hs = []\n",
    "    for i in tqdm(range(num_Hs), leave=False, desc='build_Hs()'):\n",
    "\n",
    "        # H_z is not cached due to randomness.\n",
    "        H_z  = csr_matrix((2**L, 2**L))\n",
    "        h    = get_h(L, W)\n",
    "\n",
    "        for i in range(L):\n",
    "            H_z = H_z + h[i] * sz_list[i]\n",
    "\n",
    "        H = J * (H_xx + H_yy + H_zz) - H_z\n",
    "        Hs.append(H)\n",
    "\n",
    "    return Hs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     32,
     67,
     105
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def ED(H):\n",
    "    \"\"\"For comparison: obtain ground state energy from exact diagonalization.\n",
    "\n",
    "    Exponentially expensive in L, only works for small enough `L` <~ 20.\n",
    "\n",
    "    The column V[:, i] is the normalized eigenvector corresponding to the eigenvalue E[i].\n",
    "    Will return a matrix object if a is a matrix object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    H : numpy.ndarray\n",
    "        Hamiltonian to diagonalize.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    E : 1D numpy.ndarray\n",
    "        Eigenvalues, sorted in ascending order.\n",
    "    V : 2D numpy.ndarray\n",
    "        Eigenvectors.\n",
    "    \"\"\"\n",
    "\n",
    "    # if L >= 20:\n",
    "    #     warnings.warn(\"Large L: Exact diagonalization might take a long time!\")\n",
    "\n",
    "    E, V = np.linalg.eigh(H)\n",
    "\n",
    "    assert is_sorted(E), 'Eigenvalues not sorted!'\n",
    "\n",
    "    return E, V\n",
    "\n",
    "\n",
    "def ED_sparse(H, k):\n",
    "    \"\"\"For comparison: obtain ground state energy from exact diagonalization.\n",
    "\n",
    "    Exponentially expensive in L, only works for small enough `L` <~ 20.\n",
    "\n",
    "    An array representing the k eigenvectors. The column v[:, i] is the eigenvector corresponding to the eigenvalue w[i].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    H : numpy.ndarray\n",
    "        Hamiltonian to diagonalize.\n",
    "    k : int\n",
    "        Number of eigenvalues around E = 0 to obtain.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    E : 1D numpy.ndarray\n",
    "        Eigenvalues, sorted in ascending order.\n",
    "    V : 2D numpy.ndarray\n",
    "        Eigenvectors.\n",
    "    \"\"\"\n",
    "\n",
    "    # if L >= 20:\n",
    "    #     warnings.warn(\"Large L: Exact diagonalization might take a long time!\")\n",
    "\n",
    "    E, V = scipy.sparse.linalg.eigsh(H, k=k, sigma=0, which='LM', return_eigenvectors=True)\n",
    "    sorted_indices = np.abs(E).argsort()\n",
    "    E = E[sorted_indices]\n",
    "    V = V[:, sorted_indices]\n",
    "\n",
    "    assert is_sorted(np.abs(E)), 'Eigenvalues not sorted!'\n",
    "\n",
    "    return E, V\n",
    "\n",
    "\n",
    "def EDs(Hs):\n",
    "    \"\"\"For comparison: obtain ground state energy from exact diagonalization.\n",
    "\n",
    "    Exponentially expensive in L, only works for small enough `L` <~ 20.\n",
    "\n",
    "    The column V[:, i] is the normalized eigenvector corresponding to the eigenvalue E[i].\n",
    "    Will return a matrix object if a is a matrix object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Hs : list of scipy.sparse.csr_matrix\n",
    "        A list of Hamiltonians to diagonalize.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    E : list of 1D numpy.ndarray\n",
    "        Eigenvalues of each Hamiltonian, sorted in ascending order.\n",
    "    V : list of 2D numpy.ndarray\n",
    "        Eigenvectors of each Hamiltonian.\n",
    "    \"\"\"\n",
    "\n",
    "    # if L >= 20:\n",
    "    #     warnings.warn(\"Large L: Exact diagonalization might take a long time!\")\n",
    "\n",
    "    Es = []\n",
    "    Vs = []\n",
    "    for H in Hs:\n",
    "\n",
    "        # Can't use scipy's eigsh, because we need ALL eigenwhatevers.\n",
    "        # E, V = eigsh(H, k=10, which='SM', return_eigenvectors=True)\n",
    "        # E, V = np.linalg.eigh(H.A)\n",
    "        E, V = ED(H.toarray())\n",
    "        Es.append(E)\n",
    "        Vs.append(V)\n",
    "\n",
    "    return Es, Vs\n",
    "\n",
    "\n",
    "def EDs_sparse(Hs, k):\n",
    "    \"\"\"For comparison: obtain ground state energy from exact diagonalization.\n",
    "\n",
    "    Exponentially expensive in L, only works for small enough `L` <~ 20.\n",
    "\n",
    "    An array representing the k eigenvectors. The column v[:, i] is the eigenvector corresponding to the eigenvalue w[i].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Hs : list of scipy.sparse.csr_matrix\n",
    "        A list of Hamiltonians to diagonalize.\n",
    "    k : int\n",
    "        Number of eigenvalues around E = 0 to obtain.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    E : 1D numpy.ndarray\n",
    "        Eigenvalues, sorted in ascending order.\n",
    "    V : 2D numpy.ndarray\n",
    "        Eigenvectors.\n",
    "    \"\"\"\n",
    "\n",
    "    # if L >= 20:\n",
    "    #     warnings.warn(\"Large L: Exact diagonalization might take a long time!\")\n",
    "\n",
    "    Es = []\n",
    "    Vs = []\n",
    "    for H in tqdm(Hs, leave=False, desc='EDs_sparse()'):\n",
    "\n",
    "        E, V = scipy.sparse.linalg.eigsh(H, k=k, sigma=0, which='LM', return_eigenvectors=True)\n",
    "        sorted_indices = np.abs(E).argsort()\n",
    "        E = E[sorted_indices]\n",
    "        V = V[:, sorted_indices]\n",
    "        Es.append(E)\n",
    "        Vs.append(V)\n",
    "\n",
    "    return Es, Vs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# @njit\n",
    "def select_N_eigenvalues(E, V, n, where='zeroest'):\n",
    "    \"\"\"\n",
    "    Select N eigenvalues closest to the lowest, to zero, or to the highest.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    E : 1D numpy.ndarray\n",
    "        Sorted eigenvalues in ascending order.\n",
    "    V : 2D numpy.ndarray\n",
    "        Corresponding eigenvectors.\n",
    "        The column V[:, i] is the normalized eigenvector corresponding to the eigenvalue E[i].\n",
    "    n : int\n",
    "        Number of eigenvalues to store.\n",
    "    where : str\n",
    "        Where to select the eigenvalues. where = {'lowest', 'zeroest', 'highest'}\n",
    "    \"\"\"\n",
    "\n",
    "    if where == 'lowest':\n",
    "        E = E[:n]\n",
    "        V = V[:, :n]\n",
    "    elif where == 'highest':\n",
    "        E = E[-n:]\n",
    "        V = V[:, -n:]\n",
    "    elif where == 'zeroest':\n",
    "        # closest_indices = np.abs(E).argsort()[:n]\n",
    "        # E = E[closest_indices]\n",
    "        # V = V[:, closest_indices]\n",
    "        # Faster implementation, but Numba doesn't support this. Still, it is a few microseconds faster.\n",
    "        # Source: https://stackoverflow.com/questions/16817948/i-have-need-the-n-minimum-index-values-in-a-numpy-array\n",
    "        closest_indices = np.argpartition(np.abs(E), n)[:n]\n",
    "        E_temp = E[closest_indices]\n",
    "        V_temp = V[:, closest_indices]\n",
    "        sorted_indices = np.abs(E_temp).argsort()[:n]\n",
    "        E = E_temp[sorted_indices]\n",
    "        V = V_temp[:, sorted_indices]\n",
    "        # assert np.all(E1 == E), 'Both sorting methods should be identical.'\n",
    "        # assert np.all(V1 == V), 'Both sorting methods should be identical.'\n",
    "\n",
    "    return E, V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test solving a single Hamiltonian.\n",
    "L = 8\n",
    "W = 0.5\n",
    "J = 1\n",
    "periodic = False\n",
    "num_Hs = 10\n",
    "\n",
    "H = build_H(L, W, J, periodic=False)\n",
    "E, V = ED(H.toarray())\n",
    "print('2^L: {}'.format(2**L))\n",
    "print('#eigenvalues: {}'.format(len(E)))\n",
    "print('E.shape: {}'.format(E.shape))\n",
    "print('V.shape: {}'.format(V.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Solve using numpy's dense solver\n",
    "E, V = ED(H.toarray())\n",
    "E0, V0 = select_N_eigenvalues(E, V, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Solve using scipy's sparse solver instead.\n",
    "E1, V1 = ED_sparse(H, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Solve using numpy's dense solver\n",
    "E, V = ED(H.toarray())\n",
    "E0, V0 = select_N_eigenvalues(E, V, 20)\n",
    "# Solve using scipy's sparse solver instead.\n",
    "E1, V1 = ED_sparse(H, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check eigenvalues are sorted correctly, and eigenvectors are selected along the correct axis.\n",
    "V0_norm = []\n",
    "V1_norm = []\n",
    "for i in range(len(E0)):\n",
    "    V0_norm.append(np.linalg.norm(V0[:,i]))\n",
    "    V1_norm.append(np.linalg.norm(V1[:,i]))\n",
    "\n",
    "for E0i, E1i, V0i, V1i in zip(E0, E1, V0_norm, V1_norm):\n",
    "    print('Numpy: {:+.12f} | Numpy norm: {:.16f}'.format(E0i, V0i))\n",
    "    print('Scipy: {:+.12f} | Scipy norm: {:.16f}'.format(E1i, V1i))\n",
    "\n",
    "assert np.allclose(V0i, V1i), 'Eigenvectors evaluated using Numpy and Scipy should be identical.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test solving multiple Hamiltonians.\n",
    "Hs = build_Hs(L, W, J, periodic, num_Hs)\n",
    "Es, Vs = EDs(Hs)\n",
    "E0s, V0s = EDs_sparse(Hs, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def batch_generate_ED_data(L, W, J=1, periodic=False, num_Hs=1000, num_EV=20, save_data=False, npsp='sp'):\n",
    "\n",
    "    obj_params = {'J': J, 'periodic': periodic} # Drop L and W. We use them for subdirectories.\n",
    "    Hs = build_Hs(L, W, J, periodic, num_Hs)\n",
    "\n",
    "    if npsp == 'sp':\n",
    "        E0s, V0s = EDs_sparse(Hs, num_EV)\n",
    "    else:\n",
    "        Es, Vs = EDs(Hs)\n",
    "        E0s = []\n",
    "        V0s = []\n",
    "        for E, V in zip(Es, Vs):\n",
    "            E0, V0 = select_N_eigenvalues(E, V, num_EV)\n",
    "            E0s.append(E0)\n",
    "            V0s.append(V0)\n",
    "\n",
    "    if save_data:\n",
    "        save_ED(E0s, 'E0s', L, W, periodic)\n",
    "        save_ED(V0s, 'V0s', L, W, periodic)\n",
    "\n",
    "    return E0s, V0s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test execution time.\n",
    "J  = 1                      # Always = 1\n",
    "Ws = [8]                    # Disorder strength W.\n",
    "Ls = list(range(8,12))      # System size L.\n",
    "ns = [1]*len(Ls)            # Number of samples for each L.\n",
    "ps = [False]                # Periodic or not.\n",
    "et = []                     # Execution time.\n",
    "num_EV = 20                 # Number of eigenvalues near zero to save.\n",
    "\n",
    "for L, num_Hs in zip(Ls, ns):\n",
    "    for W in Ws:\n",
    "        for p in ps:\n",
    "            start_time = time.time()\n",
    "            batch_generate_ED_data(L, W, J, p, num_Hs, num_EV, save_data=False, npsp='np')\n",
    "            exec_time = time.time() - start_time\n",
    "            et.append(exec_time)\n",
    "            print('Computed: L={:02d} | W={:.2f} | periodic={: <5}. Execution took {: 8.2f}s or {: 6.2f}min'.format(L, W, str(p), exec_time, exec_time/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(fig_w/dpi,fig_h/dpi), dpi=dpi, squeeze=False)\n",
    "\n",
    "base = 10\n",
    "axes[0,0].plot(Ls, et)\n",
    "axes[0,1].plot(np.array(Ls), np.log(et) / np.log(base))\n",
    "\n",
    "axes[0,0].set_title('Execution time vs System size')\n",
    "axes[0,1].set_title('Log(Execution time) vs System size')\n",
    "axes[0,0].set_ylabel('Execution time')\n",
    "axes[0,1].set_ylabel('Log(Execution time)')\n",
    "\n",
    "for axe in axes:\n",
    "    for ax in axe:\n",
    "        ax.set_xlabel('System size L')\n",
    "        # ax.legend(loc='best')\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test execution time.\n",
    "J  = 1                      # Always = 1\n",
    "Ws = [8]                    # Disorder strength W.\n",
    "Ls = list(range(8,16))      # System size L.\n",
    "ns = [1]*len(Ls)            # Number of samples for each L.\n",
    "ps = [False]                # Periodic or not.\n",
    "et = []                     # Execution time.\n",
    "num_EV = 20                 # Number of eigenvalues near zero to save.\n",
    "\n",
    "for L, num_Hs in zip(Ls, ns):\n",
    "    for W in Ws:\n",
    "        for p in ps:\n",
    "            start_time = time.time()\n",
    "            batch_generate_ED_data(L, W, J, p, num_Hs, num_EV, save_data=False, npsp='sp')\n",
    "            exec_time = time.time() - start_time\n",
    "            et.append(exec_time)\n",
    "            print('Computed: L={:02d} | W={:.2f} | periodic={: <5}. Execution took {: 8.2f}s or {: 6.2f}min'.format(L, W, str(p), exec_time, exec_time/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(fig_w/dpi,fig_h/dpi), dpi=dpi, squeeze=False)\n",
    "\n",
    "base = 10\n",
    "axes[0,0].plot(Ls, et)\n",
    "axes[0,1].plot(np.array(Ls), np.log(et) / np.log(base))\n",
    "\n",
    "axes[0,0].set_title('Execution time vs System size')\n",
    "axes[0,1].set_title('Log(Execution time) vs System size')\n",
    "axes[0,0].set_ylabel('Execution time')\n",
    "axes[0,1].set_ylabel('Log(Execution time)')\n",
    "\n",
    "for axe in axes:\n",
    "    for ax in axe:\n",
    "        ax.set_xlabel('System size L')\n",
    "        # ax.legend(loc='best')\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Sample parameters.\n",
    "J  = 1                                 # Always = 1\n",
    "Ws = [0.5, 8] * 10 + [i/2 for i in range(1*2, 10*2)] # Disorder strength W.\n",
    "Ls = [   8,   9,  10,  11, 12, 13, 14] # System size L.\n",
    "ns = [1000, 500, 250, 100, 50, 20, 10] # Number of samples for each L.\n",
    "ps = [False, True]                     # Periodic or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test file size. It's not feasible to store all eigenvectors.\n",
    "J  = 1             # Always = 1\n",
    "Ws = [0.5, 8] * 10 + [i/2 for i in range(1*2, 10*2)] # Disorder strength W.\n",
    "Ls = [   8]        # System size L.\n",
    "ns = [1000]        # Number of samples for each L.\n",
    "ps = [False, True] # Periodic or not.\n",
    "\n",
    "fs = 0\n",
    "for L, num_Hs in zip(Ls, ns):\n",
    "    for W in Ws:\n",
    "        for p in ps:\n",
    "            start_time = time.time()\n",
    "            # batch_generate_ED_data(L, W, J, p, num_Hs)\n",
    "            fs += 1\n",
    "            exec_time = time.time() - start_time\n",
    "            # print('Computed: L={:02d} | W={:.2f} | periodic={: <5}. Execution took {: 8.2f}s or {: 6.2f}min'.format(L, W, str(p), exec_time, exec_time/60))\n",
    "print('Eigenvectors `Vs` dominates the file size. Assume 1000 samples are generated for each W, for around 20 Ws:')\n",
    "print('For L=8, each Es file is 2MB, Hs is 23MB, Vs is 1000MB.')\n",
    "print('Estimate file size of each run is thus {}MB'.format(fs*(2+23+1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('The full eigenvectors `Vs` dominates the file size.')\n",
    "for i in range(5):\n",
    "    print('For L = {:2d}, Vs is {:3d} MB.'.format(8+i, 4**i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Extract reduced density matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "According to my limited understanding, a so-called partial trace is simply summing contributions of coefficients outside a chosen subsystem. \n",
    "\n",
    "Let's start with a simple example of two spins, divided into two subsystems $A$ and $B$, each with one spin:\n",
    "\n",
    "\\begin{equation}\n",
    "    | s_1 \\rangle \\otimes | s_2 \\rangle = | s_1 \\rangle_A \\otimes | s_2 \\rangle_B = | A \\rangle \\otimes | B \\rangle\n",
    "\\end{equation}\n",
    "\n",
    "An eigenvector has the following basis:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{pmatrix}\n",
    "        | (\\downarrow)_A (\\downarrow)_B \\rangle \\\\\n",
    "        | (\\downarrow)_A (\\uparrow)_B   \\rangle \\\\\n",
    "        | (\\uparrow)_A   (\\downarrow)_B \\rangle \\\\\n",
    "        | (\\uparrow)_A   (\\uparrow)_B   \\rangle \\\\\n",
    "    \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Now add coefficients $C_i$ of the eigenvector:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{pmatrix}\n",
    "        C_0 | (\\downarrow)_A (\\downarrow)_B \\rangle \\\\\n",
    "        C_1 | (\\downarrow)_A (\\uparrow)_B   \\rangle \\\\\n",
    "        C_2 | (\\uparrow)_A   (\\downarrow)_B \\rangle \\\\\n",
    "        C_3 | (\\uparrow)_A   (\\uparrow)_B   \\rangle \\\\\n",
    "    \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Or $C_{i,j}$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{pmatrix}\n",
    "        C_{0,0} | (\\downarrow)_A (\\downarrow)_B \\rangle \\\\\n",
    "        C_{0,1} | (\\downarrow)_A (\\uparrow)_B   \\rangle \\\\\n",
    "        C_{1,0} | (\\uparrow)_A   (\\downarrow)_B \\rangle \\\\\n",
    "        C_{1,1} | (\\uparrow)_A   (\\uparrow)_B   \\rangle \\\\\n",
    "    \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "The reduce density matrix of subsystem $A$ is ${\\rho}_A$ is defined as a partial trace over $B$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\rho_A = Tr_B(\\rho)\n",
    "\\end{equation}\n",
    "\n",
    "That can be obtained simply by summing coefficients related to $B$, thereby removing its contribution:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}[c]\n",
    "        \\begin{pmatrix}\n",
    "            (C_0 + C_1) | (\\downarrow)_A \\rangle \\\\\n",
    "            (C_2 + C_3) | (\\uparrow)_A   \\rangle \\\\\n",
    "        \\end{pmatrix}\n",
    "    \\end{aligned}\n",
    "    \\quad or \\quad\n",
    "    \\begin{aligned}[c]\n",
    "        \\begin{pmatrix}\n",
    "            (C_{0,0} + C_{0,1}) | (\\downarrow)_A \\rangle \\\\\n",
    "            (C_{1,0} + C_{1,1}) | (\\uparrow)_A   \\rangle \\\\\n",
    "        \\end{pmatrix}\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "We observe that if we have an eigenvector in an array, the notation $C_i$ used on the left hand side is the array index, while the right hand side is the binary representation of said index. Does this relation extend beyond two spins? (Probably not, unless each spin is its own subsystem...)\n",
    "\n",
    "But this is clearly wrong, because this is just a vector, not a matrix. Recall from Chapter 5 of our lecture notes that the definition of a reduced density matrix is also:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\rho_A = Tr_B(\\rho) = Tr_B(| \\psi \\rangle \\langle \\psi |) = \\sum_{i',j,i,j} \\psi^\\ast_{i',j} \\psi_{i,j} | i' \\rangle \\langle i | ,\n",
    "\\end{equation}\n",
    "\n",
    "where $i$ are spins/sites related to subsystem $A$, and $j$ to $B$. An element of this matrix is simply:\n",
    "\n",
    "\\begin{equation}\n",
    "    {(\\rho_A)}_{i',i} = Tr_B(\\rho)_{i',i} = \\sum_{j} \\psi^\\ast_{i',j} \\psi_{i,j} | i' \\rangle \\langle i |\n",
    "\\end{equation}\n",
    "\n",
    "The basis $ | i' \\rangle \\langle i | $ is always implicit in all calculations and never really affect anything, ever, really. The only purpose is to make everything more complicated and confusing.\n",
    "\n",
    "Reverting back to the notation of our previous 2-spin example, with $\\psi_{i,j} \\to C_i$ and $| i \\rangle \\to | s_1 \\rangle_A$, a reduced density matrix can be constructed as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{pmatrix}\n",
    "        (C_0 + C_1)^\\ast \\times (C_0 + C_1) &                     ?               \\\\\n",
    "                            ?               & (C_2 + C_3)^\\ast \\times (C_2 + C_3) \\\\\n",
    "    \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Now what should the off-diagonal terms look like? We have two choices:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}[c]\n",
    "        \\begin{pmatrix}\n",
    "            (C_0 + C_1)^\\ast \\times (C_0 + C_1) & (C_2 + C_3)^\\ast \\times (C_0 + C_1) \\\\\n",
    "            (C_0 + C_1)^\\ast \\times (C_2 + C_3) & (C_2 + C_3)^\\ast \\times (C_2 + C_3) \\\\\n",
    "        \\end{pmatrix}\n",
    "    \\end{aligned}\n",
    "    \\quad or \\quad\n",
    "    \\begin{aligned}[c]\n",
    "        \\begin{pmatrix}\n",
    "            (C_0 + C_1)^\\ast \\times (C_0 + C_1) & (C_0 + C_1)^\\ast \\times (C_2 + C_3) \\\\\n",
    "            (C_2 + C_3)^\\ast \\times (C_0 + C_1) & (C_2 + C_3)^\\ast \\times (C_2 + C_3) \\\\\n",
    "        \\end{pmatrix}\n",
    "    \\end{aligned}\n",
    "    \\quad ...? \n",
    "\\end{equation}\n",
    "\n",
    "We are using a neural network and treating this as an image, so it would not matter if we flip the off-diagonal terms. The latter appears to be correct though, because:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{pmatrix}\n",
    "        a^\\ast_0 \\\\\n",
    "        a^\\ast_1\n",
    "    \\end{pmatrix}\n",
    "    \\otimes\n",
    "    \\begin{pmatrix}\n",
    "        b_0 & b_1\n",
    "    \\end{pmatrix}\n",
    "    \\; = \\;\n",
    "    \\begin{pmatrix}\n",
    "        a^\\ast_0 b_0 & a^\\ast_0 b_1 \\\\\n",
    "        a^\\ast_1 b_0 & a^\\ast_1 b_1\n",
    "    \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Finally, how to efficiently identify which coefficients such as $(C_0 + C_1)$ to sum, in a vectorized manner? All this nonsense is probably a single line in programming code.\n",
    "\n",
    "Some references:\n",
    "* https://arxiv.org/pdf/1601.07458.pdf\n",
    "* http://www.quantum.umb.edu/Jacobs/QMT/QMT_AppendixA.pdf\n",
    "* https://physics.stackexchange.com/questions/179671/how-to-take-partial-trace\n",
    "\n",
    "To construct a reduced density matrix by taking a partial trace of a full density matrix, the dimensions of the matrix goes from $2^L \\times 2^L$ to $2^n \\times 2^n$. This requires a **RECTANGULAR** matrix. However, through the million definitions and tutorials and stackexchange articles I've read, none, **NONE**, explicitly stated how to construct it for the general case. They either kept it in terms of abstract notations, or they have already simplified it to a specific case. **NONE** are usable.\n",
    "\n",
    "To add insult to injury, our system is a bit more complicated than just $ | A \\rangle \\otimes | B \\rangle $. But rather, because we want $n$ consecutive spins in the center, the system is actually $ | B_1 \\rangle \\otimes | A \\rangle \\otimes | B_2 \\rangle $!\n",
    "\n",
    "### My current understanding / implementation of the rectangular matrix is as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbb{B} = | j \\rangle = | 1 \\rangle_{B1} \\otimes \\mathbb{I}_{A} \\otimes | 1 \\rangle_{B2} =\n",
    "    \\begin{pmatrix}\n",
    "        1 \\\\\n",
    "        \\vdots \\\\\n",
    "        1\n",
    "    \\end{pmatrix}\n",
    "    \\otimes\n",
    "    \\begin{pmatrix}\n",
    "        1 & \\dots & 0 \\\\ \n",
    "        \\vdots & \\ddots  & \\vdots \\\\\n",
    "        0 & \\dots & 1\n",
    "    \\end{pmatrix}\n",
    "    \\otimes\n",
    "    \\begin{pmatrix}\n",
    "        1 \\\\\n",
    "        \\vdots \\\\\n",
    "        1\n",
    "    \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Then apply this matrix to the full density matrix:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbb{B}^\\dagger \\mathbb{\\rho} \\mathbb{B}\n",
    "\\end{equation}\n",
    "\n",
    "This assumes the sum over sites of $j$ in subsystem $B$ is whatever-ative, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "    Tr_B(\\rho) = \\sum_j (\\langle B_1 | \\otimes \\langle A | \\otimes \\langle B_2 |) \\; \\rho \\; (| B_1 \\rangle \\otimes | A \\rangle \\otimes | B_2 \\rangle) = (\\langle 1 |_{B1} \\otimes \\mathbb{I}_{A} \\otimes \\langle 1 |_{B2}) \\; \\rho \\; (| 1 \\rangle_{B1} \\otimes \\mathbb{I}_{A} \\otimes | 1 \\rangle_{B2})\n",
    "\\end{equation}\n",
    "\n",
    "# In the end, tensor contraction is used!\n",
    "\n",
    "See function `partial_trace_tensor()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Other resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Given any orthonormal basis sets $|a_i\\rangle$ and $|b_i\\rangle$ for $\\mathcal H_a$ and $\\mathcal H_b$ respectively, any operator $K$ on the space $\\mathcal H_a\\otimes \\mathcal H_b$ can be written:\n",
    "\n",
    "$$K = \\sum_{ij k\\ell} K_{ijk\\ell} |a_i\\rangle |b_j\\rangle \\langle a_k|\\langle b_\\ell|$$\n",
    "\n",
    "where $K_{ijk\\ell} \\equiv \\langle a_i|\\langle b_j| K |a_k\\rangle |b_\\ell\\rangle$, and $\\langle a_i|\\langle b_j| \\equiv \\langle a_i|\\otimes \\langle b_j|$ (I omit the $\\otimes$ for notational clarity).  The partial trace is then defined to be\n",
    "\n",
    "$$\\mathrm{Tr}_b(K):= \\sum_{ik\\ell}K_{i\\ell k\\ell}|a_i\\rangle\\langle a_k|$$\n",
    "\n",
    "which is now a linear operator on $\\mathcal H_a$ alone, with coefficients $$\\bigg(\\mathrm{Tr}_b(K)\\bigg)_{ik} = \\sum_\\ell K_{i\\ell k\\ell}$$\n",
    "\n",
    "Source: https://physics.stackexchange.com/questions/616061/where-does-the-expression-mathrmtrk-sum-j-1n-langle-psi-jk-psi-j/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Test how outer product works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a0 =  1 +  2j\n",
    "a1 =  3 +  5j\n",
    "b0 =  7 + 11j\n",
    "b1 = 13 + 17j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = np.array([1 +  2j,  3 +  5j])\n",
    "b = np.array([7 + 11j, 13 + 17j])\n",
    "np.outer(a.conj(), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(1 -  2j) * (7 + 11j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(1 -  2j) * (13 + 17j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Test bit shift (unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_E     = 10 # #eigenstates, ~100 - ~1000.\n",
    "num_sites = 6  # #n consecutive spins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Source: https://stackoverflow.com/questions/147713/how-do-i-manipulate-bits-in-python\n",
    "def get_bit(value, n):\n",
    "    return ((value >> n & 1) != 0)\n",
    "\n",
    "def set_bit(value, n):\n",
    "    return value | (1 << n)\n",
    "\n",
    "def clear_bit(value, n):\n",
    "    return value & ~(1 << n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(get_bit(32, 4))\n",
    "print(get_bit(32, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def drop_sites(L, sites):\n",
    "    \"\"\"Select sites to drop.\n",
    "    This is only for demonstration. The mechanics is probably wrong.\"\"\"\n",
    "\n",
    "    idx = []\n",
    "    # `i` is the index along an eigenvector.\n",
    "    # `site` is the site to be dropped / summed over while constructing a reduced density matrix.\n",
    "    for i in range(2**L):\n",
    "        # Spins are numbered from left to right.\n",
    "        # But bits are numbered from right to left.\n",
    "        # Hence we need to flip the binary representation.\n",
    "        for site in sites:\n",
    "            print('Check site {} for removal.'.format(site))\n",
    "            print('{:0{}b}'.format(i, L))\n",
    "            if site == 0:\n",
    "                print('^')\n",
    "            else:\n",
    "                print(' ' * (site) + '^')\n",
    "            if get_bit(i, L - site - 1) == 1:\n",
    "                idx.append(i)\n",
    "\n",
    "    return idx\n",
    "\n",
    "print(drop_sites(2, [0]))\n",
    "print('='*25)\n",
    "print(drop_sites(2, [1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Code reduced density and partial trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def get_rho(V):\n",
    "    \"\"\"Computes a full-density matrix from an eigenvector `V`.\"\"\"\n",
    "    return np.outer(V, V.conj())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     37
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_partial_trace_matrix(L, A_sites, B1_sites, B2_sites):\n",
    "    \"\"\"Build a rectangular matrix to take partial trace over subsystem B.\n",
    "    Can only handle n consecutive sites in the middle of the system, surrounded by B1 and B2.\n",
    "    |B1> x |A> x |B2>\n",
    "    \"\"\"\n",
    "\n",
    "    n    = L - len(B1_sites) - len(B2_sites)\n",
    "    I_A  = identity(2**len(A_sites))\n",
    "    # Tr_B = csr_matrix((2**L, 2**n))\n",
    "\n",
    "    if len(B1_sites) != 0:\n",
    "        B1 = np.ones((2**len(B1_sites), 1))\n",
    "        B1 = csr_matrix(B1)\n",
    "    else:\n",
    "        B1 = None\n",
    "    if len(B2_sites) != 0:\n",
    "        B2 = np.ones((2**len(B2_sites), 1))\n",
    "        B2 = csr_matrix(B2)\n",
    "    else:\n",
    "        B2 = None\n",
    "\n",
    "    if B1 is not None:\n",
    "        Tr_B = kron(  B1, I_A, 'csr')\n",
    "    else:\n",
    "        Tr_B = I_A\n",
    "    if B2 is not None:\n",
    "        Tr_B = kron(Tr_B,  B2, 'csr')\n",
    "\n",
    "    # if B1 is not None:\n",
    "    #     print(B1.shape)\n",
    "    # print(I_A.shape)\n",
    "    # if B2 is not None:\n",
    "    #     print(B2.shape)\n",
    "    # print(Tr_B.shape)\n",
    "\n",
    "    return Tr_B\n",
    "\n",
    "def partial_trace_matrix(L, A_sites, B1_sites, B2_sites, V):\n",
    "    \"\"\"Take partial trace over subsystem B using matrix product.\n",
    "    Can only handle n consecutive sites in the middle of the system, surrounded by B1 and B2.\n",
    "    |B1> x |A> x |B2>\n",
    "    \"\"\"\n",
    "\n",
    "    rho = get_rho(V)\n",
    "    Tr_B = build_partial_trace_matrix(L, A_sites, B1_sites, B2_sites)\n",
    "    rho_A = csr_matrix.dot(Tr_B.T.conj(), csr_matrix.dot(rho, Tr_B))\n",
    "\n",
    "    return rho_A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def partial_trace_tensor(L, A_sites, B1_sites, B2_sites, V):\n",
    "    \"\"\"Take partial trace over subsystem B using tensor contraction.\n",
    "    Can only handle n consecutive sites in the middle of the system, surrounded by B1 and B2.\n",
    "    |B1> x |A> x |B2>\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(A_sites)\n",
    "    V = V.reshape([2]*L)\n",
    "    rho_A = np.tensordot(V.conj(), V, axes=(B1_sites+B2_sites, B1_sites+B2_sites))\n",
    "    rho_A = rho_A.reshape((2**n, 2**n))\n",
    "\n",
    "    return rho_A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def partial_trace_kevin(L, A_sites, B1_sites, B2_sites, V):\n",
    "    \"\"\"Take partial trace over subsystem B using tensor contraction, but using Kevin's order of contraction.\n",
    "    Can only handle n consecutive sites in the middle of the system, surrounded by B1 and B2.\n",
    "    |B1> x |A> x |B2>\n",
    "    \"\"\"\n",
    "\n",
    "    V = V.reshape((2**len(B1_sites), 2**len(A_sites), 2**len(B2_sites)))\n",
    "    rho_A = np.tensordot(V.conj(), V, axes=([0, 2], [0, 2]))\n",
    "\n",
    "    return rho_A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def partial_trace_jonas(L, A_sites, B1_sites, B2_sites, V):\n",
    "    \"\"\"Take partial trace over subsystem B using tensor contraction, but using Jonas's implementation.\n",
    "    Can only handle n consecutive sites in the middle of the system, surrounded by B1 and B2.\n",
    "    |B1> x |A> x |B2>\n",
    "    \"\"\"\n",
    "\n",
    "    dm = get_rho(V)\n",
    "\n",
    "    # Jonas' priginal implementation:\n",
    "    # dm_res = dm.reshape(2**sitesA, 2**sitesB, 2**sitesA, 2**sitesB) # rho as 4-tensor\n",
    "    # rhoA = np.trace(dm_res, axis1=1, axis2=3)\n",
    "\n",
    "    # My modification:\n",
    "    dm_res = dm.reshape(2**len(B1_sites), 2**len(A_sites), 2**len(B2_sites), 2**len(B1_sites), 2**len(A_sites), 2**len(B2_sites)) # rho as 6-tensor\n",
    "    dm_res = np.trace(dm_res, axis1=0, axis2=2)\n",
    "    rho_A  = np.trace(dm_res, axis1=1, axis2=3)\n",
    "\n",
    "    return rho_A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Tr_B = build_partial_trace_matrix(2, [0], [1], [])\n",
    "print(Tr_B.shape)\n",
    "print(Tr_B.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Sample Hamiltonian.\n",
    "L  = 8\n",
    "W1 = 0.5\n",
    "W2 = 8\n",
    "J  = 1\n",
    "periodic = False\n",
    "num_Hs = 10\n",
    "\n",
    "H1 = build_H(L, W1, J, periodic=False)\n",
    "E1, V1 = ED(H1.toarray())\n",
    "\n",
    "print('2^L: {}'.format(2**L))\n",
    "print('#eigenvalues: {}'.format(len(E1)))\n",
    "print('E.shape: {}'.format(E1.shape))\n",
    "print('V.shape: {}'.format(V1.shape))\n",
    "\n",
    "H2 = build_H(L, W2, J, periodic=False)\n",
    "E2, V2 = ED(H2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rho = get_rho(V1[:,0])\n",
    "print(rho.shape)\n",
    "print(np.max(rho))\n",
    "max_idx = np.unravel_index(rho.argmax(), rho.shape) # 2D index of np.argmax().\n",
    "print(max_idx)\n",
    "\n",
    "pos = max_idx[0] # Which 3x3 sub-matrix along the diagonal to print.\n",
    "print(rho[pos-1:pos+2, pos-1:pos+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Compare matrix-generated rho_A vs tensor-generated rho_A.\n",
    "A_sites  = [1,2,3,4,5,6] # Keep n consecutive\n",
    "B1_sites = [0]\n",
    "B2_sites = [7]\n",
    "rho_A_mat1 = partial_trace_matrix(L, A_sites, B1_sites, B2_sites, V1[:,0])\n",
    "rho_A_mat2 = partial_trace_matrix(L, A_sites, B1_sites, B2_sites, V2[:,0])\n",
    "print('Theoretical size of reduced density matrix rho_A: {}'.format(2**len(A_sites)))\n",
    "print('Shape of computed rho_A: {}'.format(rho_A_mat1.shape))\n",
    "print('Size of computed rho_A: {}'.format(rho_A_mat1.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rho_A_ten1 = partial_trace_tensor(L, A_sites, B1_sites, B2_sites, V1[:,0])\n",
    "rho_A_ten2 = partial_trace_tensor(L, A_sites, B1_sites, B2_sites, V2[:,0])\n",
    "print('Theoretical size of reduced density matrix rho_A: {}'.format(2**len(A_sites)))\n",
    "print('Shape of computed rho_A: {}'.format(rho_A_ten1.shape))\n",
    "print('Size of computed rho_A: {}'.format(rho_A_ten1.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rho_A_kev1 = partial_trace_kevin(L, A_sites, B1_sites, B2_sites, V1[:,0])\n",
    "rho_A_kev2 = partial_trace_kevin(L, A_sites, B1_sites, B2_sites, V2[:,0])\n",
    "print('Theoretical size of reduced density matrix rho_A: {}'.format(2**len(A_sites)))\n",
    "print('Shape of computed rho_A: {}'.format(rho_A_kev1.shape))\n",
    "print('Size of computed rho_A: {}'.format(rho_A_kev1.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rho_A_jon1 = partial_trace_jonas(L, A_sites, B1_sites, B2_sites, V1[:,0])\n",
    "rho_A_jon2 = partial_trace_jonas(L, A_sites, B1_sites, B2_sites, V2[:,0])\n",
    "print('Theoretical size of reduced density matrix rho_A: {}'.format(2**len(A_sites)))\n",
    "print('Shape of computed rho_A: {}'.format(rho_A_jon1.shape))\n",
    "print('Size of computed rho_A: {}'.format(rho_A_jon1.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check Kevin's implementation and mine.\n",
    "assert np.allclose(rho_A_ten1, rho_A_kev1)\n",
    "assert np.allclose(rho_A_ten2, rho_A_kev2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "rho_A_ten1 = partial_trace_tensor(L, A_sites, B1_sites, B2_sites, V1[:,0])\n",
    "rho_A_ten2 = partial_trace_tensor(L, A_sites, B1_sites, B2_sites, V2[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "rho_A_kev1 = partial_trace_kevin(L, A_sites, B1_sites, B2_sites, V1[:,0])\n",
    "rho_A_kev2 = partial_trace_kevin(L, A_sites, B1_sites, B2_sites, V2[:,0])\n",
    "# Kevin is faster by 10-20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(fig_w/dpi,fig_h/dpi*2), dpi=dpi, squeeze=False)\n",
    "\n",
    "im1 = axes[0, 0].imshow(np.abs(rho_A_mat1))\n",
    "im2 = axes[0, 1].imshow(np.abs(rho_A_ten1))\n",
    "im3 = axes[1, 0].imshow(np.abs(rho_A_mat2))\n",
    "im4 = axes[1, 1].imshow(np.abs(rho_A_ten2))\n",
    "\n",
    "axes[0, 0].set_title('$W=0.5 \\quad \\\\rho_A$ computed using matrix (seems wrong)')\n",
    "axes[0, 1].set_title('$W=0.5 \\quad \\\\rho_A$ computed using tensor')\n",
    "axes[1, 0].set_title('$W=8.0 \\quad \\\\rho_A$ computed using matrix (almost correct)')\n",
    "axes[1, 1].set_title('$W=8.0 \\quad \\\\rho_A$ computed using tensor')\n",
    "\n",
    "# plt.colorbar(im1, ax=axes[0, 0])#.set_label('Entropy')\n",
    "# plt.colorbar(im2, ax=axes[0, 1])#.set_label('Entropy')\n",
    "\n",
    "for axe in axes:\n",
    "    for ax in axe:\n",
    "        pass\n",
    "        # ax.set_title('Both $\\\\rho_A$ are the same')\n",
    "        # ax.legend(loc='best')\n",
    "        # ax.xaxis.set_ticklabels([])\n",
    "        # ax.yaxis.set_ticklabels([])\n",
    "        # ax.xaxis.set_visible(False)\n",
    "        # ax.yaxis.set_visible(False)\n",
    "\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(fig_w/dpi,fig_h/dpi*2), dpi=dpi, squeeze=False)\n",
    "\n",
    "im1 = axes[0, 0].imshow(np.abs(rho_A_kev1))\n",
    "im2 = axes[0, 1].imshow(np.abs(rho_A_ten1))\n",
    "im3 = axes[1, 0].imshow(np.abs(rho_A_kev2))\n",
    "im4 = axes[1, 1].imshow(np.abs(rho_A_ten2))\n",
    "\n",
    "axes[0, 0].set_title('$W=0.5 \\quad \\\\rho_A$ computed using Kevin')\n",
    "axes[0, 1].set_title('$W=0.5 \\quad \\\\rho_A$ computed using tensor')\n",
    "axes[1, 0].set_title('$W=8.0 \\quad \\\\rho_A$ computed using Kevin')\n",
    "axes[1, 1].set_title('$W=8.0 \\quad \\\\rho_A$ computed using tensor')\n",
    "\n",
    "# plt.colorbar(im1, ax=axes[0, 0])#.set_label('Entropy')\n",
    "# plt.colorbar(im2, ax=axes[0, 1])#.set_label('Entropy')\n",
    "\n",
    "for axe in axes:\n",
    "    for ax in axe:\n",
    "        pass\n",
    "        # ax.set_title('Both $\\\\rho_A$ are the same')\n",
    "        # ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(fig_w/dpi,fig_h/dpi*2), dpi=dpi, squeeze=False)\n",
    "\n",
    "im1 = axes[0, 0].imshow(np.abs(rho_A_jon1))\n",
    "im2 = axes[0, 1].imshow(np.abs(rho_A_ten1))\n",
    "im3 = axes[1, 0].imshow(np.abs(rho_A_jon2))\n",
    "im4 = axes[1, 1].imshow(np.abs(rho_A_ten2))\n",
    "\n",
    "axes[0, 0].set_title('$W=0.5 \\quad \\\\rho_A$ computed using Jonas')\n",
    "axes[0, 1].set_title('$W=0.5 \\quad \\\\rho_A$ computed using tensor')\n",
    "axes[1, 0].set_title('$W=8.0 \\quad \\\\rho_A$ computed using Jonas')\n",
    "axes[1, 1].set_title('$W=8.0 \\quad \\\\rho_A$ computed using tensor')\n",
    "\n",
    "# plt.colorbar(im1, ax=axes[0, 0])#.set_label('Entropy')\n",
    "# plt.colorbar(im2, ax=axes[0, 1])#.set_label('Entropy')\n",
    "\n",
    "for axe in axes:\n",
    "    for ax in axe:\n",
    "        pass\n",
    "        # ax.set_title('Both $\\\\rho_A$ are the same')\n",
    "        # ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Batch generate data and visualize reduced density matrices for different W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def batch_gen_rho_data_core(L, Ws, J=1, periodic=False, num_Hs=1000, num_EV=5, max_n=99999, clamp_zero=1e-32):\n",
    "\n",
    "    rho_As = {}\n",
    "\n",
    "    for W in Ws:\n",
    "\n",
    "        Hs = build_Hs(L, W, J, periodic, num_Hs)\n",
    "        E0s, V0s = EDs_sparse(Hs, num_EV)\n",
    "\n",
    "        for E0, V0 in tqdm(zip(E0s, V0s), leave=False, desc='partial_trace()'): # Still `num_EV` eigenvectors per E0, V0.\n",
    "            for i in range(len(E0)):\n",
    "\n",
    "                A_sites  = list(range(1, L-1)) # Keep n consecutive sites, where n = L - 2.\n",
    "                A_sites0 = A_sites\n",
    "                parity   = 0\n",
    "\n",
    "                while len(A_sites) != 0:\n",
    "\n",
    "                    n = len(A_sites)\n",
    "\n",
    "                    # Only compute reduced density matrix is size is small.\n",
    "                    if n <= max_n:\n",
    "\n",
    "                        B1_sites = list(range(0, A_sites[0]))\n",
    "                        B2_sites = list(range(A_sites[-1]+1, L))\n",
    "                        rho_A    = partial_trace_kevin(L, A_sites, B1_sites, B2_sites, V0[:,i])\n",
    "\n",
    "                        # Investigate how many data points are actually zero.\n",
    "                        # if n > 5:\n",
    "                        #     print('W:', W)\n",
    "                        #     print('rho_A <= 2'    , (np.abs(rho_A) <= 2    ).sum())\n",
    "                        #     print('rho_A <= 1'    , (np.abs(rho_A) <= 1    ).sum())\n",
    "                        #     print('rho_A <= 1e-8' , (np.abs(rho_A) <= 1e-8 ).sum())\n",
    "                        #     print('rho_A <= 1e-12', (np.abs(rho_A) <= 1e-12).sum())\n",
    "                        #     print('rho_A <= 1e-16', (np.abs(rho_A) <= 1e-16).sum())\n",
    "                        #     print('rho_A <= 1e-20', (np.abs(rho_A) <= 1e-20).sum())\n",
    "                        #     print('rho_A <= 1e-32', (np.abs(rho_A) <= 1e-32).sum())\n",
    "                        #     print('rho_A == 0', (np.abs(rho_A) == 0).sum())\n",
    "                        #     rho_A[np.abs(rho_A) <= 1e-32] = 0\n",
    "                        #     print('rho_A == 0', (np.abs(rho_A) == 0).sum())\n",
    "\n",
    "                        # If the magnitude of data is below a certain threshold, set it to zero.\n",
    "                        if clamp_zero is not None:\n",
    "                            rho_A[np.abs(rho_A) <= clamp_zero] = 0\n",
    "                        \n",
    "                        # Check if imaginary part is negligible.\n",
    "                        # Conclusion: They are basically 1e-16.\n",
    "                        # if n > 5:\n",
    "                        #     print('W:', W)\n",
    "                        #     print('Max rho_A:', np.max(np.abs(rho_A)))\n",
    "                        #     print('Max real rho_A:', np.max(np.abs(np.real(rho_A))))\n",
    "                        #     print('Max imag rho_A:', np.max(np.abs(np.imag(rho_A))))\n",
    "\n",
    "                        if n not in rho_As:\n",
    "                            rho_As[n] = []\n",
    "                        rho_As[n].append([rho_A.astype(np.float32), W])\n",
    "\n",
    "                    # Shorten n:\n",
    "                    if parity % 3 == 0:\n",
    "                        A_sites  = A_sites0[1:]\n",
    "                        parity  += 1\n",
    "                    elif parity % 3 == 1:\n",
    "                        A_sites  = A_sites0[:-1]\n",
    "                        parity  += 1\n",
    "                    else:\n",
    "                        A_sites  = A_sites0[1:-1]\n",
    "                        A_sites0 = A_sites\n",
    "                        parity   = 0\n",
    "                    # print(L, len(A_sites0), len(A_sites), parity, flush=True)\n",
    "\n",
    "    return rho_As\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def batch_gen_rho_data_main(L, Ws, J=1, periodic=False, num_Hs=1000, num_EV=5, max_n=99999, clamp_zero=1e-32, save_data=True):\n",
    "    \"\"\"Generate a list of reduced density matrices with W = {0.5, 8}, and save them to file.\n",
    "    The data is used to train a classifier neutral network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    L : int\n",
    "        System size.\n",
    "    Ws : list of float\n",
    "        A list of disorder strength W to realize.\n",
    "    J : float\n",
    "        Coupling strength. Always set to 1.\n",
    "    periodic : bool\n",
    "        Whether the Hamiltonian is periodic.\n",
    "    num_Hs : int\n",
    "        Number of Hamiltonians to generate, per disorder strength W.\n",
    "    num_EV : int\n",
    "        Number of eigenvalues around zero to use as samples.\n",
    "    max_n : int\n",
    "        Maximum number of consecutive sites n to store.\n",
    "        Values beyond 8 will consume a GB of storage per 10000 samples.\n",
    "        Recommended: 8.\n",
    "    clamp_zero : float\n",
    "        Threshold of reduced density matrix, magnitudes below which will be clamped to zero.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    rho_As : dict of list of list\n",
    "        A `dict` of keys `n`, which are consecutive spins around the middle of the system, that the reduced density matrix is computed.\n",
    "        The value of the dict is a list of lists, where each reduced density matrix is paired with its disorder strength W.\n",
    "        i.e. rho_As[6][i][0] is a 2D numpy.ndarray of the reduced density matrix, and rho_As[6][i][1] is the disorder strength used to generate it.\n",
    "        Number of data must be a multiple of 10.\n",
    "    \"\"\"\n",
    "\n",
    "    rho_As = batch_gen_rho_data_core(L, Ws, J, periodic, num_Hs, num_EV, max_n, clamp_zero)\n",
    "\n",
    "    if save_data:\n",
    "        for n, rho_A in rho_As.items():\n",
    "            # For odd `n`, #samples = num_Hs * num_EV * 2\n",
    "            # For even `n`, #samples = num_Hs * num_EV\n",
    "            print('Writing {: 5d} `rho_A` of system size L = {:02d} of n = {:02d}.'.format(len(rho_A), L, n), flush=True)\n",
    "            save_rho_train(rho_A, 'rho_A', L, n, periodic, num_EV)\n",
    "\n",
    "    return rho_As\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def batch_gen_rho_data_rand(L, Ws, J=1, periodic=False, num_Hs=1000, num_EV=5, max_n=99999, clamp_zero=1e-32, save_data=True):\n",
    "    \"\"\"Generate a list of reduced density matrices with W != {0.5, 8}, and save them to file.\n",
    "    The data is used to predict transition disorder strength W_c.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    L : int\n",
    "        System size.\n",
    "    Ws : list of float\n",
    "        A list of disorder strength W to realize.\n",
    "    J : float\n",
    "        Coupling strength. Always set to 1.\n",
    "    periodic : bool\n",
    "        Whether the Hamiltonian is periodic.\n",
    "    num_Hs : int\n",
    "        Number of Hamiltonians to generate, per disorder strength W.\n",
    "    num_EV : int\n",
    "        Number of eigenvalues around zero to use as samples.\n",
    "    max_n : int\n",
    "        Maximum number of consecutive sites n to store.\n",
    "        Values beyond 8 will consume a GB of storage per 10000 samples.\n",
    "        Recommended: 8.\n",
    "    clamp_zero : float\n",
    "        Threshold of reduced density matrix, magnitudes below which will be clamped to zero.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    rho_As : dict of list of list\n",
    "        A `dict` of keys `n`, which are consecutive spins around the middle of the system, that the reduced density matrix is computed.\n",
    "        The value of the dict is a list of lists, where each reduced density matrix is paired with its disorder strength W.\n",
    "        i.e. rho_As[6][i][0] is a 2D numpy.ndarray of the reduced density matrix, and rho_As[6][i][1] is the disorder strength used to generate it.\n",
    "        Number of data must be a multiple of 10.\n",
    "    \"\"\"\n",
    "\n",
    "    rho_As = batch_gen_rho_data_core(L, Ws, J, periodic, num_Hs, num_EV, max_n, clamp_zero)\n",
    "\n",
    "    if save_data:\n",
    "        for n, rho_A in rho_As.items():\n",
    "            # For odd `n`, #samples = num_Hs * num_EV * len(Ws) * 2\n",
    "            # For even `n`, #samples = num_Hs * num_EV * len(Ws)\n",
    "            print('Writing {: 5d} `rho_A` of system size L = {:02d} of n = {:02d}.'.format(len(rho_A), L, n), flush=True)\n",
    "            save_rho_valid(rho_A, 'rho_A', L, n, periodic, num_EV)\n",
    "\n",
    "    return rho_As\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_samples = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test drawing images of reduced density matrix.\n",
    "J  = 1                      # Always = 1\n",
    "Ws = [0.5, 8]               # Disorder strength W.\n",
    "Ls = [8]                    # System size L.\n",
    "Hs = [base_samples]         # Number of samples per L per W.\n",
    "ps = [False]                # Periodic or not.\n",
    "et = []                     # Execution time.\n",
    "num_EV = 5                  # Number of eigenvalues near zero to save.\n",
    "rho_As_dict = {}\n",
    "\n",
    "for L, num_Hs in zip(Ls, Hs):\n",
    "    for p in ps:\n",
    "        start_time = time.time()\n",
    "        rho_As = batch_gen_rho_data_main(L, Ws, J, p, num_Hs, num_EV, max_n=10, save_data=False)\n",
    "        for n, rho_A in rho_As.items():\n",
    "            if n not in rho_As_dict:\n",
    "                rho_As_dict[n] = []\n",
    "            rho_As_dict[n] = rho_As_dict[n] + rho_A\n",
    "        exec_time = time.time() - start_time\n",
    "        et.append(exec_time)\n",
    "        print('Computed: L={:02d} | periodic={: <5}.'.format(L, str(p)))\n",
    "        print('Execution took {: 8.2f}s or {: 6.2f}min.'.format(exec_time, exec_time/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test drawing images of reduced density matrix.\n",
    "J  = 1                      # Always = 1\n",
    "Ws = np.random.uniform(0.1, high=9.9, size=(2 * base_samples,))               # Disorder strength W.\n",
    "Ls = [8]                    # System size L.\n",
    "Hs = [1]                    # Number of samples per L per W.\n",
    "ps = [False]                # Periodic or not.\n",
    "et = []                     # Execution time.\n",
    "num_EV = 5                  # Number of eigenvalues near zero to save.\n",
    "rho_As_rand = {}\n",
    "\n",
    "for L, num_Hs in zip(Ls, Hs):\n",
    "    for p in ps:\n",
    "        start_time = time.time()\n",
    "        rho_As = batch_gen_rho_data_rand(L, Ws, J, p, num_Hs, num_EV, max_n=10, save_data=False)\n",
    "        for n, rho_A in rho_As.items():\n",
    "            if n not in rho_As_rand:\n",
    "                rho_As_rand[n] = []\n",
    "            rho_As_rand[n] = rho_As_rand[n] + rho_A\n",
    "        exec_time = time.time() - start_time\n",
    "        et.append(exec_time)\n",
    "        print('Computed: L={:02d} | periodic={: <5}.'.format(L, str(p)))\n",
    "        print('Execution took {: 8.2f}s or {: 6.2f}min.'.format(exec_time, exec_time/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n = 6\n",
    "sample_idx = np.random.randint(0, len(rho_As_dict[n]), size=5*5)\n",
    "\n",
    "fig, axes = plt.subplots(5, 5, figsize=(fig_w/dpi,fig_h/dpi*2), dpi=dpi, squeeze=False)\n",
    "\n",
    "for i, idx in enumerate(sample_idx):\n",
    "    axes[i%5,i//5].imshow(np.abs(rho_As_dict[n][idx][0]))\n",
    "    axes[i%5,i//5].annotate('W={:3.1f}'.format(rho_As_dict[n][idx][1]), (0.5,0.5), xycoords='axes fraction', ha='center', color='w')\n",
    "\n",
    "for axe in axes:\n",
    "    for ax in axe:\n",
    "        # ax.legend(loc='best')\n",
    "        ax.xaxis.set_ticklabels([])\n",
    "        ax.yaxis.set_ticklabels([])\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n = 6\n",
    "sample_idx = np.random.randint(0, len(rho_As_rand[n]), size=5*5)\n",
    "\n",
    "fig, axes = plt.subplots(5, 5, figsize=(fig_w/dpi,fig_h/dpi*2), dpi=dpi, squeeze=False)\n",
    "\n",
    "for i, idx in enumerate(sample_idx):\n",
    "    axes[i%5,i//5].imshow(np.abs(rho_As_rand[n][idx][0]))\n",
    "    axes[i%5,i//5].annotate('W={:3.1f}'.format(rho_As_rand[n][idx][1]), (0.5,0.5), xycoords='axes fraction', ha='center', color='w')\n",
    "\n",
    "for axe in axes:\n",
    "    for ax in axe:\n",
    "        # ax.legend(loc='best')\n",
    "        ax.xaxis.set_ticklabels([])\n",
    "        ax.yaxis.set_ticklabels([])\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch generate data (execution part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Estimated runtime to generate 10,000 samples per W:')\n",
    "for i in range(7):\n",
    "    print('For L = {:2d}, est runtime {:3d} min or {: 2.2f} hrs.'.format(8+i, 7 * 2**i, 7 * 2**i / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Estimated runtime to generate 100,000 samples per W:')\n",
    "for i in range(7):\n",
    "    print('For L = {:2d}, est runtime {:4d} min or {:5.2f} hrs.'.format(8+i, 70 * 2**i, 70 * 2**i / 60))\n",
    "print(' ')\n",
    "print('It will therefore be safer to break down generation to segments of 1-hr:')\n",
    "for i in range(7):\n",
    "    print('For L = {:2d}, separate execution into {:2d} batches with {:6d} samples each.'.format(8+i, 2**i, 100000 // 2**i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Batch generate reduced density matrix.\n",
    "\n",
    "k = 5\n",
    "batches = 1\n",
    "base_sample = 10000 // k // batches # Divide by k (num_EV)\n",
    "rand_sample = 50            # Samples per random W.\n",
    "Ws_main = [0.5, 8]          # Disorder strength W.\n",
    "Ws_rand = np.random.uniform(0.1, high=5.9, size=(2 * base_sample // rand_sample,))\n",
    "J  = 1                      # Always = 1\n",
    "Ls = list(range(8,15,2))      # System sizes L.\n",
    "ps = [False, True]          # Periodic or not.\n",
    "et = []                     # Execution time.\n",
    "Hs_main = [base_sample]*len(Ls) # Number of samples per L per W.\n",
    "Hs_rand = [rand_sample]*len(Ls) # Number of samples per L per W.\n",
    "num_EVs = [k]               # Number of eigenvalues near zero to save.\n",
    "\n",
    "for i in range(batches):\n",
    "    print('{} | Processing batch {:03d} of {:d}:'.format(dt(), i+1, batches), flush=True)\n",
    "    print(' ', flush=True)\n",
    "    for L, num_Hs_m, num_Hs_r in zip(Ls, Hs_main, Hs_rand):\n",
    "        for num_EV in num_EVs:\n",
    "            for p in ps:\n",
    "                start_time = time.time()\n",
    "                print('{} | Generating training data for L={:02d}...'.format(dt(), L), flush=True)\n",
    "                batch_gen_rho_data_main(L, Ws_main, J, p, num_Hs_m, num_EV, max_n=6, clamp_zero=1e-32, save_data=True)\n",
    "                print('{} | Generating random data for L={:02d}...'.format(dt(), L), flush=True)\n",
    "                batch_gen_rho_data_rand(L, Ws_rand, J, p, num_Hs_r, num_EV, max_n=6, clamp_zero=1e-32, save_data=True)\n",
    "                exec_time = time.time() - start_time\n",
    "                et.append(exec_time)\n",
    "                print('{} | Computed: L={:02d} | num_EV={} | periodic={: <5}.'.format(dt(), L, num_EV, str(p)), flush=True)\n",
    "                print('{} | Execution took {: 8.2f}s or {: 6.2f}min.'.format(dt(), exec_time, exec_time/60), flush=True)\n",
    "                print(' ', flush=True)\n",
    "\n",
    "    if check_shutdown_signal():\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "See the other notebook for training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_rho_train('rho_A', L=8, n=4, periodic=False, num_EV=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data) # [[rho_A, W], [rho_A, W], [rho_A, W], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[0]) # [rho_A, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit",
   "language": "python",
   "name": "python37264bit22d8f94fb4124d8cb7bc86dc616da5cb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
