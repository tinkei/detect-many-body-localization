{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_colab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 13*: Machine Learning of Many Body Localization (Exact diagonalization + Machine Learning)\n",
    "\n",
    "Use exact diagonalization to obtain all eigenstates of the the Heisenberg model with a\n",
    "random field, \n",
    "\n",
    "\\begin{equation}\n",
    "    H = J \\sum_i \\vec{S}_{i} \\cdot \\vec{S}_{i+1} - \\sum_i h_i S^z_i\n",
    "\\end{equation}\n",
    "\n",
    ", where the values of the field $ h_i \\in [-W, W] $ are chosen from a uniform random distribution with a \"disorder strength\" $W$ (Use moderate system sizes $L = [10, 12]$). \n",
    "\n",
    "The exciting property of this model is that it is believed to undergo a phase transition from an extended phase (small $W$) to a localized phase (large $W$). \n",
    "\n",
    "We will use ML to detect this transition: Pick a number of eigenstates that are near energy $E = 0$ and obtain the reduced density matrices $\\rho^A$, where $A$ is a region of $n$ consecutive spins (a few hundred to thousands eigenstates for different disorder realizations). \n",
    "\n",
    "Now use the density matrices for $W = 0.5 J$ and $W = 8.0 J$ to train a neural network (just interpret the entries of $\\rho^A$ as an image with $2^n \\times 2^n$ pixel). Then use this network and study the output of the neural network for different $W$. \n",
    "\n",
    "How does the results depend on system size $L$ and block size $n$? At which $W_c$ do you expect the\n",
    "transition to occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Author: Tin Kei CHENG_  \n",
    "_TUM, 2021_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numba import jit, njit # Set \"nopython\" mode for best performance, equivalent to @njit # cache=True, parallel=True\n",
    "from datetime import datetime\n",
    "\n",
    "tz = pytz.timezone('Europe/Berlin')\n",
    "\n",
    "import scipy\n",
    "import scipy.linalg\n",
    "import scipy.sparse.linalg\n",
    "\n",
    "from scipy.sparse import csr_matrix, kron, identity\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.linalg import svd\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "dpi = 100\n",
    "fig_w = 1280\n",
    "fig_h = 640\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if in_colab:\n",
    "    ED_data_dir = 'drive/MyDrive/Colab Data/CMMP/ED_data'\n",
    "    rho_train_data_dir = 'drive/MyDrive/Colab Data/CMMP/rho_train_data'\n",
    "    rho_valid_data_dir = 'drive/MyDrive/Colab Data/CMMP/rho_valid_data'\n",
    "    model_dir  = 'drive/MyDrive/Colab Data/CMMP/models'\n",
    "    signal_dir = 'drive/MyDrive/Colab Data/CMMP'\n",
    "    eval_valid_data_dir = 'drive/MyDrive/Colab Data/CMMP/eval_valid_data'\n",
    "else:\n",
    "    ED_data_dir = 'ED_data'\n",
    "    rho_train_data_dir = 'rho_train_data'\n",
    "    rho_valid_data_dir = 'rho_valid_data'\n",
    "    model_dir  = 'models'\n",
    "    signal_dir = '.'\n",
    "    eval_valid_data_dir = 'eval_valid_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if in_colab:\n",
    "    !cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if in_colab:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if in_colab:\n",
    "    !pip install pytorch_lightning==0.7.6 torchsummary==1.5.1\n",
    "    !pip install torch==1.4.0+cu92 torchvision==0.5.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pip install torch==1.4.0+cu92 torchsummary==1.5.1 torchvision==0.5.0+cu92 pytorch-lightning==0.7.6  -f https://download.pytorch.org/whl/torch_stable.html\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "# from torchsummary import summary\n",
    "# help(summary)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(device)\n",
    "# python -c \"import torch; print(torch.__version__)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3,
     13,
     35,
     59,
     85,
     118
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def dt():\n",
    "    return datetime.now(tz=tz).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def dict_to_str(_dict):\n",
    "    \n",
    "    od = OrderedDict(sorted(_dict.items())) # Sort keys.\n",
    "    s = json.dumps(od) # Turn dict into str.\n",
    "    s = s[1:-1].replace('\\\"', '').replace(' ', '') # Replace some special characeters.\n",
    "    s = ''.join(x if x.isalnum() else ('=' if x == ':' else '_') for x in s) # Replace all remaining special characters.\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def save_cache(obj, obj_name, obj_params, cache_dir='cache'):\n",
    "    \"\"\"Cache an object, together with the parameters used to generate it.\n",
    "    For `obj_params`, try not to use nested dict or with complicated objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj : object\n",
    "        An `object` you want to cache.\n",
    "    obj_name : str\n",
    "        A unique name you give to this object.\n",
    "    obj_params : dict\n",
    "        A `dict` of all parameters necessary to generate this object.\n",
    "    cache_dir : str, optional\n",
    "        Directory where the cache is located.\n",
    "    \"\"\"\n",
    "\n",
    "    param_str = dict_to_str(obj_params)\n",
    "    os.makedirs(os.path.join(cache_dir, obj_name), exist_ok=True)\n",
    "    with gzip.open(os.path.join(cache_dir, obj_name, param_str + '.pkl.gz'), 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_cache(obj_name, obj_params, cache_dir='cache'):\n",
    "    \"\"\"Check if the object is cached. If not, return None.\n",
    "    For `obj_params`, try not to use nested dict or with complicated objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj_name : str\n",
    "        A unique name you give to this object.\n",
    "    obj_params : dict\n",
    "        A `dict` of all parameters necessary to generate this object.\n",
    "    cache_dir : str, optional\n",
    "        Directory where the cache is located.\n",
    "    \"\"\"\n",
    "\n",
    "    param_str = dict_to_str(obj_params)\n",
    "    os.makedirs(os.path.join(cache_dir, obj_name), exist_ok=True)\n",
    "    if os.path.isfile(os.path.join(cache_dir, obj_name, param_str + '.pkl.gz')):\n",
    "        with gzip.open(os.path.join(cache_dir, obj_name, param_str + '.pkl.gz'), 'rb') as handle:\n",
    "            obj = pickle.load(handle)\n",
    "            return obj\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def check_shutdown_signal(signal_dir=signal_dir):\n",
    "    \"\"\"To gracefully stop generating data by making sure a loop is completed, this function will read a text file in a directory for the value `1`.\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    shutdown : bool\n",
    "        Shutdown signal detected.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(os.path.join(signal_dir), exist_ok=True)\n",
    "    if os.path.isfile(os.path.join(signal_dir, 'shutdown_signal.txt')):\n",
    "        with open(os.path.join(signal_dir, 'shutdown_signal.txt')) as f:\n",
    "            lines = f.readlines()\n",
    "        if lines is not None and len(lines) > 0:\n",
    "            lines = [x.strip() for x in lines]\n",
    "            if lines[0] == '1':\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "@njit\n",
    "def is_sorted(arr):\n",
    "    return np.all(arr[:-1] <= arr[1:])\n",
    "\n",
    "\n",
    "def save_ED(obj, obj_name, L, W, periodic, data_dir=ED_data_dir):\n",
    "    \"\"\"Save a list of exact diagonalization results, organized by the parameters used to generate them.\n",
    "    For `obj_params`, try not to use nested dict or with complicated objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj : list\n",
    "        A list of lists, where each reduced density matrix is paired with its disorder strength W.\n",
    "        Number of data must be a multiple of 10.\n",
    "    obj_name : str\n",
    "        A unique name you give to this object. Call it `rho_A`.\n",
    "    L : int\n",
    "        System size.\n",
    "    W : float\n",
    "        Disorder strength.\n",
    "    periodic : bool\n",
    "        Whether the Hamiltonian is periodic.\n",
    "    data_dir : str, optional\n",
    "        Directory where the data is saved.\n",
    "    \"\"\"\n",
    "\n",
    "    directory = os.path.join(data_dir, obj_name, 'L={:02d}'.format(L), 'W={:.2f}'.format(W), 'periodic={}'.format(periodic))\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Check if file exists, and increment suffix.\n",
    "    i = 0\n",
    "    while os.path.exists(os.path.join(directory, obj_name + '-{:09d}.pkl.gz'.format(i))):\n",
    "        i += 1\n",
    "\n",
    "    with gzip.open(os.path.join(directory, obj_name + '-{:09d}.pkl.gz'.format(i)), 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_ED(obj_name, L, W, periodic, data_dir=ED_data_dir):\n",
    "    \"\"\"Check if the object is cached. If not, return None.\n",
    "    For `obj_params`, try not to use nested dict or with complicated objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj_name : str\n",
    "        A unique name you give to this object. Call it `rho_A`.\n",
    "    L : int\n",
    "        System size.\n",
    "    W : float\n",
    "        Disorder strength.\n",
    "    periodic : bool\n",
    "        Whether the Hamiltonian is periodic.\n",
    "    data_dir : str, optional\n",
    "        Directory where the data is saved.\n",
    "    \"\"\"\n",
    "\n",
    "    raise NotImplementedError('Function not implemented.')\n",
    "\n",
    "    param_str = dict_to_str(obj_params)\n",
    "    os.makedirs(os.path.join(data_dir, obj_name, 'L={:2d}'.format(L), 'W={:.2d}'.format(W)), exist_ok=True)\n",
    "    if os.path.isfile(os.path.join(data_dir, obj_name, 'L={:2d}'.format(L), 'W={:.2d}'.format(W), param_str + '.pkl.gz')):\n",
    "        with gzip.open(os.path.join(data_dir, obj_name, 'L={:2d}'.format(L), 'W={:.2d}'.format(W), param_str + '.pkl.gz'), 'rb') as handle:\n",
    "            obj = pickle.load(handle)\n",
    "            return obj\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     35
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def save_rho_train(obj, obj_name, L, n, periodic, num_EV, data_dir=rho_train_data_dir):\n",
    "    \"\"\"Save a list of reduced density matrices with W = {0.5, 8}.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj : list\n",
    "        A list of lists, where each reduced density matrix is paired with its disorder strength W.\n",
    "        i.e. obj[i][0] is a 2D numpy.ndarray of the reduced density matrix, and obj[i][1] is the disorder strength used to generate it.\n",
    "        Number of data must be a multiple of 10.\n",
    "    obj_name : str\n",
    "        A name you give to this object. Call it `rho_A`.\n",
    "    L : int\n",
    "        System size.\n",
    "    n : int\n",
    "        Number of consecutive spins sampled.\n",
    "    periodic : bool\n",
    "        Whether the Hamiltonian is periodic.\n",
    "    num_EV : int\n",
    "        Number of eigenvalues around zero being sampled.\n",
    "    data_dir : str, optional\n",
    "        Directory where the data is saved.\n",
    "    \"\"\"\n",
    "\n",
    "    directory = os.path.join(data_dir, 'L={:02d}'.format(L), 'n={:02d}'.format(n), 'periodic={}'.format(periodic), 'num_EV={}'.format(num_EV))\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Check if file exists, and increment suffix.\n",
    "    i = 0\n",
    "    while os.path.exists(os.path.join(directory, obj_name + '-{:09d}.pkl.gz'.format(i))):\n",
    "        i += 1\n",
    "\n",
    "    with gzip.open(os.path.join(directory, obj_name + '-{:09d}.pkl.gz'.format(i)), 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def save_rho_valid(obj, obj_name, L, n, periodic, num_EV, data_dir=rho_valid_data_dir):\n",
    "    \"\"\"Save a list of reduced density matrices with random W != {0.5, 8}.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj : list\n",
    "        A list of lists, where each reduced density matrix is paired with its disorder strength W.\n",
    "        i.e. obj[i][0] is a 2D numpy.ndarray of the reduced density matrix, and obj[i][1] is the disorder strength used to generate it.\n",
    "        Number of data must be a multiple of 10.\n",
    "    obj_name : str\n",
    "        A name you give to this object. Call it `rho_A`.\n",
    "    L : int\n",
    "        System size.\n",
    "    n : int\n",
    "        Number of consecutive spins sampled.\n",
    "    periodic : bool\n",
    "        Whether the Hamiltonian is periodic.\n",
    "    num_EV : int\n",
    "        Number of eigenvalues around zero being sampled.\n",
    "    data_dir : str, optional\n",
    "        Directory where the data is saved.\n",
    "    \"\"\"\n",
    "\n",
    "    directory = os.path.join(data_dir, 'L={:02d}'.format(L), 'n={:02d}'.format(n), 'periodic={}'.format(periodic), 'num_EV={}'.format(num_EV))\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Check if file exists, and increment suffix.\n",
    "    i = 0\n",
    "    while os.path.exists(os.path.join(directory, obj_name + '-{:09d}.pkl.gz'.format(i))):\n",
    "        i += 1\n",
    "\n",
    "    with gzip.open(os.path.join(directory, obj_name + '-{:09d}.pkl.gz'.format(i)), 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     40
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_rho_train(obj_name, L, n, periodic, num_EV, data_dir=rho_train_data_dir):\n",
    "    \"\"\"Load a list of reduced density matrices with W = {0.5, 8}.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj_name : str\n",
    "        A name you give to this object. Call it `rho_A`.\n",
    "    L : int\n",
    "        System size.\n",
    "    n : int\n",
    "        Number of consecutive spins sampled.\n",
    "    periodic : bool\n",
    "        Whether the Hamiltonian is periodic.\n",
    "    num_EV : int\n",
    "        Number of eigenvalues around zero being sampled.\n",
    "    data_dir : str, optional\n",
    "        Directory where the data is saved.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    obj : list\n",
    "        A list of lists, where each reduced density matrix is paired with its disorder strength W.\n",
    "        i.e. obj[i][0] is a 2D numpy.ndarray of the reduced density matrix, and obj[i][1] is the disorder strength used to generate it.\n",
    "        Number of data must be a multiple of 10.\n",
    "    \"\"\"\n",
    "\n",
    "    directory = os.path.join(data_dir, 'L={:02d}'.format(L), 'n={:02d}'.format(n), 'periodic={}'.format(periodic), 'num_EV={}'.format(num_EV))\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Check if file exists, load the file, and increment suffix.\n",
    "    i = 0\n",
    "    data = []\n",
    "    while os.path.exists(os.path.join(directory, obj_name + '-{:09d}.pkl.gz'.format(i))):\n",
    "        with gzip.open(os.path.join(directory, obj_name + '-{:09d}.pkl.gz'.format(i)), 'rb') as handle:\n",
    "            data = data + pickle.load(handle)\n",
    "        i += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_rho_valid(obj_name, L, n, periodic, num_EV, data_dir=rho_valid_data_dir):\n",
    "    \"\"\"Load a list of reduced density matrices with random W != {0.5, 8}.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj_name : str\n",
    "        A name you give to this object. Call it `rho_A`.\n",
    "    L : int\n",
    "        System size.\n",
    "    n : int\n",
    "        Number of consecutive spins sampled.\n",
    "    periodic : bool\n",
    "        Whether the Hamiltonian is periodic.\n",
    "    num_EV : int\n",
    "        Number of eigenvalues around zero being sampled.\n",
    "    data_dir : str, optional\n",
    "        Directory where the data is saved.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    obj : list\n",
    "        A list of lists, where each reduced density matrix is paired with its disorder strength W.\n",
    "        i.e. obj[i][0] is a 2D numpy.ndarray of the reduced density matrix, and obj[i][1] is the disorder strength used to generate it.\n",
    "        Number of data must be a multiple of 10.\n",
    "    \"\"\"\n",
    "\n",
    "    directory = os.path.join(data_dir, 'L={:02d}'.format(L), 'n={:02d}'.format(n), 'periodic={}'.format(periodic), 'num_EV={}'.format(num_EV))\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Check if file exists, load the file, and increment suffix.\n",
    "    i = 0\n",
    "    data = []\n",
    "    while os.path.exists(os.path.join(directory, obj_name + '-{:09d}.pkl.gz'.format(i))):\n",
    "        with gzip.open(os.path.join(directory, obj_name + '-{:09d}.pkl.gz'.format(i)), 'rb') as handle:\n",
    "            data = data + pickle.load(handle)\n",
    "        i += 1\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     18,
     35
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, file_name, L, n, periodic, num_EV, directory=model_dir):\n",
    "    \"\"\"Save model as pickle\"\"\"\n",
    "\n",
    "    model = model.cpu()\n",
    "    model_dict = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"hparams\": model.hparams\n",
    "    }\n",
    "\n",
    "    directory = os.path.join(directory, 'L={:02d}'.format(L), 'n={:02d}'.format(n), 'periodic={}'.format(periodic), 'num_EV={}'.format(num_EV))\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    model_path = os.path.join(directory, file_name)\n",
    "    with gzip.open(model_path, 'wb', 4) as handle:\n",
    "        pickle.dump(model_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return model_path\n",
    "\n",
    "\n",
    "def load_model(file_name, L, n, periodic, num_EV, directory=model_dir):\n",
    "\n",
    "    directory = os.path.join(directory, 'L={:02d}'.format(L), 'n={:02d}'.format(n), 'periodic={}'.format(periodic), 'num_EV={}'.format(num_EV))\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    model_path = os.path.join(directory, file_name)\n",
    "    with gzip.open(model_path, 'rb') as fp:\n",
    "        model_params = pickle.load(fp)\n",
    "\n",
    "    hparams = model_params[\"hparams\"]\n",
    "    model = MBLModel(hparams=hparams)\n",
    "    model.load_state_dict( model_params[\"state_dict\"] )\n",
    "    model.prepare_data()\n",
    "\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "def model_exists(file_name, L, n, periodic, num_EV, directory=model_dir):\n",
    "\n",
    "    directory = os.path.join(directory, 'L={:02d}'.format(L), 'n={:02d}'.format(n), 'periodic={}'.format(periodic), 'num_EV={}'.format(num_EV))\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    model_path = os.path.join(directory, file_name)\n",
    "    return os.path.exists(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     29
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def save_eval_valid(obj, model_version, L, n, periodic, num_EV, data_dir=eval_valid_data_dir):\n",
    "    \"\"\"Save model predictions of random W != {0.5, 8}.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj : list\n",
    "        A list of five numpy.ndarray [Ws, Ps, Ws_uniq, Ps_mean, Ps_std]\n",
    "        Where `W` are disorder strength, and `P` the probability of being in the localized phase.\n",
    "    model_version : int\n",
    "        Version of the neural network model.\n",
    "    L : int\n",
    "        System size.\n",
    "    n : int\n",
    "        Number of consecutive spins sampled.\n",
    "    periodic : bool\n",
    "        Whether the Hamiltonian is periodic.\n",
    "    num_EV : int\n",
    "        Number of eigenvalues around zero being sampled.\n",
    "    data_dir : str, optional\n",
    "        Directory where the data is saved.\n",
    "    \"\"\"\n",
    "\n",
    "    directory = os.path.join(data_dir, 'L={:02d}'.format(L), 'n={:02d}'.format(n), 'periodic={}'.format(periodic), 'num_EV={}'.format(num_EV))\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    with gzip.open(os.path.join(directory, 'model_v{}_eval.pkl.gz'.format(model_version)), 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_eval_valid(model_version, L, n, periodic, num_EV, data_dir=eval_valid_data_dir):\n",
    "    \"\"\"Load model predictions of random W != {0.5, 8}.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_version : int\n",
    "        Version of the neural network model.\n",
    "    L : int\n",
    "        System size.\n",
    "    n : int\n",
    "        Number of consecutive spins sampled.\n",
    "    periodic : bool\n",
    "        Whether the Hamiltonian is periodic.\n",
    "    num_EV : int\n",
    "        Number of eigenvalues around zero being sampled.\n",
    "    data_dir : str, optional\n",
    "        Directory where the data is saved.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    obj : list\n",
    "        A list of five numpy.ndarray [Ws, Ps, Ws_uniq, Ps_mean, Ps_std]\n",
    "        Where `W` are disorder strength, and `P` the probability of being in the localized phase.\n",
    "    \"\"\"\n",
    "\n",
    "    directory = os.path.join(data_dir, 'L={:02d}'.format(L), 'n={:02d}'.format(n), 'periodic={}'.format(periodic), 'num_EV={}'.format(num_EV))\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    if os.path.isfile(os.path.join(directory, 'model_v{}_eval.pkl.gz'.format(model_version))):\n",
    "        with gzip.open(os.path.join(directory, 'model_v{}_eval.pkl.gz'.format(model_version)), 'rb') as handle:\n",
    "            data = pickle.load(handle)\n",
    "        return data\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Demo data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MBL = {\n",
    "    \"obj_name\": 'rho_A',\n",
    "    \"L\": 12,\n",
    "    \"n\": 5,\n",
    "    \"periodic\": True,\n",
    "    \"num_EV\": 5,\n",
    "    \"rho_train_data_dir\": rho_train_data_dir,\n",
    "    \"rho_valid_data_dir\": rho_valid_data_dir\n",
    "}\n",
    "obj_name = MBL['obj_name']\n",
    "L        = MBL['L']\n",
    "n        = MBL['n']\n",
    "periodic = MBL['periodic']\n",
    "p        = MBL['periodic']\n",
    "num_EV   = MBL['num_EV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# data = load_rho_train(obj_name, L, n, periodic, num_EV)\n",
    "# print(type(data))\n",
    "# print(len(data))    # [[rho_A, W], [rho_A, W], [rho_A, W], ...]\n",
    "# print(type(data[0]))\n",
    "# print(len(data[0])) # [rho_A, W]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<class 'list'>  \n",
    "20000  \n",
    "<class 'list'>  \n",
    "2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from MBL_dataset import MBLDataset\n",
    "\n",
    "data_train = load_rho_train(obj_name, L, n, periodic, num_EV, data_dir=rho_train_data_dir)\n",
    "data_valid = load_rho_valid(obj_name, L, n, periodic, num_EV, data_dir=rho_valid_data_dir)\n",
    "\n",
    "train_dataset = MBLDataset(\n",
    "    data=data_train,\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "valid_dataset = MBLDataset(\n",
    "    data=data_valid,\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "print('Number of training samples  :', len(train_dataset))\n",
    "print('Number of validation samples:', len(valid_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Number of training samples: 20000  \n",
    "Number of validation samples: 20000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Two classes.\n",
    "labels = ['Extended (Low W)', 'Localized (High W)']\n",
    "\n",
    "image, W, label = train_dataset[0][\"image\"], train_dataset[0][\"W\"], train_dataset[0][\"label\"]\n",
    "print(\"W: {:.2f}\\nLabel: {}\".format(W, labels[label]))\n",
    "print(\"Shape of the image:\", image.size())\n",
    "print(\"Smallest value in the image:\", torch.min(image))\n",
    "print(\"Largest value in the image:\", torch.max(image))\n",
    "# print(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "W: 0.50  \n",
    "Label: Extended (Low W)  \n",
    "Shape of the image: torch.Size([1, 64, 64])  \n",
    "Smallest value in the image: tensor(-0.0984)  \n",
    "Largest value in the image: tensor(0.1544)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def visualize_data(dataset):\n",
    "\n",
    "    sample_idx = np.random.randint(0, len(dataset), size=5*5)\n",
    "\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(fig_w/dpi,fig_h/dpi*2), dpi=dpi, squeeze=False)\n",
    "\n",
    "    for i, idx in enumerate(sample_idx):\n",
    "        axes[i%5,i//5].imshow(np.abs(dataset[idx][\"image\"].squeeze(axis=0)))\n",
    "        axes[i%5,i//5].annotate('W={:3.1f}'.format(dataset[idx][\"W\"]), (0.5,0.5), xycoords='axes fraction', ha='center', color='w')\n",
    "\n",
    "    for axe in axes:\n",
    "        for ax in axe:\n",
    "            # ax.legend(loc='best')\n",
    "            ax.xaxis.set_ticklabels([])\n",
    "            ax.yaxis.set_ticklabels([])\n",
    "            ax.xaxis.set_visible(False)\n",
    "            ax.yaxis.set_visible(False)\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Visualize training data:')\n",
    "visualize_data(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Visualize validation data:')\n",
    "visualize_data(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "del data_train\n",
    "del data_valid\n",
    "del train_dataset\n",
    "del valid_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Neural network\n",
    "\n",
    "Since NNs with the same `n` have the same input size, we will evaluate them using the same NN structure. As a side effect, results different `n` are not entirely comparable, but we will compare them anyway because reasons.  \n",
    "\n",
    "Two classes `MBLModel` and `MBLDataset`, modified from a previous CNN facial recoginition code (own work), are used. The model structure and hyperparameters are defined using a dict called `hparams`. Inside it, specifications of the training data are passed using a nested dict `hparams[\"MBL\"]`. The models are stored in a directory structure that mirrors that of the training data (reduced density matrices $\\rho_A$).  \n",
    "\n",
    "Caveat: Validation data isn't really unseen data from the training distribution $W \\in \\{0.5, 8\\}$, but rather random W's that we'll be using them to predict $W_c$.  \n",
    "\n",
    "See the other notebook for data generation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from MBL_model import MBLModel\n",
    "model_version = 1\n",
    "\n",
    "# Two classes.\n",
    "labels = ['Extended (Low W)', 'Localized (High W)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Default parameters that works.\n",
    "input_size = (1, 2**n, 2**n) # train_dataset[0][\"image\"].size()\n",
    "output_size = 2 # [0, 1], two phases. == len(labels)\n",
    "\n",
    "default_hparams = {\n",
    "    # MBL Parameters:\n",
    "    \"MBL\": None, # Insert later.\n",
    "    # NN Parameters:\n",
    "    \"input_size\" : (1, 2**n, 2**n), # train_dataset[0][\"image\"].size(),\n",
    "    \"output_size\": output_size,\n",
    "    \"weight_decay\": 0,\n",
    "    \"batch_size\" : 2000,\n",
    "    \"entry_count\": 1, # #CNN before inception unit. \n",
    "    \"group_count\": 0, # #Inception units.\n",
    "    \"group_size\" : 3, # #CNN in each Inception unit.\n",
    "    # \"exit_count\": 2,\n",
    "    \"pool_every\": 4,\n",
    "    \"layers_cnn\": [\n",
    "        {\n",
    "            \"in_channels\": 1,\n",
    "            \"out_channels\": 6,\n",
    "            \"kernel_size\": 3,\n",
    "            \"stride\": 1,\n",
    "            \"use_max_pool\": False,\n",
    "        },\n",
    "        {\n",
    "            \"in_channels\": 6,\n",
    "            \"out_channels\": 12,\n",
    "            \"kernel_size\": 3,\n",
    "            \"stride\": 1,\n",
    "            \"use_max_pool\": True,\n",
    "        },\n",
    "    ],\n",
    "    # RuntimeError: size mismatch, m1: [2 x 13254], m2: [53016 x 30]\n",
    "    # 13254 = ((96-2)/2) ^2 * 6\n",
    "    \"layers_fc\": [\n",
    "        {\n",
    "            \"in_features\": 2352, # = ((2^n - 2) / 2)^2 * 6\n",
    "            \"out_features\": 640,\n",
    "            \"dropout\": 0.5,\n",
    "        },\n",
    "        {\n",
    "            \"in_features\": 640,\n",
    "            \"out_features\": output_size,\n",
    "        },\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Sample training data parameters.\n",
    "MBL = {\n",
    "    \"obj_name\": 'rho_A',\n",
    "    \"L\": 8,\n",
    "    \"n\": 5,\n",
    "    \"periodic\": False,\n",
    "    \"num_EV\": 1,\n",
    "    \"rho_train_data_dir\": rho_train_data_dir,\n",
    "    \"rho_valid_data_dir\": rho_valid_data_dir\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_MBL(L, n, p, num_EV):\n",
    "    MBL = {\n",
    "        \"obj_name\": 'rho_A',\n",
    "        \"L\": L,\n",
    "        \"n\": n,\n",
    "        \"periodic\": p,\n",
    "        \"num_EV\": num_EV,\n",
    "        \"rho_train_data_dir\": rho_train_data_dir,\n",
    "        \"rho_valid_data_dir\": rho_valid_data_dir\n",
    "    }\n",
    "    return MBL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def training_loop(default_hparams, MBL, epochs=60, filename='model_v{}.pkl.gz'.format(model_version), save=True):\n",
    "\n",
    "    hparams = copy.deepcopy(default_hparams)\n",
    "    hparams['MBL'] = MBL\n",
    "    # seed_everything(hparams[\"seed\"])\n",
    "    model = MBLModel(hparams=hparams)\n",
    "    # model.prepare_data()\n",
    "    # print(model)\n",
    "\n",
    "    obj_name = MBL['obj_name']\n",
    "    L        = MBL['L']\n",
    "    n        = MBL['n']\n",
    "    periodic = MBL['periodic']\n",
    "    p        = MBL['periodic']\n",
    "    num_EV   = MBL['num_EV']\n",
    "\n",
    "    if str(device) == 'cpu':\n",
    "        gpus = 0\n",
    "    else:\n",
    "        gpus = -1\n",
    "    logger = TensorBoardLogger('lightning_logs', name='MBL_v{:d}'.format(model_version))\n",
    "    scale_accum = 1\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=gpus,\n",
    "        logger=logger,\n",
    "        max_epochs=epochs,\n",
    "        min_epochs=10,\n",
    "        profiler=True,\n",
    "        # {5: 2, 10: 8} means no accumulation for epochs 1-4. accumulate 2 for epochs 5-10. accumulate 8 after that\n",
    "        accumulate_grad_batches={\n",
    "            1 : scale_accum * 1, \n",
    "            20: scale_accum * 2, \n",
    "            40: scale_accum * 4, \n",
    "            80: scale_accum * 8,\n",
    "        },\n",
    "        # accumulate_grad_batches=4,\n",
    "        weights_summary=None # [None,'top','full']\n",
    "    )\n",
    "\n",
    "    # print(hparams)\n",
    "    # for (k, v) in hparams.items():\n",
    "    #     print(k, v)\n",
    "\n",
    "    trainer.fit(model)\n",
    "\n",
    "    if save:\n",
    "        save_model(model, filename, L, n, periodic, num_EV)\n",
    "        # model.to(device)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Demo training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MBL = get_MBL(L, n, p, num_EV)\n",
    "model = training_loop(default_hparams, MBL, epochs=10, save=False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Visualize model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     20
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def visualize_predictions(model, mode='train'):\n",
    "    \"\"\"Mode = ['train' | 'valid']\"\"\"\n",
    "\n",
    "    labels = ['Extended (Low W)', 'Localized (High W)']\n",
    "\n",
    "    # Sample model predictions.\n",
    "    result_images  = []\n",
    "    result_targets = []\n",
    "    result_Ws      = []\n",
    "    result_preds   = []\n",
    "    result_probs   = []\n",
    "\n",
    "    model.eval()\n",
    "    SM  = torch.nn.Softmax()\n",
    "    LSM = torch.nn.LogSoftmax()\n",
    "    if mode == 'train':\n",
    "        dataloader = DataLoader(model.dataset[\"train\"], batch_size=25, shuffle=True)#, pin_memory=True)\n",
    "    else:\n",
    "        dataloader = DataLoader(model.dataset[\"val\"], batch_size=25, shuffle=True)#, pin_memory=True)\n",
    "\n",
    "    for batch in dataloader:\n",
    "\n",
    "        images, targets, Ws = batch[\"image\"], batch[\"label\"], batch[\"W\"]\n",
    "        images  = images.to(device)\n",
    "        outputs = model(images)\n",
    "        images  = images.to('cpu')\n",
    "        outputs = outputs.to('cpu')\n",
    "\n",
    "        preds   = outputs.argmax(axis=1)\n",
    "        probs   = SM(outputs)\n",
    "        # probs2  = - LSM(outputs)\n",
    "        # out_sum = probs2[:,0] + probs2[:,1]\n",
    "        # probs2[:,0] = probs2[:,0] / out_sum\n",
    "        # probs2[:,1] = probs2[:,1] / out_sum\n",
    "        # Simple averaging doesn't work, because it's negative...\n",
    "        # out_sum = outputs[:,0] + outputs[:,1]\n",
    "        # outputs[:,0] = outputs[:,0] / out_sum\n",
    "        # outputs[:,1] = outputs[:,1] / out_sum\n",
    "        shape   = images.shape\n",
    "        result_images  = result_images  + images.reshape(shape[0], shape[2], shape[3]).tolist()\n",
    "        result_targets = result_targets + targets.tolist()\n",
    "        result_Ws      = result_Ws      + Ws.tolist()\n",
    "        result_preds   = result_preds   + preds.tolist()\n",
    "        result_probs   = result_probs   + probs.tolist()\n",
    "        # result_probs   = result_probs   + probs2.tolist()\n",
    "        break # Because we only need 25 images.\n",
    "\n",
    "    # Display images.\n",
    "    sample_idx = np.random.randint(0, len(result_preds), size=5*5)\n",
    "\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(fig_w/dpi,fig_h/dpi*2), dpi=dpi, squeeze=False)\n",
    "\n",
    "    for i, idx in enumerate(sample_idx):\n",
    "        axes[i%5,i//5].imshow(np.abs(result_images[idx]))\n",
    "        W      = result_Ws[idx]\n",
    "        W_in   = result_targets[idx]\n",
    "        W_pred = result_preds[idx]\n",
    "        W_prob = result_probs[idx]\n",
    "        annotation  = 'Input  : \\n{}\\nW={:.2f}\\n\\n'.format(labels[W_in], W)\n",
    "        annotation += 'Predict: \\n{}\\n{:.0f}%'.format(labels[W_pred], W_prob[W_pred]*100)\n",
    "        # annotation += 'Predict: \\n{}\\n{:.0f}%'.format(labels[W_pred], W_prob[(W_pred+1)%2]*100)\n",
    "        if W_in == W_pred:\n",
    "            ec = 'lime'\n",
    "        else:\n",
    "            ec = 'red'\n",
    "        axes[i%5,i//5].annotate(annotation, (0.5,0.275), xycoords='axes fraction', ha='center', color='w', bbox=dict(facecolor='none', edgecolor=ec, boxstyle='round,pad=1', linewidth=2))\n",
    "\n",
    "    for axe in axes:\n",
    "        for ax in axe:\n",
    "            # ax.legend(loc='best')\n",
    "            ax.xaxis.set_ticklabels([])\n",
    "            ax.yaxis.set_ticklabels([])\n",
    "            ax.xaxis.set_visible(False)\n",
    "            ax.yaxis.set_visible(False)\n",
    "\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Sample training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_predictions(model, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Sample validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visualize_predictions(model, 'valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     20
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model_core(model, dataset):\n",
    "\n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    SM = torch.nn.Softmax()\n",
    "    dataloader = DataLoader(dataset, batch_size=1000, shuffle=False)#, pin_memory=True)\n",
    "    loss = 0\n",
    "    n_correct = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        images, targets = batch[\"image\"], batch[\"label\"]\n",
    "        images  = images.to(device)\n",
    "        outputs = model(images).to('cpu')\n",
    "        preds   = outputs.argmax(axis=1)\n",
    "        # print(SM(outputs))\n",
    "        loss += criterion(outputs, targets).item()\n",
    "        n_correct += (preds == targets).sum().item()\n",
    "\n",
    "    return loss, n_correct / len(dataset)\n",
    "\n",
    "def evaluate_model(model):\n",
    "\n",
    "    print(\"Training accuracy  : {:.4f}%\".format(evaluate_model_core(model, model.dataset[\"train\"])[1] * 100))\n",
    "    print(\"Validation accuracy: {:.4f}%\".format(evaluate_model_core(model, model.dataset[\"val\"])[1]   * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Estimate transition disorder strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, x0, y0, b):\n",
    "    y = 1 / (1 + np.exp(-b * (x - x0))) + y0\n",
    "    return y\n",
    "\n",
    "# Logit function is the inverse of sigmoid.\n",
    "def logit(y, x0, y0, b):\n",
    "    x = np.log((y - y0) / (1 - (y - y0))) / b + x0\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sigmoid(0,0,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logit(0.5,0,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove y0 because it should be bounded/aligned with y = 0 and y = 1.\n",
    "def sigmoid(x, x0, b):\n",
    "    y = 1 / (1 + np.exp(-b * (x - x0))) # + y0\n",
    "    return y\n",
    "\n",
    "# Logit function is the inverse of sigmoid.\n",
    "def logit(y, x0, b):\n",
    "    x = np.log((y) / (1 - (y))) / b + x0\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calc_probs(model, dataset):\n",
    "\n",
    "    # Sample model predictions.\n",
    "    result_images  = []\n",
    "    result_targets = []\n",
    "    result_Ws      = []\n",
    "    result_preds   = []\n",
    "    result_probs   = []\n",
    "\n",
    "    model.eval()\n",
    "    SM  = torch.nn.Softmax()\n",
    "    LSM = torch.nn.LogSoftmax()\n",
    "    dataloader = DataLoader(dataset, batch_size=200, shuffle=False)#, pin_memory=True)\n",
    "    for batch in dataloader:\n",
    "\n",
    "        images, targets, Ws = batch[\"image\"], batch[\"label\"], batch[\"W\"]\n",
    "        images  = images.to(device)\n",
    "        outputs = model(images)\n",
    "        images  = images.to('cpu')\n",
    "        outputs = outputs.to('cpu')\n",
    "\n",
    "        preds   = outputs.argmax(axis=1)\n",
    "        Ps      = SM(outputs)\n",
    "        shape   = images.shape\n",
    "        result_images  = result_images  + images.reshape(shape[0], shape[2], shape[3]).tolist()\n",
    "        result_targets = result_targets + targets.tolist()\n",
    "        result_Ws      = result_Ws      + Ws.tolist()\n",
    "        result_preds   = result_preds   + preds.tolist()\n",
    "        result_probs   = result_probs   + Ps.tolist()\n",
    "\n",
    "    result_Ws    = np.array(result_Ws)\n",
    "    result_probs = np.array(result_probs)\n",
    "    sorted_idx   = result_Ws.argsort()\n",
    "    Ws = result_Ws[sorted_idx]\n",
    "    Ps = result_probs[sorted_idx]\n",
    "\n",
    "    # Compute mean and std.\n",
    "    Ws_dict = OrderedDict()\n",
    "    Ws_uniq = []\n",
    "    Ps_mean = []\n",
    "    Ps_std  = []\n",
    "    # Ws is already sorted in `calc_probs()`.\n",
    "    for W, P in zip(Ws, Ps[:,1]):\n",
    "        if W not in Ws_dict:\n",
    "            Ws_dict[W] = []\n",
    "        Ws_dict[W].append(P)\n",
    "    for (W, P) in Ws_dict.items():\n",
    "        Ws_uniq.append(W)\n",
    "        Ps_mean.append(np.mean(P))\n",
    "        Ps_std.append(np.std(P, ddof=1))\n",
    "\n",
    "    return Ws, Ps, np.array(Ws_uniq), np.array(Ps_mean), np.array(Ps_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_crossing(Ws, Ps, Ws_uniq, Ps_mean, Ps_std):\n",
    "\n",
    "    labels = ['Extended (Low W)', 'Localized (High W)']\n",
    "\n",
    "    # Plot probability P(Localized) against W.\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(fig_w/dpi,fig_h/dpi), dpi=dpi, squeeze=False)\n",
    "\n",
    "    # Plot averaged values with error bars.\n",
    "    markers, caps, bars = axes[0,0].errorbar(Ws_uniq, Ps_mean, Ps_std, ls=' ', marker='x',capsize=2, capthick=2, label='P(Localized) Mean')\n",
    "    # Loop through bars and caps and set the alpha value\n",
    "    [bar.set_alpha(0.5) for bar in bars]\n",
    "    [cap.set_alpha(0.5) for cap in caps]\n",
    "\n",
    "    # Plot raw data.\n",
    "    axes[0,0].plot(Ws, Ps,   ls=' ', marker='x', label='P(Localized)', alpha=0.1)\n",
    "    # axes[0,0].plot(Ws, probs[:,0], ls=' ', marker='x', label='P(Extended)  (W small)')\n",
    "    axes[0,0].set_title('Probability vs W (L={}, n={})'.format(MBL['L'], MBL['n']))\n",
    "    axes[0,0].set_xlabel('W')\n",
    "    axes[0,0].set_ylabel('Probability')\n",
    "\n",
    "    # Curve fit a sigmoid using all data.\n",
    "    # Fitting only the mean with `Ws_uniq` and `Ps_mean` gives identical results.\n",
    "    # popt, pcov = curve_fit(sigmoid, Ws, Ps, p0=[3, 0, 2]) # Add bounds or initial values if it doesn't converge.\n",
    "    popt, pcov = curve_fit(sigmoid, Ws, Ps, p0=[3, 2]) # Add bounds or initial values if it doesn't converge.\n",
    "    # x0, y0, b = popt\n",
    "    x0, b = popt\n",
    "    x = np.linspace(0, 10, 100)\n",
    "    y = sigmoid(x, *popt)\n",
    "    axes[0,0].plot(x, y, ls='--', label='Fit')\n",
    "    # print('Fitted sigmoid function 1 / (1 + Exp(-{:.4f} (x - {:.4f}))) + {:.4f}'.format(b, x0, y0))\n",
    "    print('Fitted sigmoid function 1 / (1 + Exp(-{:.4f} (x - {:.4f})))'.format(b, x0))\n",
    "\n",
    "    W_c = logit(0.5, *popt)\n",
    "    perr = np.sqrt(np.diag(pcov))\n",
    "    print('Transition W_C is found to be at W = {:.4f}  {:.4f}'.format(W_c, perr[0]))\n",
    "    axes[0,0].axvline(W_c, c='r',         ls='--', label='$W_c$')\n",
    "    axes[0,0].axhline(0.5, c='lightgrey', ls='--', label='$P=0.5$')\n",
    "\n",
    "    for axe in axes:\n",
    "        for ax in axe:\n",
    "            ax.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_Ws, train_Ps, train_Ws_uniq, train_Ps_mean, train_Ps_std = calc_probs(model, model.dataset[\"train\"])\n",
    "# for model, dataset in zip(models, datasets):\n",
    "#     Ws, probs = calc_probs(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_crossing(train_Ws, train_Ps[:,1], train_Ws_uniq, train_Ps_mean, train_Ps_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "valid_Ws, valid_Ps, valid_Ws_uniq, valid_Ps_mean, valid_Ps_std = calc_probs(model, model.dataset[\"val\"])\n",
    "# for model, dataset in zip(models, datasets):\n",
    "#     Ws, probs = calc_probs(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_crossing(valid_Ws, valid_Ps[:,1], valid_Ws_uniq, valid_Ps_mean, valid_Ps_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch training\n",
    "The for-loop should be comparable to the one used to generate reduced density matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model.dataset[\"train\"]\n",
    "del model.dataset[\"val\"]\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Batch generate reduced density matrix.\n",
    "n  = 5                   # !!! Important !!! Number of consecutive sites.\n",
    "k  = 5                   # Number of eigenvalues near zero to save.\n",
    "J  = 1                   # Always = 1\n",
    "Ls = list(range(8,13,2)) # System sizes L.\n",
    "ps = [False, True]       # Periodic or not.\n",
    "et = []                  # Execution time.\n",
    "num_EVs = [k]            # Number of eigenvalues near zero to save.\n",
    "model_filename = 'model_v{}.pkl.gz'.format(model_version)\n",
    "\n",
    "for L in Ls:\n",
    "    for num_EV in num_EVs:\n",
    "        for p in ps:\n",
    "            start_time = time.time()\n",
    "            print('{} | Training model for L={:02d} | n={:02d} | periodic={: <5} | num_EV={} ...'.format(dt(), L, n, str(p), num_EV), flush=True)\n",
    "\n",
    "            if model_exists(model_filename, L, n, p, num_EV):\n",
    "                print('Model exists. Training skipped.', flush=True)\n",
    "            else:\n",
    "                MBL = get_MBL(L, n, p, num_EV)\n",
    "                try:\n",
    "                    model = training_loop(default_hparams, MBL).to(device)\n",
    "                    data = calc_probs(model, model.dataset[\"val\"])\n",
    "                    save_eval_valid(data, model_version, L, n, p, num_EV)\n",
    "                    del model.dataset[\"train\"]\n",
    "                    del model.dataset[\"val\"]\n",
    "                    del model\n",
    "                except RuntimeError as err:\n",
    "                    print('RuntimeError: {0}'.format(err), flush=True)\n",
    "                    print('Insufficient data. Training skipped.', flush=True)\n",
    "\n",
    "            exec_time = time.time() - start_time\n",
    "            et.append(exec_time)\n",
    "            print('{} | Computed: L={:02d} | n={:02d} | periodic={: <5} | num_EV={}.'.format(dt(), L, n, str(p), num_EV), flush=True)\n",
    "            print('{} | Execution took {: 8.2f}s or {: 6.2f}min.'.format(dt(), exec_time, exec_time/60), flush=True)\n",
    "            print(' ', flush=True)\n",
    "\n",
    "# if check_shutdown_signal():\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for \"annealing\".\n",
    "# hparams[\"use_adam\"] = 1\n",
    "# model_adam = MBLModel(hparams=hparams)\n",
    "# model_adam.prepare_data()\n",
    "# model_adam.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ws, Ps, Ws_uniq, Ps_mean, Ps_std = load_eval_valid(model_version, L, n, p, num_EV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit",
   "language": "python",
   "name": "python37264bit22d8f94fb4124d8cb7bc86dc616da5cb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
