{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this variable yourself.\n",
    "running_on_colab = False\n",
    "# Store data as reduced density matrix `rho` or eigenvector tuple `EVW`.\n",
    "rho_or_EVW = 'rho'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning of Many Body Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['running_on_colab'] = str(running_on_colab)\n",
    "# running_on_colab = (os.getenv('running_on_colab', 'False') == 'True')\n",
    "\n",
    "if running_on_colab:\n",
    "    data_root             = 'drive/MyDrive/Colab Data/MBL/'\n",
    "    sys.path.append(data_root)\n",
    "else:\n",
    "    data_root             = './'\n",
    "\n",
    "# Store data as reduced density matrix `rho` or eigenvector tuple `EVW`.\n",
    "os.environ['rho_or_EVW'] = str(rho_or_EVW)\n",
    "# running_on_colab = (os.getenv('rho_or_EVW', 'EVW') == 'rho')\n",
    "\n",
    "from file_io import *\n",
    "from data_gen import *\n",
    "from plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "dpi = 100\n",
    "fig_w = 1280\n",
    "fig_h = 640\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if running_on_colab:\n",
    "    !cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if running_on_colab:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if running_on_colab:\n",
    "    !pip install pytorch_lightning==0.7.6 torchsummary==1.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Demo data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MBL = {\n",
    "    \"obj_name\": 'rho_A',\n",
    "    \"L\": 12,\n",
    "    \"n\": 2,\n",
    "    \"periodic\": True,\n",
    "    \"num_EV\": 5,\n",
    "}\n",
    "obj_name = MBL['obj_name']\n",
    "L        = MBL['L']\n",
    "n        = MBL['n']\n",
    "periodic = MBL['periodic']\n",
    "p        = MBL['periodic']\n",
    "num_EV   = MBL['num_EV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from MBL_dataset_rho import MBLDatasetRho\n",
    "\n",
    "train_dataset = MBLDatasetRho(\n",
    "    MBL_params=MBL,\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "valid_dataset = MBLDatasetRho(\n",
    "    MBL_params=MBL,\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "print('Number of training samples:', len(train_dataset))\n",
    "print('Number of random samples  :', len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Two classes.\n",
    "labels = ['Extended (Low W)', 'Localized (High W)']\n",
    "\n",
    "image, W, label = train_dataset[0][\"image\"], train_dataset[0][\"W\"], train_dataset[0][\"label\"]\n",
    "print(\"W: {:.2f}\\nLabel: {}\".format(W, labels[label]))\n",
    "print(\"Shape of the image:\", image.size())\n",
    "print(\"Smallest value in the image:\", torch.min(image))\n",
    "print(\"Largest value in the image:\", torch.max(image))\n",
    "# print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Visualize training data:')\n",
    "visualize_dataset_rho(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Visualize random data:')\n",
    "visualize_dataset_rho(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "del train_dataset\n",
    "del valid_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Neural network\n",
    "\n",
    "Since NNs with the same `n` have the same input size, we will evaluate them using the same NN structure. As a side effect, results different `n` are not entirely comparable, but we will compare them anyway because reasons.  \n",
    "\n",
    "Two classes `MBLModel` and `MBLDataset`, modified from a previous CNN facial recoginition code (own work), are used. The model structure and hyperparameters are defined using a dict called `hparams`. Inside it, specifications of the training data are passed using a nested dict `hparams[\"MBL\"]`. The models are stored in a directory structure that mirrors that of the training data (reduced density matrices $\\rho_A$).  \n",
    "\n",
    "Caveat: Validation data isn't really unseen data from the training distribution $W \\in \\{0.5, 8\\}$, but rather random W's that we'll be using them to predict $W_c$.  \n",
    "\n",
    "See the other notebook for data generation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from MBL_model import MBLModel\n",
    "model_version = 1\n",
    "\n",
    "# Two classes.\n",
    "labels = ['Extended (Low W)', 'Localized (High W)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Default parameters that works.\n",
    "input_size = (1, 2**n, 2**n) # train_dataset[0][\"image\"].size()\n",
    "output_size = 2 # [0, 1], two phases. == len(labels)\n",
    "\n",
    "default_hparams = {\n",
    "    # MBL Parameters:\n",
    "    \"MBL\": None, # Insert later.\n",
    "    # NN Parameters:\n",
    "    \"input_size\" : (1, 2**n, 2**n), # train_dataset[0][\"image\"].size(),\n",
    "    \"output_size\": output_size,\n",
    "    \"weight_decay\": 0,\n",
    "    \"batch_size\" : 2000,\n",
    "    \"entry_count\": 0, # #CNN before inception unit. \n",
    "    \"group_count\": 0, # #Inception units.\n",
    "    \"group_size\" : 3, # #CNN in each Inception unit.\n",
    "    # \"exit_count\": 2,\n",
    "    \"pool_every\": 4,\n",
    "    \"layers_cnn\": [\n",
    "        {\n",
    "            \"in_channels\": 1,\n",
    "            \"out_channels\": 24,\n",
    "            \"kernel_size\": 2,\n",
    "            \"stride\": 1,\n",
    "            \"use_max_pool\": False,\n",
    "        },\n",
    "    ],\n",
    "    # RuntimeError: size mismatch, m1: [2 x 13254], m2: [53016 x 30]\n",
    "    # 13254 = ((96-2)/2) ^2 * 6\n",
    "    \"layers_fc\": [\n",
    "        {\n",
    "            \"in_features\": 216, # = ((2^n - 2) / 2)^2 * 6\n",
    "            \"out_features\": 120,\n",
    "            \"dropout\": 0.5,\n",
    "        },\n",
    "        {\n",
    "            \"in_features\": 120,\n",
    "            \"out_features\": output_size,\n",
    "        },\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Sample training data parameters.\n",
    "MBL = {\n",
    "    \"obj_name\": 'rho_A',\n",
    "    \"L\": 8,\n",
    "    \"n\": 2,\n",
    "    \"periodic\": False,\n",
    "    \"num_EV\": 1,\n",
    "    \"rho_train_data_dir\": rho_train_data_dir,\n",
    "    \"rho_valid_data_dir\": rho_valid_data_dir\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_MBL(L, n, p, num_EV):\n",
    "    MBL = {\n",
    "        \"obj_name\": 'rho_A',\n",
    "        \"L\": L,\n",
    "        \"n\": n,\n",
    "        \"periodic\": p,\n",
    "        \"num_EV\": num_EV,\n",
    "        \"rho_train_data_dir\": rho_train_data_dir,\n",
    "        \"rho_valid_data_dir\": rho_valid_data_dir\n",
    "    }\n",
    "    return MBL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def training_loop(default_hparams, MBL, epochs=60, filename='model_v{}.pkl.gz'.format(model_version), save=True):\n",
    "\n",
    "    hparams = copy.deepcopy(default_hparams)\n",
    "    hparams['MBL'] = MBL\n",
    "    # seed_everything(hparams[\"seed\"])\n",
    "    model = MBLModel(hparams=hparams)\n",
    "    # model.prepare_data()\n",
    "    # print(model)\n",
    "\n",
    "    obj_name = MBL['obj_name']\n",
    "    L        = MBL['L']\n",
    "    n        = MBL['n']\n",
    "    periodic = MBL['periodic']\n",
    "    p        = MBL['periodic']\n",
    "    num_EV   = MBL['num_EV']\n",
    "\n",
    "    if str(device) == 'cpu':\n",
    "        gpus = 0\n",
    "    else:\n",
    "        gpus = -1\n",
    "    logger = TensorBoardLogger('lightning_logs', name='MBL_v{:d}'.format(model_version))\n",
    "    scale_accum = 1\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=gpus,\n",
    "        logger=logger,\n",
    "        max_epochs=epochs,\n",
    "        min_epochs=10,\n",
    "        profiler=True,\n",
    "        # {5: 2, 10: 8} means no accumulation for epochs 1-4. accumulate 2 for epochs 5-10. accumulate 8 after that\n",
    "        accumulate_grad_batches={\n",
    "            1 : scale_accum * 1, \n",
    "            20: scale_accum * 2, \n",
    "            40: scale_accum * 4, \n",
    "            80: scale_accum * 8,\n",
    "        },\n",
    "        # accumulate_grad_batches=4,\n",
    "        weights_summary=None # [None,'top','full']\n",
    "    )\n",
    "\n",
    "    # print(hparams)\n",
    "    # for (k, v) in hparams.items():\n",
    "    #     print(k, v)\n",
    "\n",
    "    trainer.fit(model)\n",
    "\n",
    "    if save:\n",
    "        save_model(model, filename, L, n, periodic, num_EV)\n",
    "        # model.to(device)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Demo training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MBL = get_MBL(L, n, p, num_EV)\n",
    "model = training_loop(default_hparams, MBL, epochs=10, save=False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Visualize model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     20
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def visualize_predictions(model, mode='train'):\n",
    "    \"\"\"Mode = ['train' | 'valid']\"\"\"\n",
    "\n",
    "    labels = ['Extended (Low W)', 'Localized (High W)']\n",
    "\n",
    "    # Sample model predictions.\n",
    "    result_images  = []\n",
    "    result_targets = []\n",
    "    result_Ws      = []\n",
    "    result_preds   = []\n",
    "    result_probs   = []\n",
    "\n",
    "    model.eval()\n",
    "    SM  = torch.nn.Softmax()\n",
    "    LSM = torch.nn.LogSoftmax()\n",
    "    if mode == 'train':\n",
    "        dataloader = DataLoader(model.dataset[\"train\"], batch_size=25, shuffle=True)#, pin_memory=True)\n",
    "    else:\n",
    "        dataloader = DataLoader(model.dataset[\"val\"], batch_size=25, shuffle=True)#, pin_memory=True)\n",
    "\n",
    "    for batch in dataloader:\n",
    "\n",
    "        images, targets, Ws = batch[\"image\"], batch[\"label\"], batch[\"W\"]\n",
    "        images  = images.to(device)\n",
    "        outputs = model(images)\n",
    "        images  = images.to('cpu')\n",
    "        outputs = outputs.to('cpu')\n",
    "\n",
    "        preds   = outputs.argmax(axis=1)\n",
    "        probs   = SM(outputs)\n",
    "        # probs2  = - LSM(outputs)\n",
    "        # out_sum = probs2[:,0] + probs2[:,1]\n",
    "        # probs2[:,0] = probs2[:,0] / out_sum\n",
    "        # probs2[:,1] = probs2[:,1] / out_sum\n",
    "        # Simple averaging doesn't work, because it's negative...\n",
    "        # out_sum = outputs[:,0] + outputs[:,1]\n",
    "        # outputs[:,0] = outputs[:,0] / out_sum\n",
    "        # outputs[:,1] = outputs[:,1] / out_sum\n",
    "        shape   = images.shape\n",
    "        result_images  = result_images  + images.reshape(shape[0], shape[2], shape[3]).tolist()\n",
    "        result_targets = result_targets + targets.tolist()\n",
    "        result_Ws      = result_Ws      + Ws.tolist()\n",
    "        result_preds   = result_preds   + preds.tolist()\n",
    "        result_probs   = result_probs   + probs.tolist()\n",
    "        # result_probs   = result_probs   + probs2.tolist()\n",
    "        break # Because we only need 25 images.\n",
    "\n",
    "    # Display images.\n",
    "    sample_idx = np.random.randint(0, len(result_preds), size=5*5)\n",
    "\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(fig_w/dpi,fig_h/dpi*2), dpi=dpi, squeeze=False)\n",
    "\n",
    "    for i, idx in enumerate(sample_idx):\n",
    "        axes[i%5,i//5].imshow(np.abs(result_images[idx]))\n",
    "        W      = result_Ws[idx]\n",
    "        W_in   = result_targets[idx]\n",
    "        W_pred = result_preds[idx]\n",
    "        W_prob = result_probs[idx]\n",
    "        annotation  = 'Input  : \\n{}\\nW={:.2f}\\n\\n'.format(labels[W_in], W)\n",
    "        annotation += 'Predict: \\n{}\\n{:.0f}%'.format(labels[W_pred], W_prob[W_pred]*100)\n",
    "        # annotation += 'Predict: \\n{}\\n{:.0f}%'.format(labels[W_pred], W_prob[(W_pred+1)%2]*100)\n",
    "        if W_in == W_pred:\n",
    "            ec = 'lime'\n",
    "        else:\n",
    "            ec = 'red'\n",
    "        axes[i%5,i//5].annotate(annotation, (0.5,0.275), xycoords='axes fraction', ha='center', color='w', bbox=dict(facecolor='none', edgecolor=ec, boxstyle='round,pad=1', linewidth=2))\n",
    "\n",
    "    for axe in axes:\n",
    "        for ax in axe:\n",
    "            # ax.legend(loc='best')\n",
    "            ax.xaxis.set_ticklabels([])\n",
    "            ax.yaxis.set_ticklabels([])\n",
    "            ax.xaxis.set_visible(False)\n",
    "            ax.yaxis.set_visible(False)\n",
    "\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Sample training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_predictions(model, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Sample validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visualize_predictions(model, 'valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     20
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model_core(model, dataset):\n",
    "\n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    SM = torch.nn.Softmax()\n",
    "    dataloader = DataLoader(dataset, batch_size=1000, shuffle=False)#, pin_memory=True)\n",
    "    loss = 0\n",
    "    n_correct = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        images, targets = batch[\"image\"], batch[\"label\"]\n",
    "        images  = images.to(device)\n",
    "        outputs = model(images).to('cpu')\n",
    "        preds   = outputs.argmax(axis=1)\n",
    "        # print(SM(outputs))\n",
    "        loss += criterion(outputs, targets).item()\n",
    "        n_correct += (preds == targets).sum().item()\n",
    "\n",
    "    return loss, n_correct / len(dataset)\n",
    "\n",
    "def evaluate_model(model):\n",
    "\n",
    "    print(\"Training accuracy  : {:.4f}%\".format(evaluate_model_core(model, model.dataset[\"train\"])[1] * 100))\n",
    "    print(\"Validation accuracy: {:.4f}%\".format(evaluate_model_core(model, model.dataset[\"val\"])[1]   * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Estimate transition disorder strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, x0, y0, b):\n",
    "    y = 1 / (1 + np.exp(-b * (x - x0))) + y0\n",
    "    return y\n",
    "\n",
    "# Logit function is the inverse of sigmoid.\n",
    "def logit(y, x0, y0, b):\n",
    "    x = np.log((y - y0) / (1 - (y - y0))) / b + x0\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sigmoid(0,0,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logit(0.5,0,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove y0 because it should be bounded/aligned with y = 0 and y = 1.\n",
    "def sigmoid(x, x0, b):\n",
    "    y = 1 / (1 + np.exp(-b * (x - x0))) # + y0\n",
    "    return y\n",
    "\n",
    "# Logit function is the inverse of sigmoid.\n",
    "def logit(y, x0, b):\n",
    "    x = np.log((y) / (1 - (y))) / b + x0\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calc_probs(model, dataset):\n",
    "\n",
    "    # Sample model predictions.\n",
    "    result_images  = []\n",
    "    result_targets = []\n",
    "    result_Ws      = []\n",
    "    result_preds   = []\n",
    "    result_probs   = []\n",
    "\n",
    "    model.eval()\n",
    "    SM  = torch.nn.Softmax()\n",
    "    LSM = torch.nn.LogSoftmax()\n",
    "    dataloader = DataLoader(dataset, batch_size=200, shuffle=False)#, pin_memory=True)\n",
    "    for batch in dataloader:\n",
    "\n",
    "        images, targets, Ws = batch[\"image\"], batch[\"label\"], batch[\"W\"]\n",
    "        images  = images.to(device)\n",
    "        outputs = model(images)\n",
    "        images  = images.to('cpu')\n",
    "        outputs = outputs.to('cpu')\n",
    "\n",
    "        preds   = outputs.argmax(axis=1)\n",
    "        Ps      = SM(outputs)\n",
    "        shape   = images.shape\n",
    "        result_images  = result_images  + images.reshape(shape[0], shape[2], shape[3]).tolist()\n",
    "        result_targets = result_targets + targets.tolist()\n",
    "        result_Ws      = result_Ws      + Ws.tolist()\n",
    "        result_preds   = result_preds   + preds.tolist()\n",
    "        result_probs   = result_probs   + Ps.tolist()\n",
    "\n",
    "    result_Ws    = np.array(result_Ws)\n",
    "    result_probs = np.array(result_probs)\n",
    "    sorted_idx   = result_Ws.argsort()\n",
    "    Ws = result_Ws[sorted_idx]\n",
    "    Ps = result_probs[sorted_idx]\n",
    "\n",
    "    # Compute mean and std.\n",
    "    Ws_dict = OrderedDict()\n",
    "    Ws_uniq = []\n",
    "    Ps_mean = []\n",
    "    Ps_std  = []\n",
    "    # Ws is already sorted in `calc_probs()`.\n",
    "    for W, P in zip(Ws, Ps[:,1]):\n",
    "        if W not in Ws_dict:\n",
    "            Ws_dict[W] = []\n",
    "        Ws_dict[W].append(P)\n",
    "    for (W, P) in Ws_dict.items():\n",
    "        Ws_uniq.append(W)\n",
    "        Ps_mean.append(np.mean(P))\n",
    "        Ps_std.append(np.std(P, ddof=1))\n",
    "\n",
    "    return Ws, Ps, np.array(Ws_uniq), np.array(Ps_mean), np.array(Ps_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_crossing(Ws, Ps, Ws_uniq, Ps_mean, Ps_std):\n",
    "\n",
    "    labels = ['Extended (Low W)', 'Localized (High W)']\n",
    "\n",
    "    # Plot probability P(Localized) against W.\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(fig_w/dpi,fig_h/dpi), dpi=dpi, squeeze=False)\n",
    "\n",
    "    # Plot averaged values with error bars.\n",
    "    markers, caps, bars = axes[0,0].errorbar(Ws_uniq, Ps_mean, Ps_std, ls=' ', marker='x',capsize=2, capthick=2, label='P(Localized) Mean')\n",
    "    # Loop through bars and caps and set the alpha value\n",
    "    [bar.set_alpha(0.5) for bar in bars]\n",
    "    [cap.set_alpha(0.5) for cap in caps]\n",
    "\n",
    "    # Plot raw data.\n",
    "    axes[0,0].plot(Ws, Ps,   ls=' ', marker='x', label='P(Localized)', alpha=0.1)\n",
    "    # axes[0,0].plot(Ws, probs[:,0], ls=' ', marker='x', label='P(Extended)  (W small)')\n",
    "    axes[0,0].set_title('Probability vs W (L={}, n={})'.format(MBL['L'], MBL['n']))\n",
    "    axes[0,0].set_xlabel('W')\n",
    "    axes[0,0].set_ylabel('Probability')\n",
    "\n",
    "    # Curve fit a sigmoid using all data.\n",
    "    # Fitting only the mean with `Ws_uniq` and `Ps_mean` gives identical results.\n",
    "    # popt, pcov = curve_fit(sigmoid, Ws, Ps, p0=[3, 0, 2]) # Add bounds or initial values if it doesn't converge.\n",
    "    popt, pcov = curve_fit(sigmoid, Ws, Ps, p0=[3, 2]) # Add bounds or initial values if it doesn't converge.\n",
    "    # x0, y0, b = popt\n",
    "    x0, b = popt\n",
    "    x = np.linspace(0, 10, 100)\n",
    "    y = sigmoid(x, *popt)\n",
    "    axes[0,0].plot(x, y, ls='--', label='Fit')\n",
    "    # print('Fitted sigmoid function 1 / (1 + Exp(-{:.4f} (x - {:.4f}))) + {:.4f}'.format(b, x0, y0))\n",
    "    print('Fitted sigmoid function 1 / (1 + Exp(-{:.4f} (x - {:.4f})))'.format(b, x0))\n",
    "\n",
    "    W_c = logit(0.5, *popt)\n",
    "    perr = np.sqrt(np.diag(pcov))\n",
    "    print('Transition W_C is found to be at W = {:.4f} ± {:.4f}'.format(W_c, perr[0]))\n",
    "    axes[0,0].axvline(W_c, c='r',         ls='--', label='$W_c$')\n",
    "    axes[0,0].axhline(0.5, c='lightgrey', ls='--', label='$P=0.5$')\n",
    "\n",
    "    for axe in axes:\n",
    "        for ax in axe:\n",
    "            ax.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_Ws, train_Ps, train_Ws_uniq, train_Ps_mean, train_Ps_std = calc_probs(model, model.dataset[\"train\"])\n",
    "# for model, dataset in zip(models, datasets):\n",
    "#     Ws, probs = calc_probs(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_crossing(train_Ws, train_Ps[:,1], train_Ws_uniq, train_Ps_mean, train_Ps_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "valid_Ws, valid_Ps, valid_Ws_uniq, valid_Ps_mean, valid_Ps_std = calc_probs(model, model.dataset[\"val\"])\n",
    "# for model, dataset in zip(models, datasets):\n",
    "#     Ws, probs = calc_probs(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_crossing(valid_Ws, valid_Ps[:,1], valid_Ws_uniq, valid_Ps_mean, valid_Ps_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch training\n",
    "The for-loop should be comparable to the one used to generate reduced density matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model.dataset[\"train\"]\n",
    "del model.dataset[\"val\"]\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Batch generate reduced density matrix.\n",
    "n  = 2                   # !!! Important !!! Number of consecutive sites.\n",
    "k  = 5                   # Number of eigenvalues near zero to save.\n",
    "J  = 1                   # Always = 1\n",
    "Ls = list(range(8,13,2)) # System sizes L.\n",
    "ps = [False, True]       # Periodic or not.\n",
    "et = []                  # Execution time.\n",
    "num_EVs = [k]            # Number of eigenvalues near zero to save.\n",
    "model_filename = 'model_v{}.pkl.gz'.format(model_version)\n",
    "\n",
    "for L in Ls:\n",
    "    for num_EV in num_EVs:\n",
    "        for p in ps:\n",
    "            start_time = time.time()\n",
    "            print('{} | Training model for L={:02d} | n={:02d} | periodic={: <5} | num_EV={} ...'.format(dt(), L, n, str(p), num_EV), flush=True)\n",
    "\n",
    "            if model_exists(model_filename, L, n, p, num_EV):\n",
    "                print('Model exists. Training skipped.', flush=True)\n",
    "            else:\n",
    "                MBL = get_MBL(L, n, p, num_EV)\n",
    "                try:\n",
    "                    model = training_loop(default_hparams, MBL).to(device)\n",
    "                    data = calc_probs(model, model.dataset[\"val\"])\n",
    "                    save_eval_valid(data, model_version, L, n, p, num_EV)\n",
    "                    del model.dataset[\"train\"]\n",
    "                    del model.dataset[\"val\"]\n",
    "                    del model\n",
    "                except RuntimeError as err:\n",
    "                    print('RuntimeError: {0}'.format(err), flush=True)\n",
    "                    print('Insufficient data. Training skipped.', flush=True)\n",
    "\n",
    "            exec_time = time.time() - start_time\n",
    "            et.append(exec_time)\n",
    "            print('{} | Computed: L={:02d} | n={:02d} | periodic={: <5} | num_EV={}.'.format(dt(), L, n, str(p), num_EV), flush=True)\n",
    "            print('{} | Execution took {: 8.2f}s or {: 6.2f}min.'.format(dt(), exec_time, exec_time/60), flush=True)\n",
    "            print(' ', flush=True)\n",
    "\n",
    "# if check_shutdown_signal():\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for \"annealing\".\n",
    "# hparams[\"use_adam\"] = 1\n",
    "# model_adam = MBLModel(hparams=hparams)\n",
    "# model_adam.prepare_data()\n",
    "# model_adam.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ws, Ps, Ws_uniq, Ps_mean, Ps_std = load_eval_valid(model_version, L, n, p, num_EV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit",
   "language": "python",
   "name": "python37264bit22d8f94fb4124d8cb7bc86dc616da5cb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
