{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this variable yourself.\n",
    "running_on_colab = False\n",
    "# Store data as reduced density matrix `rho` or eigenvector tuple `EVW`.\n",
    "rho_or_EVW = 'rho'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning of Many Body Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Use exact diagonalization to obtain a few eigenstates near energy $E = 0$ from the Heisenberg model with a\n",
    "random field, \n",
    "\n",
    "\\begin{equation}\n",
    "    H = J \\sum_i \\vec{S}_{i} \\cdot \\vec{S}_{i+1} - \\sum_i h_i S^z_i\n",
    "\\end{equation}\n",
    "\n",
    ", where the values of the field $ h_i \\in [-W, W] $ are chosen from a uniform random distribution with a \"disorder strength\" $W$ (with moderate system sizes $L \\approx 12$). \n",
    "\n",
    "The exciting property of this model is that it is believed to undergo a phase transition from an extended phase (small $W$) to a localized phase (large $W$). \n",
    "\n",
    "We will use ML to detect this transition: Pick a number of eigenstates that are near energy $E = 0$ and obtain the reduced density matrices $\\rho^A$, where $A$ is a region of $n$ consecutive spins (a few hundred to thousands eigenstates for different disorder realizations). \n",
    "\n",
    "Now use the density matrices for $W = 0.5 J$ and $W = 8.0 J$ to train a neural network (just interpret the entries of $\\rho^A$ as an image with $2^n \\times 2^n$ pixel). \n",
    "Then use this network and study the output of the neural network for different $W$. \n",
    "\n",
    "How does the results depend on system size $L$ and block size $n$? \n",
    "At which $W_c$ do you expect the transition to occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['running_on_colab'] = str(running_on_colab)\n",
    "# running_on_colab = (os.getenv('running_on_colab', 'False') == 'True')\n",
    "\n",
    "if running_on_colab:\n",
    "    data_root             = 'drive/MyDrive/Colab Data/MBL/'\n",
    "    sys.path.append(data_root)\n",
    "else:\n",
    "    data_root             = './'\n",
    "\n",
    "# Store data as reduced density matrix `rho` or eigenvector tuple `EVW`.\n",
    "os.environ['rho_or_EVW'] = str(rho_or_EVW)\n",
    "# running_on_colab = (os.getenv('rho_or_EVW', 'EVW') == 'rho')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from file_io import *\n",
    "from data_gen import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "dpi = 100\n",
    "fig_w = 640\n",
    "fig_h = 480\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if running_on_colab:\n",
    "    !cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if running_on_colab:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if running_on_colab:\n",
    "    !pip install pytorch_lightning==0.7.6 torchsummary==1.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Plot reduced density matrix\n",
    "Plot the magnitude of $ 4 \\times 4 $ typical reduced density matrices.  \n",
    "$L = 12, n = 6, k = 1$ is selected.  \n",
    "Actually $L = 10, n = 6, k = 5$ now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MBL = {\n",
    "    \"obj_name\": 'rho_A',\n",
    "    \"L\": 10,\n",
    "    \"n\": 6,\n",
    "    \"periodic\": True,\n",
    "    \"num_EV\": 5,\n",
    "    \"rho_train_data_dir\": rho_train_data_dir,\n",
    "    \"rho_valid_data_dir\": rho_valid_data_dir\n",
    "}\n",
    "obj_name = MBL['obj_name']\n",
    "L        = MBL['L']\n",
    "n        = MBL['n']\n",
    "periodic = MBL['periodic']\n",
    "p        = MBL['periodic']\n",
    "num_EV   = MBL['num_EV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from MBL_dataset import MBLDataset\n",
    "\n",
    "data_train = load_rho_train(obj_name, L, n, periodic, num_EV, data_dir=rho_train_data_dir)\n",
    "data_valid = load_rho_valid(obj_name, L, n, periodic, num_EV, data_dir=rho_valid_data_dir)\n",
    "\n",
    "train_dataset = MBLDataset(\n",
    "    data=data_train,\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "valid_dataset = MBLDataset(\n",
    "    data=data_valid,\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "print('Number of training samples  :', len(train_dataset))\n",
    "print('Number of validation samples:', len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Two classes.\n",
    "labels = ['Extended (Low W)', 'Localized (High W)']\n",
    "\n",
    "image, W, label = train_dataset[0][\"image\"], train_dataset[0][\"W\"], train_dataset[0][\"label\"]\n",
    "print(\"W: {:.2f}\\nLabel: {}\".format(W, labels[label]))\n",
    "print(\"Shape of the image:\", image.size())\n",
    "print(\"Smallest value in the image:\", torch.min(image))\n",
    "print(\"Largest value in the image:\", torch.max(image))\n",
    "# print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def visualize_train_data(dataset):\n",
    "    \n",
    "    num = 4\n",
    "\n",
    "    sample_idx = np.random.randint(0, len(dataset), size=num*num)\n",
    "\n",
    "    # Square image.\n",
    "    fig, axes = plt.subplots(num, num, figsize=(fig_w/dpi,fig_w/dpi), dpi=dpi, squeeze=False)\n",
    "\n",
    "    samples_ext = []\n",
    "    samples_loc = []\n",
    "    for i in range(len(dataset)):\n",
    "        data = dataset[i]\n",
    "        if data['W'] == 0.5 and len(samples_ext) < num*num//2:\n",
    "            samples_ext.append(np.abs(data['image'].squeeze(axis=0).numpy()))\n",
    "        if data['W'] == 8.0 and len(samples_loc) < num*num//2:\n",
    "            samples_loc.append(np.abs(data['image'].squeeze(axis=0).numpy()))\n",
    "        if len(samples_ext) >= num*num//2 and len(samples_loc) >= num*num//2:\n",
    "            break\n",
    "\n",
    "    i_ext = 0\n",
    "    i_loc = 0\n",
    "    for i in range(num * num):\n",
    "        if i // num < 2:\n",
    "            # np.fill_diagonal(samples_ext[i_ext], 0)\n",
    "            axes[i%num,i//num].imshow(samples_ext[i_ext])\n",
    "            axes[i%num,i//num].annotate('W={:3.1f}'.format(0.5), (0.25,0.1), xycoords='axes fraction', ha='center', color='w')\n",
    "            i_ext += 1\n",
    "        else:\n",
    "            # np.fill_diagonal(samples_loc[i_loc], 0)\n",
    "            axes[i%num,i//num].imshow(samples_loc[i_loc])\n",
    "            axes[i%num,i//num].annotate('W={:3.1f}'.format(8.0), (0.25,0.1), xycoords='axes fraction', ha='center', color='w')\n",
    "            i_loc += 1\n",
    "\n",
    "    for axe in axes:\n",
    "        for ax in axe:\n",
    "            # ax.legend(loc='best')\n",
    "            ax.xaxis.set_ticklabels([])\n",
    "            ax.yaxis.set_ticklabels([])\n",
    "            ax.xaxis.set_visible(False)\n",
    "            ax.yaxis.set_visible(False)\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Visualize training data:')\n",
    "visualize_train_data(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dumb counter instead of CNN\n",
    "  \n",
    "Using $L = 10, n = 6, k = 5$ now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MBL = {\n",
    "    \"obj_name\": 'rho_A',\n",
    "    \"L\": 10,\n",
    "    \"n\": 6,\n",
    "    \"periodic\": True,\n",
    "    \"num_EV\": 5,\n",
    "    \"rho_train_data_dir\": rho_train_data_dir,\n",
    "    \"rho_valid_data_dir\": rho_valid_data_dir\n",
    "}\n",
    "obj_name = MBL['obj_name']\n",
    "L        = MBL['L']\n",
    "n        = MBL['n']\n",
    "periodic = MBL['periodic']\n",
    "p        = MBL['periodic']\n",
    "num_EV   = MBL['num_EV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Count terms\n",
    "def count_offdiagonal(dataset):\n",
    "    \n",
    "    off_ext = {}\n",
    "    off_loc = {}\n",
    "    size = 0\n",
    "\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "\n",
    "        data  = dataset[i]\n",
    "        W     = data['W']\n",
    "        image = np.abs(data['image'].squeeze(axis=0).numpy())\n",
    "        np.fill_diagonal(image, 0)\n",
    "\n",
    "        if i == 0:\n",
    "            size = image.size\n",
    "\n",
    "        for exp in range(33):\n",
    "\n",
    "            if exp not in off_ext:\n",
    "                off_ext[exp] = []\n",
    "            if exp not in off_loc:\n",
    "                off_loc[exp] = []\n",
    "\n",
    "            count = (image <= 10**(-exp)).sum()\n",
    "\n",
    "            if W == 0.5:\n",
    "                off_ext[exp].append(count)\n",
    "            if W == 8.0:\n",
    "                off_loc[exp].append(count)\n",
    "\n",
    "    for (exp_ext, counts_ext), (exp_loc, counts_loc) in zip(off_ext.items(), off_loc.items()):\n",
    "        print('Avg #elements below 1e-{: <2}. Ext: {: >7.2f} | Loc: {: >7.2f} | Diff: {: >+8.2f}'.format(\n",
    "            exp_ext, np.mean(counts_ext), np.mean(counts_loc), np.mean(counts_ext) - np.mean(counts_loc)\n",
    "        ))\n",
    "\n",
    "    print(' ')\n",
    "\n",
    "    for (exp_ext, counts_ext), (exp_loc, counts_loc) in zip(off_ext.items(), off_loc.items()):\n",
    "        print('Avg #elements below 1e-{: <2}. Ext: {: >6.2f}% | Loc: {: >6.2f}% | Diff: {: >+7.2f}%'.format(\n",
    "            exp_ext, np.mean(counts_ext)/size*100, np.mean(counts_loc)/size*100, (np.mean(counts_ext) - np.mean(counts_loc))/size*100\n",
    "        ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_offdiagonal(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calc_probs_counter(dataset):\n",
    "\n",
    "    # Sample model predictions.\n",
    "    result_images  = []\n",
    "    result_targets = []\n",
    "    result_Ws      = []\n",
    "    result_preds   = []\n",
    "    result_probs   = []\n",
    "    size = 0\n",
    "    temp = []\n",
    "    \n",
    "    for i in tqdm(range(len(dataset))):\n",
    "\n",
    "        data  = dataset[i]\n",
    "        W     = data['W']\n",
    "        image = np.abs(data['image'].squeeze(axis=0).numpy())\n",
    "        np.fill_diagonal(image, 0)\n",
    "\n",
    "        if i == 0:\n",
    "            size = image.size\n",
    "\n",
    "        ratio = (image <= 1e-18).mean()\n",
    "        # ratio = (ratio - 0.2974)/.4385\n",
    "        temp.append(ratio)\n",
    "\n",
    "        if ratio < 0.5:\n",
    "            Ps = [1, 0]\n",
    "        else:\n",
    "            Ps = [0, 1]\n",
    "        # Ps = [1-ratio, ratio]\n",
    "\n",
    "        # result_Ws      = result_Ws      + [W]\n",
    "        # result_probs   = result_probs   + [Ps]\n",
    "        result_Ws.append(W)\n",
    "        result_probs.append(Ps)\n",
    "\n",
    "    # print(np.mean(temp))\n",
    "    # print(np.min(temp))\n",
    "    # print(np.max(temp))\n",
    "\n",
    "    result_Ws    = np.array(result_Ws)\n",
    "    result_probs = np.array(result_probs)\n",
    "    sorted_idx   = result_Ws.argsort()\n",
    "    Ws = result_Ws[sorted_idx]\n",
    "    Ps = result_probs[sorted_idx]\n",
    "\n",
    "    # Compute mean and std.\n",
    "    Ws_dict = OrderedDict()\n",
    "    Ws_uniq = []\n",
    "    Ps_mean = []\n",
    "    Ps_std  = []\n",
    "    # Ws is already sorted in `calc_probs()`.\n",
    "    for W, P in zip(Ws, Ps[:,1]):\n",
    "        if W not in Ws_dict:\n",
    "            Ws_dict[W] = []\n",
    "        Ws_dict[W].append(P)\n",
    "    for (W, P) in Ws_dict.items():\n",
    "        Ws_uniq.append(W)\n",
    "        Ps_mean.append(np.mean(P))\n",
    "        Ps_std.append(np.std(P, ddof=1))\n",
    "\n",
    "    return Ws, Ps, np.array(Ws_uniq), np.array(Ps_mean), np.array(Ps_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove y0 because it should be bounded/aligned with y = 0 and y = 1.\n",
    "def sigmoid(x, x0, y0, b, C):\n",
    "    y = C / (1 + np.exp(-b * (x - x0))) + y0\n",
    "    return y\n",
    "\n",
    "# Logit function is the inverse of sigmoid.\n",
    "def logit(y, x0, y0, b, C):\n",
    "    x = np.log((y - y0) / (C - (y - y0))) / b + x0\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_counter_crossing(Ws, Ps, Ws_uniq, Ps_mean, Ps_std, L, n, periodic):\n",
    "\n",
    "    labels = ['Extended (Low W)', 'Localized (High W)']\n",
    "\n",
    "    # Plot probability P(Localized) against W.\n",
    "    # 4:3 aspect ratio.\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(fig_w/dpi, fig_h/dpi), dpi=dpi, squeeze=False)\n",
    "\n",
    "    fig.suptitle('Probability vs Disorder Strength ($L={}, n={}, {}$)'.format(L, n, 'periodic' if periodic else 'non-periodic'))\n",
    "\n",
    "    axes[0,0].axhline(0.5, c='lightgrey', ls='--', label='$P=0.5$')\n",
    "\n",
    "    # Plot averaged values with error bars.\n",
    "    # markers, caps, bars = axes[0,0].errorbar(Ws_uniq, Ps_mean, Ps_std, ls=' ', marker='x', capsize=2, capthick=2, label='P(Localized) Mean')\n",
    "    # Loop through bars and caps and set the alpha value\n",
    "    # [bar.set_alpha(0.5) for bar in bars]\n",
    "    # [cap.set_alpha(0.5) for cap in caps]\n",
    "    \n",
    "    # Actually, don't plot error bars. That's distracting.\n",
    "    axes[0,0].plot(Ws_uniq, Ps_mean, ls=' ', marker='x', c='#0065bd', label='$E_W[P(Localized)]$')\n",
    "\n",
    "    # Plot raw data.\n",
    "    axes[0,0].plot(Ws, Ps,   ls=' ', marker='.', c='#98c6ea', label='$P(Localized)$', alpha=0.05)\n",
    "    axes[0,0].set_xlabel('Disorder strength $W$')\n",
    "    axes[0,0].set_ylabel('Probability $P(Localized)$')\n",
    "\n",
    "    # Curve fit a sigmoid using all data.\n",
    "    # Fitting only the mean with `Ws_uniq` and `Ps_mean` gives identical results.\n",
    "    # popt, pcov = curve_fit(sigmoid, Ws, Ps, p0=[3, 0, 2]) # Add bounds or initial values if it doesn't converge.\n",
    "    popt, pcov = curve_fit(sigmoid, Ws, Ps, p0=[3, 0, 2, 1]) # Add bounds or initial values if it doesn't converge.\n",
    "    # x0, y0, b = popt\n",
    "    x0, y0, b, C = popt\n",
    "    x = np.linspace(0, 10, 100)\n",
    "    y = sigmoid(x, *popt)\n",
    "    axes[0,0].plot(x, y, ls='--', lw=2, c='#e37222', label='Fited sigmoid')\n",
    "    axes[0,0].set_title('$\\sigma(x) = {:.4f} / (1 + Exp(-{:.4f} (x - {:.4f}))) + {:.4f}$'.format(C, b, x0, y0), fontsize=10)\n",
    "    print('Fitted sigmoid function {:.4f} / (1 + Exp(-{:.4f} (x - {:.4f}))) + {:.4f}'.format(C, b, x0, y0))\n",
    "    # print('Fitted sigmoid function 1 / (1 + Exp(-{:.4f} (x - {:.4f}))) + {:.4f}'.format(b, x0, y0))\n",
    "\n",
    "    W_c = logit(0.5, *popt)\n",
    "    perr = np.sqrt(np.diag(pcov))\n",
    "    # The error is actually wrong here. Should have put it through logit.\n",
    "    print('Transition W_C is found to be at W = {:.4f} ± {:.4f}'.format(W_c, perr[0]))\n",
    "    axes[0,0].axvline(W_c, c='r',         ls='--', label='Fitted $W_c$')\n",
    "\n",
    "    for axe in axes:\n",
    "        for ax in axe:\n",
    "            ax.tick_params(direction=\"in\")\n",
    "            ax.legend(loc='best')\n",
    "            ax.set_xlim(0, 6)\n",
    "            # ax.set_ylim(-0.2,1.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "valid_Ws, valid_Ps, valid_Ws_uniq, valid_Ps_mean, valid_Ps_std = calc_probs_counter(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_counter_crossing(valid_Ws, valid_Ps[:,1], valid_Ws_uniq, valid_Ps_mean, valid_Ps_std, MBL['L'], MBL['n'], MBL['periodic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "del data_train\n",
    "del data_valid\n",
    "del train_dataset\n",
    "del valid_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Neural network\n",
    "\n",
    "Since NNs with the same `n` have the same input size, we will evaluate them using the same NN structure. As a side effect, results different `n` are not entirely comparable, but we will compare them anyway because reasons.  \n",
    "\n",
    "Two classes `MBLModel` and `MBLDataset`, modified from a previous CNN facial recoginition code (own work), are used. The model structure and hyperparameters are defined using a dict called `hparams`. Inside it, specifications of the training data are passed using a nested dict `hparams[\"MBL\"]`. The models are stored in a directory structure that mirrors that of the training data (reduced density matrices $\\rho_A$).  \n",
    "\n",
    "Caveat: Validation data isn't really unseen data from the training distribution $W \\in \\{0.5, 8\\}$, but rather random W's that we'll be using them to predict $W_c$.  \n",
    "\n",
    "See the other notebook for data generation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MBL = {\n",
    "    \"obj_name\": 'rho_A',\n",
    "    \"L\": 10,\n",
    "    \"n\": 6,\n",
    "    \"periodic\": True,\n",
    "    \"num_EV\": 5,\n",
    "    \"rho_train_data_dir\": rho_train_data_dir,\n",
    "    \"rho_valid_data_dir\": rho_valid_data_dir\n",
    "}\n",
    "obj_name = MBL['obj_name']\n",
    "L        = MBL['L']\n",
    "n        = MBL['n']\n",
    "periodic = MBL['periodic']\n",
    "p        = MBL['periodic']\n",
    "num_EV   = MBL['num_EV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from MBL_model import MBLModel\n",
    "model_version = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = load_model('model_v{}.pkl.gz'.format(model_version), L, n, periodic, num_EV)\n",
    "# model = load_H_model('model_v{}.pkl.gz'.format(model_version), L, periodic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Visualize model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     22
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def visualize_predictions(model, mode='train'):\n",
    "    \"\"\"Mode = ['train' | 'valid']\"\"\"\n",
    "\n",
    "    labels = ['Extended (Low W)', 'Localized (High W)']\n",
    "\n",
    "    num = 4\n",
    "\n",
    "    # Sample model predictions.\n",
    "    result_images  = []\n",
    "    result_targets = []\n",
    "    result_Ws      = []\n",
    "    result_preds   = []\n",
    "    result_probs   = []\n",
    "\n",
    "    model.eval()\n",
    "    SM  = torch.nn.Softmax()\n",
    "    LSM = torch.nn.LogSoftmax()\n",
    "    if mode == 'train':\n",
    "        dataloader = DataLoader(model.dataset[\"train\"], batch_size=num**2, shuffle=True)#, pin_memory=True)\n",
    "    else:\n",
    "        dataloader = DataLoader(model.dataset[\"val\"], batch_size=num**2, shuffle=True)#, pin_memory=True)\n",
    "\n",
    "    for batch in dataloader:\n",
    "\n",
    "        images, targets, Ws = batch[\"image\"], batch[\"label\"], batch[\"W\"]\n",
    "        images  = images.to(device)\n",
    "        outputs = model(images)\n",
    "        images  = images.to('cpu')\n",
    "        outputs = outputs.to('cpu')\n",
    "\n",
    "        preds   = outputs.argmax(axis=1)\n",
    "        probs   = SM(outputs)\n",
    "        # probs2  = - LSM(outputs)\n",
    "        # out_sum = probs2[:,0] + probs2[:,1]\n",
    "        # probs2[:,0] = probs2[:,0] / out_sum\n",
    "        # probs2[:,1] = probs2[:,1] / out_sum\n",
    "        # Simple averaging doesn't work, because it's negative...\n",
    "        # out_sum = outputs[:,0] + outputs[:,1]\n",
    "        # outputs[:,0] = outputs[:,0] / out_sum\n",
    "        # outputs[:,1] = outputs[:,1] / out_sum\n",
    "        shape   = images.shape\n",
    "        result_images  = result_images  + images.reshape(shape[0], shape[2], shape[3]).tolist()\n",
    "        result_targets = result_targets + targets.tolist()\n",
    "        result_Ws      = result_Ws      + Ws.tolist()\n",
    "        result_preds   = result_preds   + preds.tolist()\n",
    "        result_probs   = result_probs   + probs.tolist()\n",
    "        # result_probs   = result_probs   + probs2.tolist()\n",
    "        break # Because we only need 25 images.\n",
    "\n",
    "    # Display images.\n",
    "    sample_idx = np.random.randint(0, len(result_preds), size=num**2)\n",
    "\n",
    "    # Square image.\n",
    "    fig, axes = plt.subplots(num, num, figsize=(fig_w/dpi,fig_w/dpi), dpi=dpi, squeeze=False)\n",
    "\n",
    "    for i, idx in enumerate(sample_idx):\n",
    "        W      = result_Ws[idx]\n",
    "        W_in   = result_targets[idx]\n",
    "        W_pred = result_preds[idx]\n",
    "        W_prob = result_probs[idx]\n",
    "        if W_in == W_pred:\n",
    "            ec = 'lime'\n",
    "            tc = 'white'\n",
    "        else:\n",
    "            ec = 'red'\n",
    "            tc = 'red'\n",
    "\n",
    "        axes[i%num,i//num].imshow(np.abs(result_images[idx]))\n",
    "\n",
    "        axes[i%num,i//num].tick_params(color=ec, labelcolor=ec)\n",
    "        for spine in axes[i%num,i//num].spines.values():\n",
    "            spine.set_edgecolor(ec)\n",
    "            spine.set_linewidth(2)\n",
    "\n",
    "        annotation  = 'Input  : \\n{}\\nW={:.2f}\\n\\n'.format(labels[W_in], W)\n",
    "        annotation += 'Predict: \\n{}\\n{:.0f}%'.format(labels[W_pred], W_prob[W_pred]*100)\n",
    "        # annotation += 'Predict: \\n{}\\n{:.0f}%'.format(labels[W_pred], W_prob[(W_pred+1)%2]*100)\n",
    "        # axes[i%num,i//num].annotate(annotation, (0.5,0.275), xycoords='axes fraction', ha='center', color='w', bbox=dict(facecolor='none', edgecolor=ec, boxstyle='round,pad=1', linewidth=2))\n",
    "        axes[i%num,i//num].annotate(annotation, (0.5,0.13), xycoords='axes fraction', ha='center', color=tc, fontsize=9)#, bbox=dict(facecolor='none', edgecolor=ec, boxstyle='round,pad=0.5', linewidth=1))\n",
    "\n",
    "    for axe in axes:\n",
    "        for ax in axe:\n",
    "            # ax.legend(loc='best')\n",
    "            ax.xaxis.set_ticklabels([])\n",
    "            ax.yaxis.set_ticklabels([])\n",
    "            ax.xaxis.set_visible(False)\n",
    "            ax.yaxis.set_visible(False)\n",
    "\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Sample training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_predictions(model, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Sample validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visualize_predictions(model, 'valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     20
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model_core(model, dataset):\n",
    "\n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    SM = torch.nn.Softmax()\n",
    "    dataloader = DataLoader(dataset, batch_size=1000, shuffle=False)#, pin_memory=True)\n",
    "    loss = 0\n",
    "    n_correct = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        images, targets = batch[\"image\"], batch[\"label\"]\n",
    "        images  = images.to(device)\n",
    "        outputs = model(images).to('cpu')\n",
    "        preds   = outputs.argmax(axis=1)\n",
    "        # print(SM(outputs))\n",
    "        loss += criterion(outputs, targets).item()\n",
    "        n_correct += (preds == targets).sum().item()\n",
    "\n",
    "    return loss, n_correct / len(dataset)\n",
    "\n",
    "def evaluate_model(model):\n",
    "\n",
    "    print(\"Training accuracy  : {:.4f}%\".format(evaluate_model_core(model, model.dataset[\"train\"])[1] * 100))\n",
    "    print(\"Validation accuracy: {:.4f}%\".format(evaluate_model_core(model, model.dataset[\"val\"])[1]   * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "del model.dataset[\"train\"]\n",
    "del model.dataset[\"val\"]\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Plot curve fitting (Estimate transition disorder strength)\n",
    "Plot sigmoid curve fitting of transition disorder strength $W_c$ with error.  \n",
    "$L = 12, n = 6, k = 1$ is selected.    \n",
    "Actually $L = 10, n = 6, k = 5$ now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MBL = {\n",
    "    \"obj_name\": 'rho_A',\n",
    "    \"L\": 10,\n",
    "    \"n\": 6,\n",
    "    \"periodic\": True,\n",
    "    \"num_EV\": 5,\n",
    "    \"rho_train_data_dir\": rho_train_data_dir,\n",
    "    \"rho_valid_data_dir\": rho_valid_data_dir\n",
    "}\n",
    "obj_name = MBL['obj_name']\n",
    "L        = MBL['L']\n",
    "n        = MBL['n']\n",
    "periodic = MBL['periodic']\n",
    "p        = MBL['periodic']\n",
    "num_EV   = MBL['num_EV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Two classes.\n",
    "labels = ['Extended (Low W)', 'Localized (High W)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, x0, y0, b):\n",
    "    y = 1 / (1 + np.exp(-b * (x - x0))) + y0\n",
    "    return y\n",
    "\n",
    "# Logit function is the inverse of sigmoid.\n",
    "def logit(y, x0, y0, b):\n",
    "    x = np.log((y - y0) / (1 - (y - y0))) / b + x0\n",
    "    return x\n",
    "\n",
    "print(sigmoid(0,0,0,1))\n",
    "print(logit(0.5,0,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove y0 because it should be bounded/aligned with y = 0 and y = 1.\n",
    "def sigmoid(x, x0, b):\n",
    "    y = 1 / (1 + np.exp(-b * (x - x0))) # + y0\n",
    "    return y\n",
    "\n",
    "# Logit function is the inverse of sigmoid.\n",
    "def logit(y, x0, b):\n",
    "    x = np.log((y) / (1 - (y))) / b + x0\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calc_probs(model, dataset):\n",
    "\n",
    "    # Sample model predictions.\n",
    "    result_images  = []\n",
    "    result_targets = []\n",
    "    result_Ws      = []\n",
    "    result_preds   = []\n",
    "    result_probs   = []\n",
    "\n",
    "    model.eval()\n",
    "    SM  = torch.nn.Softmax()\n",
    "    LSM = torch.nn.LogSoftmax()\n",
    "    dataloader = DataLoader(dataset, batch_size=200, shuffle=False)#, pin_memory=True)\n",
    "    for batch in dataloader:\n",
    "\n",
    "        images, targets, Ws = batch[\"image\"], batch[\"label\"], batch[\"W\"]\n",
    "        images  = images.to(device)\n",
    "        outputs = model(images)\n",
    "        images  = images.to('cpu')\n",
    "        outputs = outputs.to('cpu')\n",
    "\n",
    "        preds   = outputs.argmax(axis=1)\n",
    "        Ps      = SM(outputs)\n",
    "        shape   = images.shape\n",
    "        result_images  = result_images  + images.reshape(shape[0], shape[2], shape[3]).tolist()\n",
    "        result_targets = result_targets + targets.tolist()\n",
    "        result_Ws      = result_Ws      + Ws.tolist()\n",
    "        result_preds   = result_preds   + preds.tolist()\n",
    "        result_probs   = result_probs   + Ps.tolist()\n",
    "\n",
    "    result_Ws    = np.array(result_Ws)\n",
    "    result_probs = np.array(result_probs)\n",
    "    sorted_idx   = result_Ws.argsort()\n",
    "    Ws = result_Ws[sorted_idx]\n",
    "    Ps = result_probs[sorted_idx]\n",
    "\n",
    "    # Compute mean and std.\n",
    "    Ws_dict = OrderedDict()\n",
    "    Ws_uniq = []\n",
    "    Ps_mean = []\n",
    "    Ps_std  = []\n",
    "    # Ws is already sorted in `calc_probs()`.\n",
    "    for W, P in zip(Ws, Ps[:,1]):\n",
    "        if W not in Ws_dict:\n",
    "            Ws_dict[W] = []\n",
    "        Ws_dict[W].append(P)\n",
    "    for (W, P) in Ws_dict.items():\n",
    "        Ws_uniq.append(W)\n",
    "        Ps_mean.append(np.mean(P))\n",
    "        Ps_std.append(np.std(P, ddof=1))\n",
    "\n",
    "    return Ws, Ps, np.array(Ws_uniq), np.array(Ps_mean), np.array(Ps_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# data = calc_probs(model, model.dataset[\"val\"])\n",
    "# save_eval_valid(  data, model_version, L, n, periodic, num_EV)\n",
    "# save_H_eval_valid(data, model_version, L, periodic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_crossing(Ws, Ps, Ws_uniq, Ps_mean, Ps_std, L, n, periodic):\n",
    "\n",
    "    labels = ['Extended (Low W)', 'Localized (High W)']\n",
    "\n",
    "    # Plot probability P(Localized) against W.\n",
    "    # 4:3 aspect ratio.\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(fig_w/dpi, fig_h/dpi), dpi=dpi, squeeze=False)\n",
    "\n",
    "    fig.suptitle('Probability vs Disorder Strength ($L={}, n={}, {}$)'.format(L, n, 'periodic' if periodic else 'non-periodic'))\n",
    "    axes[0,0].axhline(0.5, c='lightgrey', ls='--', label='$P=0.5$')\n",
    "\n",
    "    # Plot averaged values with error bars.\n",
    "    # markers, caps, bars = axes[0,0].errorbar(Ws_uniq, Ps_mean, Ps_std, ls=' ', marker='x', capsize=2, capthick=2, label='P(Localized) Mean')\n",
    "    # Loop through bars and caps and set the alpha value\n",
    "    # [bar.set_alpha(0.5) for bar in bars]\n",
    "    # [cap.set_alpha(0.5) for cap in caps]\n",
    "    \n",
    "    # Actually, don't plot error bars. That's distracting.\n",
    "    axes[0,0].plot(Ws_uniq, Ps_mean, ls=' ', marker='x', c='#0065bd', label='$E_W[P(Localized)]$')\n",
    "\n",
    "    # Plot raw data.\n",
    "    axes[0,0].plot(Ws, Ps, ls=' ', marker='.', c='#98c6ea', label='$P(Localized)$', alpha=0.05)\n",
    "    axes[0,0].set_xlabel('Disorder strength $W$')\n",
    "    axes[0,0].set_ylabel('Probability $P(Localized)$')\n",
    "\n",
    "    # Curve fit a sigmoid using all data.\n",
    "    # Fitting only the mean with `Ws_uniq` and `Ps_mean` gives identical results.\n",
    "    # popt, pcov = curve_fit(sigmoid, Ws, Ps, p0=[3, 0, 2]) # Add bounds or initial values if it doesn't converge.\n",
    "    popt, pcov = curve_fit(sigmoid, Ws, Ps, p0=[3, 2]) # Add bounds or initial values if it doesn't converge.\n",
    "    # x0, y0, b = popt\n",
    "    x0, b = popt\n",
    "    x = np.linspace(0, 10, 100)\n",
    "    y = sigmoid(x, *popt)\n",
    "    axes[0,0].plot(x, y, ls='--', lw=2, c='#e37222', label='Fited sigmoid')\n",
    "    axes[0,0].set_title('$\\sigma(x) = 1 / (1 + Exp(-{:.4f} (x - {:.4f})))$'.format(b, x0), fontsize=10)\n",
    "    # print('Fitted sigmoid function 1 / (1 + Exp(-{:.4f} (x - {:.4f}))) + {:.4f}'.format(b, x0, y0))\n",
    "    print('Fitted sigmoid function 1 / (1 + Exp(-{:.4f} (x - {:.4f})))'.format(b, x0))\n",
    "\n",
    "    W_c = logit(0.5, *popt)\n",
    "    perr = np.sqrt(np.diag(pcov))\n",
    "    print('Transition W_C is found to be at W = {:.4f} ± {:.4f}'.format(W_c, perr[0]))\n",
    "    axes[0,0].axvline(W_c, c='r', ls='--', label='Fitted $W_c$')\n",
    "\n",
    "    for axe in axes:\n",
    "        for ax in axe:\n",
    "            ax.tick_params(direction=\"in\")\n",
    "            ax.legend(loc='best')\n",
    "            ax.set_xlim(0, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_version = 1\n",
    "# num_EV = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "valid_Ws, valid_Ps, valid_Ws_uniq, valid_Ps_mean, valid_Ps_std = load_eval_valid(  model_version, 14, 6, periodic, num_EV)\n",
    "plot_crossing(valid_Ws, valid_Ps[:,1], valid_Ws_uniq, valid_Ps_mean, valid_Ps_std, 14, 6, periodic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "valid_Ws, valid_Ps, valid_Ws_uniq, valid_Ps_mean, valid_Ps_std = load_eval_valid(  model_version, 12, 6, periodic, num_EV)\n",
    "# valid_Ws, valid_Ps, valid_Ws_uniq, valid_Ps_mean, valid_Ps_std = load_H_eval_valid(model_version, L, periodic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_crossing(valid_Ws, valid_Ps[:,1], valid_Ws_uniq, valid_Ps_mean, valid_Ps_std, 12, 6, periodic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "valid_Ws, valid_Ps, valid_Ws_uniq, valid_Ps_mean, valid_Ps_std = load_eval_valid(  model_version, 10, 6, periodic, num_EV)\n",
    "plot_crossing(valid_Ws, valid_Ps[:,1], valid_Ws_uniq, valid_Ps_mean, valid_Ps_std, 10, 6, periodic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "valid_Ws, valid_Ps, valid_Ws_uniq, valid_Ps_mean, valid_Ps_std = load_eval_valid(  model_version, 8, 6, periodic, num_EV)\n",
    "plot_crossing(valid_Ws, valid_Ps[:,1], valid_Ws_uniq, valid_Ps_mean, valid_Ps_std, 8, 6, periodic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Why L=13 n=5 k=1 is so skewed:\n",
    "valid_Ws, valid_Ps, valid_Ws_uniq, valid_Ps_mean, valid_Ps_std = load_eval_valid(  model_version, 13, 5, periodic, 1)\n",
    "plot_crossing(valid_Ws, valid_Ps[:,1], valid_Ws_uniq, valid_Ps_mean, valid_Ps_std, 13, 5, periodic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Why L=14 n=6 k=1 is so skewed:\n",
    "valid_Ws, valid_Ps, valid_Ws_uniq, valid_Ps_mean, valid_Ps_std = load_eval_valid(  model_version, 14, 6, False, 1)\n",
    "plot_crossing(valid_Ws, valid_Ps[:,1], valid_Ws_uniq, valid_Ps_mean, valid_Ps_std, 14, 6, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def test_slope():\n",
    "\n",
    "    labels = ['Extended (Low W)', 'Localized (High W)']\n",
    "\n",
    "    # Plot probability P(Localized) against W.\n",
    "    # 4:3 aspect ratio.\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(fig_w/dpi, fig_h/dpi), dpi=dpi, squeeze=False)\n",
    "\n",
    "    axes[0,0].axhline(0.5, c='lightgrey', ls='--', label='$P=0.5$')\n",
    "\n",
    "    for b in [0.1, 1, 10]:\n",
    "        x0 = 3\n",
    "        x = np.linspace(0, 10, 100)\n",
    "        y = sigmoid(x, 3, b)\n",
    "        axes[0,0].plot(x, y, ls='--', lw=2, label='Fited sigmoid {}'.format(b))\n",
    "        # print('Fitted sigmoid function 1 / (1 + Exp(-{:.4f} (x - {:.4f}))) + {:.4f}'.format(b, x0, y0))\n",
    "        print('Fitted sigmoid function 1 / (1 + Exp(-{:.4f} (x - {:.4f})))'.format(b, x0))\n",
    "\n",
    "    for axe in axes:\n",
    "        for ax in axe:\n",
    "            ax.tick_params(direction=\"in\")\n",
    "            ax.legend(loc='best')\n",
    "            ax.set_xlim(0, 6)\n",
    "\n",
    "test_slope()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Batch process $W_c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fit_Wc(Ws, Ps):\n",
    "    \"\"\"Curve fit a sigmoid using all data to obtain critical transition strength W_c.\n",
    "\n",
    "    Fitting only the mean with `Ws_uniq` and `Ps_mean` gives identical results.\n",
    "    \"\"\"\n",
    "\n",
    "    # popt, pcov = curve_fit(sigmoid, Ws, Ps, p0=[3, 0, 2]) # Add bounds or initial values if it doesn't converge.\n",
    "    popt, pcov = curve_fit(sigmoid, Ws, Ps, p0=[3, 2]) # Add bounds or initial values if it doesn't converge.\n",
    "    # x0, y0, b = popt\n",
    "    x0, b = popt\n",
    "\n",
    "    # Identifying tran\n",
    "    # Wc = logit(0.5, *popt) # When y0 is considered. Otherwise W_c = x0.\n",
    "    Wc = x0\n",
    "\n",
    "    # Error terms.\n",
    "    perr = np.sqrt(np.diag(pcov))\n",
    "    We, be = perr\n",
    "    \n",
    "    return Wc, We, b, be\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_key(L, n, periodic, num_EV):\n",
    "    \"\"\"Too lazy to code a nested data structure.\"\"\"\n",
    "    return json.dumps([L, n, periodic, num_EV])#[1:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Batch generate reduced density matrix.\n",
    "\n",
    "# k = 5\n",
    "J  = 1                      # Always = 1\n",
    "# Ls = list(range(8,15,2))    # System sizes L.\n",
    "Ls = list(range(8,15))      # System sizes L.\n",
    "ps = [False, True]          # Periodic or not.\n",
    "et = []                     # Execution time.\n",
    "# num_EVs = [k]               # Number of eigenvalues near zero to save.\n",
    "num_EVs = [1,5]             # Number of eigenvalues near zero to save.\n",
    "model_version = 1\n",
    "fit_result = {}\n",
    "\n",
    "iter_count = 0\n",
    "for L in Ls: # 7\n",
    "    for n in range(1,9): # 8\n",
    "        for periodic in ps: # 2\n",
    "            for num_EV in num_EVs: # 2\n",
    "                iter_count += 1\n",
    "                # start_time = time.time()\n",
    "                print('{} | Curve fitting W_c for L={:02d} | n={:d} | num_EV={} | periodic={: <5}...'.format(dt(), L, n, num_EV, str(periodic)), flush=True)\n",
    "                try:\n",
    "                    Ws, Ps, Ws_uniq, Ps_mean, Ps_std = load_eval_valid(  model_version, L, n, periodic, num_EV)\n",
    "                    Wc, We, b, be = fit_Wc(Ws, Ps[:,1])\n",
    "                    key = get_key(L, n, periodic, num_EV)\n",
    "                    fit_result[key] = Wc, We, b, be\n",
    "                except Exception as err:\n",
    "                    print('{} | DATA NOT FOUND!'.format(dt()))\n",
    "                    print(err)\n",
    "                    key = get_key(L, n, periodic, num_EV)\n",
    "                    Wc = np.nan\n",
    "                    fit_result[key] = np.nan, np.nan, np.nan, np.nan # Return nan, s.t. matplotlib knows to skip.\n",
    "                # exec_time = time.time() - start_time\n",
    "                # et.append(exec_time)\n",
    "                print('{} | Computed Wc = {:.4f}: L={:02d} | n={:d} | num_EV={} | periodic={: <5}.'.format(dt(), Wc, L, n, num_EV, str(periodic)), flush=True)\n",
    "                # print('{} | Execution took {: 8.2f}s or {: 6.2f}min.'.format(dt(), exec_time, exec_time/60), flush=True)\n",
    "                print(' ', flush=True)\n",
    "\n",
    "print('Total number of iterations: {}'.format(iter_count)) # = 48... = 224\n",
    "# if check_shutdown_signal():\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     7
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import to_rgb, to_rgba\n",
    "def get_rgba(color, alpha_arr):\n",
    "    r, g, b = to_rgb(color)\n",
    "    colors = [(r, g, b, alpha) for alpha in alpha_arr]\n",
    "    return colors\n",
    "\n",
    "# https://stackoverflow.com/a/39634143\n",
    "def list_from_cycle(cycle):\n",
    "    first = next(cycle)\n",
    "    result = [first]\n",
    "    for current in cycle:\n",
    "        if current == first:\n",
    "            break\n",
    "        result.append(current)\n",
    "\n",
    "    # Reset iterator state:\n",
    "    for current in cycle:\n",
    "        if current == result[-1]:\n",
    "            break\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     44,
     63,
     81,
     127,
     145,
     163
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_scaling_W(fit_result, all_data=False):\n",
    "\n",
    "    labels = ['Extended (Low W)', 'Localized (High W)']\n",
    "\n",
    "    # Plot Wc against L.\n",
    "    # 4:3 aspect ratio.\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(fig_w/dpi*2, fig_h/dpi*3), dpi=dpi, squeeze=False, sharey='row')\n",
    "\n",
    "    fig.suptitle('Scaling of Critical Disorder Strength $W_c$', fontsize=14)\n",
    "    axes[0,0].set_title('Periodic boundary condition')\n",
    "    axes[0,1].set_title('Open boundary condition')\n",
    "    axes[0,1].yaxis.tick_right()\n",
    "    axes[0,1].yaxis.set_label_position(\"right\")\n",
    "    axes[1,1].yaxis.tick_right()\n",
    "    axes[1,1].yaxis.set_label_position(\"right\")\n",
    "    axes[2,1].yaxis.tick_right()\n",
    "    axes[2,1].yaxis.set_label_position(\"right\")\n",
    "    \n",
    "    axes[0,0].set_xlabel('System size $L$')\n",
    "    axes[0,1].set_xlabel('System size $L$')\n",
    "    axes[1,0].set_xlabel('Subsystem size $n$')\n",
    "    axes[1,1].set_xlabel('Subsystem size $n$')\n",
    "    axes[2,0].set_xlabel('$n / L$')\n",
    "    axes[2,1].set_xlabel('$n / L$')\n",
    "    axes[2,0].set_xlabel('$n log(n) + (L-n) log(L-n)$')\n",
    "    axes[2,1].set_xlabel('$n log(n) + (L-n) log(L-n)$')\n",
    "\n",
    "    if all_data:\n",
    "        Ls = list(range(8,15))\n",
    "        ns = list(range(1, 9))\n",
    "    else:\n",
    "        Ls = list(range(8,13))\n",
    "        ns = list(range(1, 7))\n",
    "    Ls_arr = np.array(Ls)\n",
    "    ns_arr = np.array(ns)\n",
    "\n",
    "\n",
    "\n",
    "    # ================================================================================\n",
    "    # First plot `k = 1` data from Dataset 3.\n",
    "    # ================================================================================\n",
    "\n",
    "    k = 1\n",
    "\n",
    "    for p in [False, True]:\n",
    "        for n in ns:\n",
    "            Wcs = []\n",
    "            Wcs_err = []\n",
    "            for L in Ls:\n",
    "                key = get_key(L, n, p, k)\n",
    "                Wc, We, b, be = fit_result[key]\n",
    "                Wcs.append(Wc)\n",
    "                Wcs_err.append(We)\n",
    "            # Errorbars are not shown because they are less than a pixel.\n",
    "            Wcs = np.array(Wcs)\n",
    "            nan_mask = np.isfinite(Wcs)\n",
    "            # np.power(Ls_arr[nan_mask],5/4)\n",
    "            if p:\n",
    "                axes[0,0].plot(Ls_arr[nan_mask], Wcs[nan_mask], marker='x', ls='-', label='k={:d} n={:d}'.format(k,n))\n",
    "            else:\n",
    "                axes[0,1].plot(Ls_arr[nan_mask], Wcs[nan_mask], marker='+', ls='-', label='k={:d} n={:d}'.format(k,n))\n",
    "\n",
    "\n",
    "    for p in [False, True]:\n",
    "        for L in Ls:\n",
    "            Wcs = []\n",
    "            Wcs_err = []\n",
    "            for n in ns:\n",
    "                key = get_key(L, n, p, k)\n",
    "                Wc, We, b, be = fit_result[key]\n",
    "                Wcs.append(Wc)\n",
    "                Wcs_err.append(We)\n",
    "            # Errorbars are not shown because they are less than a pixel.\n",
    "            Wcs = np.array(Wcs)\n",
    "            nan_mask = np.isfinite(Wcs)\n",
    "            if p:\n",
    "                axes[1,0].plot(ns_arr[nan_mask], Wcs[nan_mask], marker='x', ls='-', label='k={:d} L={:d}'.format(k,L))\n",
    "            else:\n",
    "                axes[1,1].plot(ns_arr[nan_mask], Wcs[nan_mask], marker='+', ls='-', label='k={:d} L={:d}'.format(k,L))\n",
    "\n",
    "\n",
    "    for p in [False, True]:\n",
    "        nL = []\n",
    "        Wcs = []\n",
    "        Wcs_err = []\n",
    "        for i, L in enumerate(Ls):\n",
    "            nL = []\n",
    "            Wcs = []\n",
    "            Wcs_err = []\n",
    "            for n in ns:\n",
    "                key = get_key(L, n, p, k)\n",
    "                Wc, We, b, be = fit_result[key]\n",
    "                # nL.append(n / L)\n",
    "                # nL.append(1 / (n / L))\n",
    "                # nL.append((n / (L - n)))\n",
    "                nL.append(n * np.log(n) + (L-n) * np.log(L-n))\n",
    "                Wcs.append(Wc)\n",
    "                Wcs_err.append(We)\n",
    "            # Errorbars are not shown because they are less than a pixel.\n",
    "            nL  = np.array(nL)\n",
    "            Wcs = np.array(Wcs)\n",
    "            nan_mask = np.isfinite(Wcs)\n",
    "            if p:\n",
    "                # Applying alpha to a list of colors somehow doesn't work.\n",
    "                # cs = get_rgba(list_from_cycle(axes[2,0]._get_lines.prop_cycler)[0]['color'], (np.array(ns)+1)/L)\n",
    "                # print(len(nL), len(ns), len(cs))\n",
    "                # cs=np.array(cs).reshape(-1,4)\n",
    "                axes[2,0].plot(nL[nan_mask], Wcs[nan_mask], marker='x', ls='-', label='k={:d} L={:d}'.format(k,L))\n",
    "            else:\n",
    "                axes[2,1].plot(nL[nan_mask], Wcs[nan_mask], marker='+', ls='-', label='k={:d} L={:d}'.format(k,L))\n",
    "        # if p:\n",
    "        #     axes[2,0].plot(nL, Wcs, marker='x', ls='' )\n",
    "        # else:\n",
    "        #     axes[2,1].plot(nL, Wcs, marker='+', ls='')\n",
    "\n",
    "\n",
    "\n",
    "    # ================================================================================\n",
    "    # Then plot `k = 5` data from Dataset 2, at half opacity.\n",
    "    # ================================================================================\n",
    "\n",
    "    for row, axe in enumerate(axes):\n",
    "        for col, ax in enumerate(axe):\n",
    "            ax.set_prop_cycle(None)\n",
    "\n",
    "    k = 5\n",
    "\n",
    "    for p in [False, True]:\n",
    "        for n in ns:\n",
    "            Wcs = []\n",
    "            Wcs_err = []\n",
    "            for L in Ls:\n",
    "                key = get_key(L, n, p, k)\n",
    "                Wc, We, b, be = fit_result[key]\n",
    "                Wcs.append(Wc)\n",
    "                Wcs_err.append(We)\n",
    "            # Errorbars are not shown because they are less than a pixel.\n",
    "            Wcs = np.array(Wcs)\n",
    "            nan_mask = np.isfinite(Wcs)\n",
    "            if p:\n",
    "                axes[0,0].plot(Ls_arr[nan_mask], Wcs[nan_mask], marker='x', ls='--', alpha=0.5, label='k={:d} n={:d}'.format(k,n))\n",
    "            else:\n",
    "                axes[0,1].plot(Ls_arr[nan_mask], Wcs[nan_mask], marker='+', ls='--', alpha=0.5, label='k={:d} n={:d}'.format(k,n))\n",
    "\n",
    "\n",
    "    for p in [False, True]:\n",
    "        for L in Ls:\n",
    "            Wcs = []\n",
    "            Wcs_err = []\n",
    "            for n in ns:\n",
    "                key = get_key(L, n, p, k)\n",
    "                Wc, We, b, be = fit_result[key]\n",
    "                Wcs.append(Wc)\n",
    "                Wcs_err.append(We)\n",
    "            # Errorbars are not shown because they are less than a pixel.\n",
    "            Wcs = np.array(Wcs)\n",
    "            nan_mask = np.isfinite(Wcs)\n",
    "            if p:\n",
    "                axes[1,0].plot(ns_arr[nan_mask], Wcs[nan_mask], marker='x', ls='--', alpha=0.5, label='k={:d} L={:d}'.format(k,L))\n",
    "            else:\n",
    "                axes[1,1].plot(ns_arr[nan_mask], Wcs[nan_mask], marker='+', ls='--', alpha=0.5, label='k={:d} L={:d}'.format(k,L))\n",
    "\n",
    "\n",
    "    for p in [False, True]:\n",
    "        nL = []\n",
    "        Wcs = []\n",
    "        Wcs_err = []\n",
    "        for i, L in enumerate(Ls):\n",
    "            nL = []\n",
    "            Wcs = []\n",
    "            Wcs_err = []\n",
    "            for n in ns:\n",
    "                key = get_key(L, n, p, k)\n",
    "                Wc, We, b, be = fit_result[key]\n",
    "                # nL.append(n / L)\n",
    "                # nL.append(1 / (n / L))\n",
    "                # nL.append((n / (L - n)))\n",
    "                nL.append(n * np.log(n) + (L-n) * np.log(L-n))\n",
    "                Wcs.append(Wc)\n",
    "                Wcs_err.append(We)\n",
    "            # Errorbars are not shown because they are less than a pixel.\n",
    "            nL  = np.array(nL)\n",
    "            Wcs = np.array(Wcs)\n",
    "            nan_mask = np.isfinite(Wcs)\n",
    "            if p:\n",
    "                # Applying alpha to a list of colors somehow doesn't work.\n",
    "                # cs = get_rgba(list_from_cycle(axes[2,0]._get_lines.prop_cycler)[0]['color'], (np.array(ns)+1)/L)\n",
    "                # print(len(nL), len(ns), len(cs))\n",
    "                # cs=np.array(cs).reshape(-1,4)\n",
    "                axes[2,0].plot(nL[nan_mask], Wcs[nan_mask], marker='x', ls='--', alpha=0.5, label='k={:d} L={:d}'.format(k,L))\n",
    "            else:\n",
    "                axes[2,1].plot(nL[nan_mask], Wcs[nan_mask], marker='+', ls='--', alpha=0.5, label='k={:d} L={:d}'.format(k,L))\n",
    "        # if p:\n",
    "        #     axes[2,0].plot(nL, Wcs, marker='x', ls='' , alpha=0.5)\n",
    "        # else:\n",
    "        #     axes[2,1].plot(nL, Wcs, marker='+', ls='', alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "    # ================================================================================\n",
    "    # Configure axes settings.\n",
    "    # ================================================================================\n",
    "\n",
    "    for row, axe in enumerate(axes):\n",
    "        for col, ax in enumerate(axe):\n",
    "            if row == 0:\n",
    "                # ax.legend(loc=(0.03, 0.58), ncol=3, framealpha=0.5)#, bbox_to_anchor=(1, 0.05))\n",
    "                if col == 0:\n",
    "                    if all_data:\n",
    "                        ax.legend(loc='lower left', ncol=3, framealpha=0.5)\n",
    "                    else:\n",
    "                        ax.legend(loc='lower right', ncol=3, framealpha=0.5, bbox_to_anchor=(0.995, 0.58))\n",
    "                if col == 1:\n",
    "                    if all_data:\n",
    "                        ax.legend(loc='lower left', ncol=3, framealpha=0.5)\n",
    "                    else:\n",
    "                        ax.legend(loc='lower right', ncol=3, framealpha=0.5, bbox_to_anchor=(0.995, 0.48))\n",
    "            if row == 1:\n",
    "                if all_data:\n",
    "                    ax.legend(loc='lower left', ncol=2, framealpha=0.5)\n",
    "                else:\n",
    "                    ax.legend(loc='upper right', ncol=2, framealpha=0.5)\n",
    "            if row == 2:\n",
    "                if all_data:\n",
    "                    ax.legend(loc='lower left', ncol=2, framealpha=0.5)\n",
    "                else:\n",
    "                    ax.legend(loc='lower right', ncol=2, framealpha=0.5)\n",
    "\n",
    "            ax.tick_params(direction=\"in\")\n",
    "            ax.set_ylabel('Critical disorder strength $W_c$')\n",
    "            ax.xaxis.get_major_locator().set_params(integer=True)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    # fig.subplots_adjust(top=0.88)\n",
    "    fig.subplots_adjust(top=0.94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     44,
     62,
     80,
     122,
     140,
     158
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_scaling_b(fit_result, all_data=False):\n",
    "\n",
    "    labels = ['Extended (Low W)', 'Localized (High W)']\n",
    "\n",
    "    # Plot Wc against L.\n",
    "    # 4:3 aspect ratio.\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(fig_w/dpi*2, fig_h/dpi*3), dpi=dpi, squeeze=False, sharey='row')\n",
    "\n",
    "    fig.suptitle('Scaling of Transition Steepness $b$', fontsize=14)\n",
    "    axes[0,0].set_title('Periodic boundary condition')\n",
    "    axes[0,1].set_title('Open boundary condition')\n",
    "    axes[0,1].yaxis.tick_right()\n",
    "    axes[0,1].yaxis.set_label_position(\"right\")\n",
    "    axes[1,1].yaxis.tick_right()\n",
    "    axes[1,1].yaxis.set_label_position(\"right\")\n",
    "    axes[2,1].yaxis.tick_right()\n",
    "    axes[2,1].yaxis.set_label_position(\"right\")\n",
    "    \n",
    "    axes[0,0].set_xlabel('System size $L$')\n",
    "    axes[0,1].set_xlabel('System size $L$')\n",
    "    axes[1,0].set_xlabel('Subsystem size $n$')\n",
    "    axes[1,1].set_xlabel('Subsystem size $n$')\n",
    "    axes[2,0].set_xlabel('$n / L$')\n",
    "    axes[2,1].set_xlabel('$n / L$')\n",
    "    axes[2,0].set_xlabel('$n log(n) + (L-n) log(L-n)$')\n",
    "    axes[2,1].set_xlabel('$n log(n) + (L-n) log(L-n)$')\n",
    "\n",
    "    if all_data:\n",
    "        Ls = list(range(8,15))\n",
    "        ns = list(range(1, 9))\n",
    "    else:\n",
    "        Ls = list(range(8,13))\n",
    "        ns = list(range(1, 7))\n",
    "    Ls_arr = np.array(Ls)\n",
    "    ns_arr = np.array(ns)\n",
    "\n",
    "\n",
    "\n",
    "    # ================================================================================\n",
    "    # First plot `k = 1` data from Dataset 3.\n",
    "    # ================================================================================\n",
    "\n",
    "    k = 1\n",
    "\n",
    "    for p in [False, True]:\n",
    "        for n in ns:\n",
    "            bcs = []\n",
    "            bcs_err = []\n",
    "            for L in Ls:\n",
    "                key = get_key(L, n, p, k)\n",
    "                Wc, We, b, be = fit_result[key]\n",
    "                bcs.append(b)\n",
    "                bcs_err.append(be)\n",
    "            # Errorbars are not shown because they are less than a pixel.\n",
    "            bcs = np.array(bcs)\n",
    "            nan_mask = np.isfinite(bcs)\n",
    "            if p:\n",
    "                axes[0,0].plot(Ls_arr[nan_mask], bcs[nan_mask], marker='x', ls='-', label='k={:d} n={:d}'.format(k,n))\n",
    "            else:\n",
    "                axes[0,1].plot(Ls_arr[nan_mask], bcs[nan_mask], marker='+', ls='-', label='k={:d} n={:d}'.format(k,n))\n",
    "\n",
    "\n",
    "    for p in [False, True]:\n",
    "        for L in Ls:\n",
    "            bcs = []\n",
    "            bcs_err = []\n",
    "            for n in ns:\n",
    "                key = get_key(L, n, p, k)\n",
    "                Wc, We, b, be = fit_result[key]\n",
    "                bcs.append(b)\n",
    "                bcs_err.append(be)\n",
    "            # Errorbars are not shown because they are less than a pixel.\n",
    "            bcs = np.array(bcs)\n",
    "            nan_mask = np.isfinite(bcs)\n",
    "            if p:\n",
    "                axes[1,0].plot(ns_arr[nan_mask], bcs[nan_mask], marker='x', ls='-', label='k={:d} L={:d}'.format(k,L))\n",
    "            else:\n",
    "                axes[1,1].plot(ns_arr[nan_mask], bcs[nan_mask], marker='+', ls='-', label='k={:d} L={:d}'.format(k,L))\n",
    "\n",
    "\n",
    "    for p in [False, True]:\n",
    "        nL = []\n",
    "        bcs = []\n",
    "        bcs_err = []\n",
    "        for L in Ls:\n",
    "            nL = []\n",
    "            bcs = []\n",
    "            bcs_err = []\n",
    "            for n in ns:\n",
    "                key = get_key(L, n, p, k)\n",
    "                Wc, We, b, be = fit_result[key]\n",
    "                # nL.append(n / L)\n",
    "                # nL.append(1 / (n / L))\n",
    "                # nL.append((n / (L - n)))\n",
    "                nL.append(n * np.log(n) + (L-n) * np.log(L-n))\n",
    "                bcs.append(b)\n",
    "                bcs_err.append(be)\n",
    "            # Errorbars are not shown because they are less than a pixel.\n",
    "            nL  = np.array(nL)\n",
    "            bcs = np.array(bcs)\n",
    "            nan_mask = np.isfinite(bcs)\n",
    "            if p:\n",
    "                axes[2,0].plot(nL[nan_mask], bcs[nan_mask], marker='x', ls='-', label='k={:d} L={:d}'.format(k,L))\n",
    "            else:\n",
    "                axes[2,1].plot(nL[nan_mask], bcs[nan_mask], marker='+', ls='-', label='k={:d} L={:d}'.format(k,L))\n",
    "        # if p:\n",
    "        #     axes[2,0].plot(nL, bcs, marker='x', ls='' )\n",
    "        # else:\n",
    "        #     axes[2,1].plot(nL, bcs, marker='+', ls='')\n",
    "\n",
    "\n",
    "\n",
    "    # ================================================================================\n",
    "    # Then plot `k = 5` data from Dataset 2, at half opacity.\n",
    "    # ================================================================================\n",
    "\n",
    "    for row, axe in enumerate(axes):\n",
    "        for col, ax in enumerate(axe):\n",
    "            ax.set_prop_cycle(None)\n",
    "\n",
    "    k = 5\n",
    "\n",
    "    for p in [False, True]:\n",
    "        for n in ns:\n",
    "            bcs = []\n",
    "            bcs_err = []\n",
    "            for L in Ls:\n",
    "                key = get_key(L, n, p, k)\n",
    "                Wc, We, b, be = fit_result[key]\n",
    "                bcs.append(b)\n",
    "                bcs_err.append(be)\n",
    "            # Errorbars are not shown because they are less than a pixel.\n",
    "            bcs = np.array(bcs)\n",
    "            nan_mask = np.isfinite(bcs)\n",
    "            if p:\n",
    "                axes[0,0].plot(Ls_arr[nan_mask], bcs[nan_mask], marker='x', ls='--', alpha=0.5, label='k={:d} n={:d}'.format(k,n))\n",
    "            else:\n",
    "                axes[0,1].plot(Ls_arr[nan_mask], bcs[nan_mask], marker='+', ls='--', alpha=0.5, label='k={:d} n={:d}'.format(k,n))\n",
    "\n",
    "\n",
    "    for p in [False, True]:\n",
    "        for L in Ls:\n",
    "            bcs = []\n",
    "            bcs_err = []\n",
    "            for n in ns:\n",
    "                key = get_key(L, n, p, k)\n",
    "                Wc, We, b, be = fit_result[key]\n",
    "                bcs.append(b)\n",
    "                bcs_err.append(be)\n",
    "            # Errorbars are not shown because they are less than a pixel.\n",
    "            bcs = np.array(bcs)\n",
    "            nan_mask = np.isfinite(bcs)\n",
    "            if p:\n",
    "                axes[1,0].plot(ns_arr[nan_mask], bcs[nan_mask], marker='x', ls='--', alpha=0.5, label='k={:d} L={:d}'.format(k,L))\n",
    "            else:\n",
    "                axes[1,1].plot(ns_arr[nan_mask], bcs[nan_mask], marker='+', ls='--', alpha=0.5, label='k={:d} L={:d}'.format(k,L))\n",
    "\n",
    "\n",
    "    for p in [False, True]:\n",
    "        nL = []\n",
    "        bcs = []\n",
    "        bcs_err = []\n",
    "        for L in Ls:\n",
    "            nL = []\n",
    "            bcs = []\n",
    "            bcs_err = []\n",
    "            for n in ns:\n",
    "                key = get_key(L, n, p, k)\n",
    "                Wc, We, b, be = fit_result[key]\n",
    "                # nL.append(n / L)\n",
    "                # nL.append(1 / (n / L))\n",
    "                # nL.append((n / (L - n)))\n",
    "                nL.append(n * np.log(n) + (L-n) * np.log(L-n))\n",
    "                bcs.append(b)\n",
    "                bcs_err.append(be)\n",
    "            # Errorbars are not shown because they are less than a pixel.\n",
    "            nL  = np.array(nL)\n",
    "            bcs = np.array(bcs)\n",
    "            nan_mask = np.isfinite(bcs)\n",
    "            if p:\n",
    "                axes[2,0].plot(nL[nan_mask], bcs[nan_mask], marker='x', ls='--', alpha=0.5, label='k={:d} L={:d}'.format(k,L))\n",
    "            else:\n",
    "                axes[2,1].plot(nL[nan_mask], bcs[nan_mask], marker='+', ls='--', alpha=0.5, label='k={:d} L={:d}'.format(k,L))\n",
    "        # if p:\n",
    "        #     axes[2,0].plot(nL, bcs, marker='x', ls='' , alpha=0.5)\n",
    "        # else:\n",
    "        #     axes[2,1].plot(nL, bcs, marker='+', ls='', alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "    # ================================================================================\n",
    "    # Configure axes settings.\n",
    "    # ================================================================================\n",
    "\n",
    "    for row, axe in enumerate(axes):\n",
    "        for col, ax in enumerate(axe):\n",
    "            if row == 0:\n",
    "                if all_data:\n",
    "                    ax.legend(loc='lower left', ncol=2, framealpha=0.5)\n",
    "                else:\n",
    "                    ax.legend(loc='lower right', ncol=2, framealpha=0.5)#, bbox_to_anchor=(1, 0.05))\n",
    "\n",
    "                # Set y scale to exponential\n",
    "                # exp = lambda x: np.power(10, x)\n",
    "                # log = lambda x: np.log(x)\n",
    "                # ax.set_yscale('function', functions=(exp, log))\n",
    "                # ax.set_yscale('log')\n",
    "            if row == 1:\n",
    "                ax.legend(loc='lower right', ncol=2, framealpha=0.5)\n",
    "                # ax.set_xscale('log')\n",
    "            if row == 2:\n",
    "                ax.legend(loc='lower left', ncol=2, framealpha=0.5)\n",
    "\n",
    "            ax.tick_params(direction=\"in\")\n",
    "            ax.set_ylabel('Transition steepness $b$')\n",
    "            ax.xaxis.get_major_locator().set_params(integer=True)\n",
    "            # ax.set_ylim(0)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    # fig.subplots_adjust(top=0.88)\n",
    "    fig.subplots_adjust(top=0.94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_scaling_W(fit_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_scaling_b(fit_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_scaling_W(fit_result, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_scaling_b(fit_result, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Direct Hamiltonian\n",
    "$L=10, periodic$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_sample = 10000 # Samples per training W.\n",
    "rand_sample = 50\n",
    "Ws_train = np.random.randint(0,     2, size=(2 * base_sample,))\n",
    "Ws_train = (Ws_train * 7.5) + 0.5 # i.e. Ws are 0.5 and 8.0.\n",
    "Ws_valid = np.random.uniform(0.1, 5.9, size=(2 * base_sample // rand_sample,))\n",
    "Ws_valid = (Ws_valid.reshape(-1, 1) * np.ones((1,50))).flatten()\n",
    "print(Ws_train.shape)\n",
    "print(Ws_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MBL = {\n",
    "    \"obj_name\": 'H',\n",
    "    \"L\": 6,\n",
    "    \"periodic\": True,\n",
    "    \"Ws_train\": Ws_train,\n",
    "    \"Ws_valid\": Ws_valid,\n",
    "}\n",
    "obj_name = MBL['obj_name']\n",
    "L        = MBL['L']\n",
    "periodic = MBL['periodic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from MBL_H_dataset import MBLHDataset\n",
    "\n",
    "train_H_dataset = MBLHDataset(\n",
    "    MBL_params=MBL,\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),\n",
    "    # transform=transforms.Compose([\n",
    "    #     transforms.ToPILImage(),\n",
    "    #     transforms.ToTensor()\n",
    "    # ]),\n",
    ")\n",
    "valid_H_dataset = MBLHDataset(\n",
    "    MBL_params=MBL,\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor(),\n",
    "    # transform=transforms.Compose([\n",
    "    #     transforms.ToPILImage(),\n",
    "    #     transforms.ToTensor()\n",
    "    # ]),\n",
    ")\n",
    "\n",
    "print('Number of training samples  :', len(train_H_dataset))\n",
    "print('Number of validation samples:', len(valid_H_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def visualize_H(dataset):\n",
    "    \n",
    "    num = 4\n",
    "\n",
    "    sample_idx = np.random.randint(0, len(dataset), size=num*num)\n",
    "\n",
    "    # Square image.\n",
    "    fig, axes = plt.subplots(num, num, figsize=(fig_w/dpi,fig_w/dpi), dpi=dpi, squeeze=False)\n",
    "\n",
    "    samples_ext = []\n",
    "    samples_loc = []\n",
    "    for i in range(len(dataset)):\n",
    "        data = dataset[i]\n",
    "        if data['W'] == 0.5 and len(samples_ext) < num*num//2:\n",
    "            samples_ext.append((data['image'][0,:,:].numpy()))\n",
    "        if data['W'] == 8.0 and len(samples_loc) < num*num//2:\n",
    "            samples_loc.append((data['image'][0,:,:].numpy()))\n",
    "        if len(samples_ext) >= num*num//2 and len(samples_loc) >= num*num//2:\n",
    "            break\n",
    "\n",
    "    i_ext = 0\n",
    "    i_loc = 0\n",
    "    for i in range(num * num):\n",
    "        if i // num < 2:\n",
    "            # np.fill_diagonal(samples_ext[i_ext], 0)\n",
    "            axes[i%num,i//num].imshow(samples_ext[i_ext])\n",
    "            axes[i%num,i//num].annotate('W={:3.1f}'.format(0.5), (0.25,0.1), xycoords='axes fraction', ha='center', color='w')\n",
    "            i_ext += 1\n",
    "        else:\n",
    "            # np.fill_diagonal(samples_loc[i_loc], 0)\n",
    "            axes[i%num,i//num].imshow(samples_loc[i_loc])\n",
    "            axes[i%num,i//num].annotate('W={:3.1f}'.format(8.0), (0.25,0.1), xycoords='axes fraction', ha='center', color='w')\n",
    "            i_loc += 1\n",
    "\n",
    "    for axe in axes:\n",
    "        for ax in axe:\n",
    "            # ax.legend(loc='best')\n",
    "            ax.xaxis.set_ticklabels([])\n",
    "            ax.yaxis.set_ticklabels([])\n",
    "            ax.xaxis.set_visible(False)\n",
    "            ax.yaxis.set_visible(False)\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Visualize Hamiltonian training data:')\n",
    "visualize_H(train_H_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from MBL_H_model import MBLHModel\n",
    "# model_version = 1\n",
    "# model_H = load_H_model('model_v{}.pkl.gz'.format(model_version), L=10, periodic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_H_crossing(Ws, Ps, Ws_uniq, Ps_mean, Ps_std, L, periodic):\n",
    "\n",
    "    labels = ['Extended (Low W)', 'Localized (High W)']\n",
    "\n",
    "    # Plot probability P(Localized) against W.\n",
    "    # 4:3 aspect ratio.\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(fig_w/dpi, fig_h/dpi), dpi=dpi, squeeze=False)\n",
    "\n",
    "    fig.suptitle('Probability vs Disorder Strength ($L={}, {}$)'.format(L, 'periodic' if periodic else 'non-periodic'))\n",
    "\n",
    "    axes[0,0].axhline(0.5, c='lightgrey', ls='--', label='$P=0.5$')\n",
    "\n",
    "    # Plot averaged values with error bars.\n",
    "    # markers, caps, bars = axes[0,0].errorbar(Ws_uniq, Ps_mean, Ps_std, ls=' ', marker='x', capsize=2, capthick=2, label='P(Localized) Mean')\n",
    "    # Loop through bars and caps and set the alpha value\n",
    "    # [bar.set_alpha(0.5) for bar in bars]\n",
    "    # [cap.set_alpha(0.5) for cap in caps]\n",
    "    \n",
    "    # Actually, don't plot error bars. That's distracting.\n",
    "    axes[0,0].plot(Ws_uniq, Ps_mean, ls=' ', marker='x', c='#0065bd', label='$E_W[P(Localized)]$')\n",
    "\n",
    "    # Plot raw data.\n",
    "    axes[0,0].plot(Ws, Ps,   ls=' ', marker='.', c='#98c6ea', label='$P(Localized)$', alpha=0.05)\n",
    "    axes[0,0].set_xlabel('Disorder strength $W$')\n",
    "    axes[0,0].set_ylabel('Probability $P(Localized)$')\n",
    "\n",
    "    # Curve fit a sigmoid using all data.\n",
    "    # Fitting only the mean with `Ws_uniq` and `Ps_mean` gives identical results.\n",
    "    # popt, pcov = curve_fit(sigmoid, Ws, Ps, p0=[3, 0, 2]) # Add bounds or initial values if it doesn't converge.\n",
    "    popt, pcov = curve_fit(sigmoid, Ws, Ps, p0=[3, 2]) # Add bounds or initial values if it doesn't converge.\n",
    "    # x0, y0, b = popt\n",
    "    x0, b = popt\n",
    "    x = np.linspace(0, 10, 100)\n",
    "    y = sigmoid(x, *popt)\n",
    "    axes[0,0].plot(x, y, ls='--', lw=2, c='#e37222', label='Fited sigmoid')\n",
    "    axes[0,0].set_title('$\\sigma(x) = 1 / (1 + Exp(-{:.4f} (x - {:.4f})))$'.format(b, x0), fontsize=10)\n",
    "    # print('Fitted sigmoid function 1 / (1 + Exp(-{:.4f} (x - {:.4f}))) + {:.4f}'.format(b, x0, y0))\n",
    "    print('Fitted sigmoid function 1 / (1 + Exp(-{:.4f} (x - {:.4f})))'.format(b, x0))\n",
    "\n",
    "    W_c = logit(0.5, *popt)\n",
    "    perr = np.sqrt(np.diag(pcov))\n",
    "    print('Transition W_C is found to be at W = {:.4f} ± {:.4f}'.format(W_c, perr[0]))\n",
    "    axes[0,0].axvline(W_c, c='r',         ls='--', label='Fitted $W_c$')\n",
    "\n",
    "    for axe in axes:\n",
    "        for ax in axe:\n",
    "            ax.tick_params(direction=\"in\")\n",
    "            ax.legend(loc='best')\n",
    "            ax.set_xlim(0, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Ws, Ps, Ws_uniq, Ps_mean, Ps_std = load_H_eval_valid(  model_version, 10, periodic)\n",
    "# Ws, Ps, Ws_uniq, Ps_mean, Ps_std = load_H_eval_valid(model_version, L, periodic)\n",
    "plot_H_crossing(Ws, Ps[:,1], Ws_uniq, Ps_mean, Ps_std, 10, periodic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# del model_H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Testing tensordot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = np.random.rand((200))\n",
    "b = np.random.rand((200))\n",
    "u = a + 1j * b\n",
    "a = np.random.rand((200))\n",
    "b = np.random.rand((200))\n",
    "v = a + 1j * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "u1 = np.tensordot(u, u, axes=[[0],[0]])\n",
    "u2 = np.tensordot(u.conj(), u, axes=[[0],[0]])\n",
    "u3 = np.tensordot(u, u.conj(), axes=[[0],[0]])\n",
    "print(u1)\n",
    "print(u2)\n",
    "print(u3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "uv1 = np.outer(u, v)\n",
    "uv2 = np.outer(u.conj(), v)\n",
    "uv3 = np.outer(u, v.conj())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.allclose(uv2.conj(), uv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit",
   "language": "python",
   "name": "python37264bit22d8f94fb4124d8cb7bc86dc616da5cb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
