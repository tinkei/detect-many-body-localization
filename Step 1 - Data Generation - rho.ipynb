{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this variable yourself.\n",
    "running_on_colab = False\n",
    "# Store data as reduced density matrix `rho` or eigenvector tuple `EVW`.\n",
    "rho_or_EVW = 'rho'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning of Many Body Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Use exact diagonalization to obtain a few eigenstates near energy $E = 0$ from the Heisenberg model with a\n",
    "random field, \n",
    "\n",
    "\\begin{equation}\n",
    "    H = J \\sum_i \\vec{S}_{i} \\cdot \\vec{S}_{i+1} - \\sum_i h_i S^z_i\n",
    "\\end{equation}\n",
    "\n",
    ", where the values of the field $ h_i \\in [-W, W] $ are chosen from a uniform random distribution with a \"disorder strength\" $W$ (with moderate system sizes $L \\approx 12$). \n",
    "\n",
    "The exciting property of this model is that it is believed to undergo a phase transition from an extended phase (small $W$) to a localized phase (large $W$). \n",
    "\n",
    "We will use ML to detect this transition: Pick a number of eigenstates that are near energy $E = 0$ and obtain the reduced density matrices $\\rho^A$, where $A$ is a region of $n$ consecutive spins (a few hundred to thousands eigenstates for different disorder realizations). \n",
    "\n",
    "Now use the density matrices for $W = 0.5 J$ and $W = 8.0 J$ to train a neural network (just interpret the entries of $\\rho^A$ as an image with $2^n \\times 2^n$ pixel). \n",
    "Then use this network and study the output of the neural network for different $W$. \n",
    "\n",
    "How does the results depend on system size $L$ and block size $n$? \n",
    "At which $W_c$ do you expect the transition to occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Wikipedia\n",
    "[Many body localization](https://en.wikipedia.org/wiki/Many_body_localization)  \n",
    "[Localization protected quantum order](https://en.wikipedia.org/wiki/Localization_protected_quantum_order)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Many-body localization (MBL) is a dynamical phenomenon which leads to the breakdown of equilibrium statistical mechanics in isolated many-body systems. Such systems never reach local thermal equilibrium, and retain local memory of their initial conditions for infinite times.\n",
    "\n",
    "MBL was first proposed by P.W. Anderson in 1958 as a possibility that could arise in strongly disordered quantum systems. The basic idea was that if particles all live in a random energy landscape, then any rearrangement of particles would change the energy of the system. Since energy is a conserved quantity in quantum mechanics, such a process can only be virtual and cannot lead to any transport of particle number or energy.  \n",
    "\n",
    "The process of thermalization erases local memory of the initial conditions. In textbooks, thermalization is ensured by coupling the system to an external environment or \"reservoir,\" with which the system can exchange energy. What happens if the system is isolated from the environment, and evolves according to its own Schrödinger equation? Does the system still thermalize?\n",
    "\n",
    "Quantum mechanical time evolution is unitary and formally preserves all information about the initial condition in the quantum state at all times.\n",
    "\n",
    "This question can be formalized by considering the quantum mechanical density matrix ρ of the system. If the system is divided into a subregion A (the region being probed) and its complement B (everything else), then all information that can be extracted by measurements made on A alone is encoded in the reduced density matrix $\\rho_A = Tr_B (\\rho(t))$. If in the long time limit $\\rho_A(t)$ approaches a thermal density matrix at a temperature set by the energy density in the state, then the system has \"thermalized,\" and no local information about the initial condition can be extracted from local measurements. This process of \"quantum thermalization\" may be understood in terms of B acting as a reservoir for A. In this perspective, the entanglement entropy $ S = - Tr \\rho_A log \\rho_A $ of a thermalizing system in a pure state plays the role of thermal entropy. Thermalizing systems therefore generically have extensive or \"volume law\" entanglement entropy at any non-zero temperature.\n",
    "\n",
    "In contrast, if $\\rho_A(t)$ fails to approach a thermal density matrix even in the long time limit, and remains instead close to its initial condition $\\rho_A(0)$, then the system retains forever a memory of its initial condition in local observables. This latter possibility is referred to as \"many body localization,\" and involves B failing to act as a reservoir for A. Eigenstates of systems exhibiting MBL do not obey the ETH, and generically follow an \"area law\" for entanglement entropy (i.e. the entanglement entropy scales with the surface area of subregion A).\n",
    "\n",
    "In thermalizing systems, energy eigenstates have volume law entanglement entropy. In MBL systems, energy eigenstates have area law entanglement entropy.\n",
    "\n",
    "In thermalizing systems, entanglement entropy grows as a power law in time starting from low entanglement initial conditions. In MBL systems, entanglement entropy grows logarithmically in time starting from low entanglement initial conditions.\n",
    "\n",
    "In thermalizing systems, the dynamics of out-of-time-ordered correlators forms a linear light cone which reflects the ballistic propagation of information. In MBL systems, the light cone is logarithmic.\n",
    "\n",
    "What's more, while individual eigenstates aren't themselves experimentally accessible, order in eigenstates nevertheless has measurable dynamical signatures. The eigenspectrum properties change in a singular fashion as the system transitions between from one type of MBL phase to another, or from an MBL phase to a thermal one---again with measurable dynamical signatures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['running_on_colab'] = str(running_on_colab)\n",
    "# running_on_colab = (os.getenv('running_on_colab', 'False') == 'True')\n",
    "\n",
    "if running_on_colab:\n",
    "    data_root             = 'drive/MyDrive/Colab Data/MBL/'\n",
    "    sys.path.append(data_root)\n",
    "else:\n",
    "    data_root             = './'\n",
    "\n",
    "# Store data as reduced density matrix `rho` or eigenvector tuple `EVW`.\n",
    "os.environ['rho_or_EVW'] = str(rho_or_EVW)\n",
    "# running_on_colab = (os.getenv('rho_or_EVW', 'EVW') == 'rho')\n",
    "\n",
    "from file_io import *\n",
    "from data_gen import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "dpi = 100\n",
    "fig_w = 1280\n",
    "fig_h = 640\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if running_on_colab:\n",
    "    !cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if running_on_colab:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if running_on_colab:\n",
    "    !pip install pytorch_lightning==0.7.6 torchsummary==1.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Test execution time of Exact Diagonalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Test solving a single Hamiltonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "L = 10\n",
    "W = 0.5\n",
    "J = 1\n",
    "periodic = False\n",
    "num_Hs = 10\n",
    "\n",
    "H = build_H(L, W, J, periodic)\n",
    "E, V = ED(H.toarray())\n",
    "print('2^L: {}'.format(2**L))\n",
    "print('#eigenvalues: {}'.format(len(E)))\n",
    "print('E.shape: {}'.format(E.shape))\n",
    "print('V.shape: {}'.format(V.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Solve using numpy's dense solver\n",
    "E, V = ED(H.toarray())\n",
    "E0, V0 = select_N_eigenvalues(E, V, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Solve using scipy's sparse solver instead.\n",
    "E1, V1 = ED_sparse(H, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Solve using numpy's dense solver\n",
    "E, V = ED(H.toarray())\n",
    "E0, V0 = select_N_eigenvalues(E, V, 20)\n",
    "\n",
    "# Solve using scipy's sparse solver instead.\n",
    "E1, V1 = ED_sparse(H, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check eigenvalues are sorted correctly, and eigenvectors are selected along the correct axis.\n",
    "V0_norm = []\n",
    "V1_norm = []\n",
    "for i in range(len(E0)):\n",
    "    V0_norm.append(np.linalg.norm(V0[:,i]))\n",
    "    V1_norm.append(np.linalg.norm(V1[:,i]))\n",
    "\n",
    "for E0i, E1i, V0i, V1i in zip(E0, E1, V0_norm, V1_norm):\n",
    "    print('Numpy: {:+.12f} | Numpy norm: {:.16f}'.format(E0i, V0i))\n",
    "    print('Scipy: {:+.12f} | Scipy norm: {:.16f}'.format(E1i, V1i))\n",
    "\n",
    "assert np.allclose(V0i, V1i), 'Eigenvectors evaluated using Numpy and Scipy should be identical.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Test solving multiple Hamiltonians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Hs = build_Hs(L, W, J, periodic, num_Hs)\n",
    "Es, Vs = EDs(Hs)\n",
    "E0s, V0s = EDs_sparse(Hs, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Test execution time, numpy vs scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test execution time: numpy.\n",
    "J  = 1                      # Always = 1\n",
    "Ws = [8]                    # Disorder strength W.\n",
    "Ls = list(range(8,12))      # System size L.\n",
    "ns = [1]*len(Ls)            # Number of samples for each L.\n",
    "ps = [False]                # Periodic or not.\n",
    "et = []                     # Execution time.\n",
    "num_EV = 20                 # Number of eigenvalues near zero to save.\n",
    "\n",
    "for L, num_Hs in zip(Ls, ns):\n",
    "    for W in Ws:\n",
    "        for p in ps:\n",
    "            start_time = time.time()\n",
    "            batch_generate_ED_data(L, W, J, p, num_Hs, num_EV, save_data=False, npsp='np')\n",
    "            exec_time = time.time() - start_time\n",
    "            et.append(exec_time)\n",
    "            print('Computed: L={:02d} | W={:.2f} | periodic={: <5}. Execution took {: 8.2f}s or {: 6.2f}min'.format(L, W, str(p), exec_time, exec_time/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(fig_w/dpi,fig_h/dpi), dpi=dpi, squeeze=False)\n",
    "\n",
    "base = 10\n",
    "axes[0,0].plot(Ls, et)\n",
    "axes[0,1].plot(np.array(Ls), np.log(et) / np.log(base))\n",
    "\n",
    "axes[0,0].set_title('Numpy Execution time vs System size')\n",
    "axes[0,1].set_title('Log(Numpy Execution time) vs System size')\n",
    "axes[0,0].set_ylabel('Execution time')\n",
    "axes[0,1].set_ylabel('Log(Execution time)')\n",
    "\n",
    "for axe in axes:\n",
    "    for ax in axe:\n",
    "        ax.set_xlabel('System size L')\n",
    "        # ax.legend(loc='best')\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test execution time: scipy.\n",
    "J  = 1                      # Always = 1\n",
    "Ws = [8]                    # Disorder strength W.\n",
    "Ls = list(range(8,16))      # System size L.\n",
    "ns = [1]*len(Ls)            # Number of samples for each L.\n",
    "ps = [False]                # Periodic or not.\n",
    "et = []                     # Execution time.\n",
    "num_EV = 20                 # Number of eigenvalues near zero to save.\n",
    "\n",
    "for L, num_Hs in zip(Ls, ns):\n",
    "    for W in Ws:\n",
    "        for p in ps:\n",
    "            start_time = time.time()\n",
    "            batch_generate_ED_data(L, W, J, p, num_Hs, num_EV, save_data=False, npsp='sp')\n",
    "            exec_time = time.time() - start_time\n",
    "            et.append(exec_time)\n",
    "            print('Computed: L={:02d} | W={:.2f} | periodic={: <5}. Execution took {: 8.2f}s or {: 6.2f}min'.format(L, W, str(p), exec_time, exec_time/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(fig_w/dpi,fig_h/dpi), dpi=dpi, squeeze=False)\n",
    "\n",
    "base = 10\n",
    "axes[0,0].plot(Ls, et)\n",
    "axes[0,1].plot(np.array(Ls), np.log(et) / np.log(base))\n",
    "\n",
    "axes[0,0].set_title('SciPy Execution time vs System size')\n",
    "axes[0,1].set_title('Log(SciPy Execution time) vs System size')\n",
    "axes[0,0].set_ylabel('Execution time')\n",
    "axes[0,1].set_ylabel('Log(Execution time)')\n",
    "\n",
    "for axe in axes:\n",
    "    for ax in axe:\n",
    "        ax.set_xlabel('System size L')\n",
    "        # ax.legend(loc='best')\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Sample parameters.\n",
    "J  = 1                                 # Always = 1\n",
    "Ws = [0.5, 8] * 10 + [i/2 for i in range(1*2, 10*2)] # Disorder strength W.\n",
    "Ls = [   8,   9,  10,  11, 12, 13, 14] # System size L.\n",
    "ns = [1000, 500, 250, 100, 50, 20, 10] # Number of samples for each L.\n",
    "ps = [False, True]                     # Periodic or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test file size. It's not feasible to store ALL eigenvectors.\n",
    "J  = 1             # Always = 1\n",
    "Ws = [0.5, 8] * 10 + [i/2 for i in range(1*2, 10*2)] # Disorder strength W.\n",
    "Ls = [   8]        # System size L.\n",
    "ns = [1000]        # Number of samples for each L.\n",
    "ps = [False, True] # Periodic or not.\n",
    "\n",
    "fs = 0\n",
    "for L, num_Hs in zip(Ls, ns):\n",
    "    for W in Ws:\n",
    "        for p in ps:\n",
    "            start_time = time.time()\n",
    "            # batch_generate_ED_data(L, W, J, p, num_Hs)\n",
    "            fs += 1\n",
    "            exec_time = time.time() - start_time\n",
    "            # print('Computed: L={:02d} | W={:.2f} | periodic={: <5}. Execution took {: 8.2f}s or {: 6.2f}min'.format(L, W, str(p), exec_time, exec_time/60))\n",
    "print('Eigenvectors `Vs` dominates the file size. Assume 1000 samples are generated for each W, for around 20 Ws:')\n",
    "print('For L=8, each Es file is 2MB, Hs is 23MB, Vs is 1000MB.')\n",
    "print('Estimate file size of each run is thus {}MB'.format(fs*(2+23+1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('The full eigenvectors `Vs` dominates the file size.')\n",
    "for i in range(5):\n",
    "    print('For L = {:2d}, Vs is {:3d} MB.'.format(8+i, 4**i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Partial trace and reduced density matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Irrelevant stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "According to my limited understanding, a so-called partial trace is simply summing contributions of coefficients outside a chosen subsystem. \n",
    "\n",
    "Let's start with a simple example of two spins, divided into two subsystems $A$ and $B$, each with one spin:\n",
    "\n",
    "\\begin{equation}\n",
    "    | s_1 \\rangle \\otimes | s_2 \\rangle = | s_1 \\rangle_A \\otimes | s_2 \\rangle_B = | A \\rangle \\otimes | B \\rangle\n",
    "\\end{equation}\n",
    "\n",
    "An eigenvector has the following basis:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{pmatrix}\n",
    "        | (\\downarrow)_A (\\downarrow)_B \\rangle \\\\\n",
    "        | (\\downarrow)_A (\\uparrow)_B   \\rangle \\\\\n",
    "        | (\\uparrow)_A   (\\downarrow)_B \\rangle \\\\\n",
    "        | (\\uparrow)_A   (\\uparrow)_B   \\rangle \\\\\n",
    "    \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Now add coefficients $C_i$ of the eigenvector:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{pmatrix}\n",
    "        C_0 | (\\downarrow)_A (\\downarrow)_B \\rangle \\\\\n",
    "        C_1 | (\\downarrow)_A (\\uparrow)_B   \\rangle \\\\\n",
    "        C_2 | (\\uparrow)_A   (\\downarrow)_B \\rangle \\\\\n",
    "        C_3 | (\\uparrow)_A   (\\uparrow)_B   \\rangle \\\\\n",
    "    \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Or $C_{i,j}$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{pmatrix}\n",
    "        C_{0,0} | (\\downarrow)_A (\\downarrow)_B \\rangle \\\\\n",
    "        C_{0,1} | (\\downarrow)_A (\\uparrow)_B   \\rangle \\\\\n",
    "        C_{1,0} | (\\uparrow)_A   (\\downarrow)_B \\rangle \\\\\n",
    "        C_{1,1} | (\\uparrow)_A   (\\uparrow)_B   \\rangle \\\\\n",
    "    \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "The reduce density matrix of subsystem $A$ is ${\\rho}_A$ is defined as a partial trace over $B$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\rho_A = Tr_B(\\rho)\n",
    "\\end{equation}\n",
    "\n",
    "That can be obtained simply by summing coefficients related to $B$, thereby removing its contribution:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}[c]\n",
    "        \\begin{pmatrix}\n",
    "            (C_0 + C_1) | (\\downarrow)_A \\rangle \\\\\n",
    "            (C_2 + C_3) | (\\uparrow)_A   \\rangle \\\\\n",
    "        \\end{pmatrix}\n",
    "    \\end{aligned}\n",
    "    \\quad or \\quad\n",
    "    \\begin{aligned}[c]\n",
    "        \\begin{pmatrix}\n",
    "            (C_{0,0} + C_{0,1}) | (\\downarrow)_A \\rangle \\\\\n",
    "            (C_{1,0} + C_{1,1}) | (\\uparrow)_A   \\rangle \\\\\n",
    "        \\end{pmatrix}\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "We observe that if we have an eigenvector in an array, the notation $C_i$ used on the left hand side is the array index, while the right hand side is the binary representation of said index. Does this relation extend beyond two spins? (Probably not, unless each spin is its own subsystem...)\n",
    "\n",
    "But this is clearly wrong, because this is just a vector, not a matrix. Recall from Chapter 5 of our lecture notes that the definition of a reduced density matrix is also:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\rho_A = Tr_B(\\rho) = Tr_B(| \\psi \\rangle \\langle \\psi |) = \\sum_{i',j,i,j} \\psi^\\ast_{i',j} \\psi_{i,j} | i' \\rangle \\langle i | ,\n",
    "\\end{equation}\n",
    "\n",
    "where $i$ are spins/sites related to subsystem $A$, and $j$ to $B$. An element of this matrix is simply:\n",
    "\n",
    "\\begin{equation}\n",
    "    {(\\rho_A)}_{i',i} = Tr_B(\\rho)_{i',i} = \\sum_{j} \\psi^\\ast_{i',j} \\psi_{i,j} | i' \\rangle \\langle i |\n",
    "\\end{equation}\n",
    "\n",
    "The basis $ | i' \\rangle \\langle i | $ is always implicit in all calculations and never really affect anything, ever, really. The only purpose is to make everything more complicated and confusing.\n",
    "\n",
    "Reverting back to the notation of our previous 2-spin example, with $\\psi_{i,j} \\to C_i$ and $| i \\rangle \\to | s_1 \\rangle_A$, a reduced density matrix can be constructed as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{pmatrix}\n",
    "        (C_0 + C_1)^\\ast \\times (C_0 + C_1) &                     ?               \\\\\n",
    "                            ?               & (C_2 + C_3)^\\ast \\times (C_2 + C_3) \\\\\n",
    "    \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Now what should the off-diagonal terms look like? We have two choices:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}[c]\n",
    "        \\begin{pmatrix}\n",
    "            (C_0 + C_1)^\\ast \\times (C_0 + C_1) & (C_2 + C_3)^\\ast \\times (C_0 + C_1) \\\\\n",
    "            (C_0 + C_1)^\\ast \\times (C_2 + C_3) & (C_2 + C_3)^\\ast \\times (C_2 + C_3) \\\\\n",
    "        \\end{pmatrix}\n",
    "    \\end{aligned}\n",
    "    \\quad or \\quad\n",
    "    \\begin{aligned}[c]\n",
    "        \\begin{pmatrix}\n",
    "            (C_0 + C_1)^\\ast \\times (C_0 + C_1) & (C_0 + C_1)^\\ast \\times (C_2 + C_3) \\\\\n",
    "            (C_2 + C_3)^\\ast \\times (C_0 + C_1) & (C_2 + C_3)^\\ast \\times (C_2 + C_3) \\\\\n",
    "        \\end{pmatrix}\n",
    "    \\end{aligned}\n",
    "    \\quad ...? \n",
    "\\end{equation}\n",
    "\n",
    "We are using a neural network and treating this as an image, so it would not matter if we flip the off-diagonal terms. The latter appears to be correct though, because:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{pmatrix}\n",
    "        a^\\ast_0 \\\\\n",
    "        a^\\ast_1\n",
    "    \\end{pmatrix}\n",
    "    \\otimes\n",
    "    \\begin{pmatrix}\n",
    "        b_0 & b_1\n",
    "    \\end{pmatrix}\n",
    "    \\; = \\;\n",
    "    \\begin{pmatrix}\n",
    "        a^\\ast_0 b_0 & a^\\ast_0 b_1 \\\\\n",
    "        a^\\ast_1 b_0 & a^\\ast_1 b_1\n",
    "    \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Finally, how to efficiently identify which coefficients such as $(C_0 + C_1)$ to sum, in a vectorized manner? All this nonsense is probably a single line in programming code.\n",
    "\n",
    "Some references:\n",
    "* https://arxiv.org/pdf/1601.07458.pdf\n",
    "* http://www.quantum.umb.edu/Jacobs/QMT/QMT_AppendixA.pdf\n",
    "* https://physics.stackexchange.com/questions/179671/how-to-take-partial-trace\n",
    "\n",
    "To construct a reduced density matrix by taking a partial trace of a full density matrix, the dimensions of the matrix goes from $2^L \\times 2^L$ to $2^n \\times 2^n$. This requires a **RECTANGULAR** matrix. However, through the million definitions and tutorials and stackexchange articles I've read, none, **NONE**, explicitly stated how to construct it for the general case. They either kept it in terms of abstract notations, or they have already simplified it to a specific case. **NONE** are usable.\n",
    "\n",
    "To add insult to injury, our system is a bit more complicated than just $ | A \\rangle \\otimes | B \\rangle $. But rather, because we want $n$ consecutive spins in the center, the system is actually $ | B_1 \\rangle \\otimes | A \\rangle \\otimes | B_2 \\rangle $!\n",
    "\n",
    "### My current understanding / implementation of the rectangular matrix is as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbb{B} = | j \\rangle = | 1 \\rangle_{B1} \\otimes \\mathbb{I}_{A} \\otimes | 1 \\rangle_{B2} =\n",
    "    \\begin{pmatrix}\n",
    "        1 \\\\\n",
    "        \\vdots \\\\\n",
    "        1\n",
    "    \\end{pmatrix}\n",
    "    \\otimes\n",
    "    \\begin{pmatrix}\n",
    "        1 & \\dots & 0 \\\\ \n",
    "        \\vdots & \\ddots  & \\vdots \\\\\n",
    "        0 & \\dots & 1\n",
    "    \\end{pmatrix}\n",
    "    \\otimes\n",
    "    \\begin{pmatrix}\n",
    "        1 \\\\\n",
    "        \\vdots \\\\\n",
    "        1\n",
    "    \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Then apply this matrix to the full density matrix:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbb{B}^\\dagger \\mathbb{\\rho} \\mathbb{B}\n",
    "\\end{equation}\n",
    "\n",
    "This assumes the sum over sites of $j$ in subsystem $B$ is whatever-ative, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "    Tr_B(\\rho) = \\sum_j (\\langle B_1 | \\otimes \\langle A | \\otimes \\langle B_2 |) \\; \\rho \\; (| B_1 \\rangle \\otimes | A \\rangle \\otimes | B_2 \\rangle) = (\\langle 1 |_{B1} \\otimes \\mathbb{I}_{A} \\otimes \\langle 1 |_{B2}) \\; \\rho \\; (| 1 \\rangle_{B1} \\otimes \\mathbb{I}_{A} \\otimes | 1 \\rangle_{B2})\n",
    "\\end{equation}\n",
    "\n",
    "# In the end, tensor contraction is used!\n",
    "\n",
    "See function `partial_trace_tensor()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Other resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Given any orthonormal basis sets $|a_i\\rangle$ and $|b_i\\rangle$ for $\\mathcal H_a$ and $\\mathcal H_b$ respectively, any operator $K$ on the space $\\mathcal H_a\\otimes \\mathcal H_b$ can be written:\n",
    "\n",
    "$$K = \\sum_{ij k\\ell} K_{ijk\\ell} |a_i\\rangle |b_j\\rangle \\langle a_k|\\langle b_\\ell|$$\n",
    "\n",
    "where $K_{ijk\\ell} \\equiv \\langle a_i|\\langle b_j| K |a_k\\rangle |b_\\ell\\rangle$, and $\\langle a_i|\\langle b_j| \\equiv \\langle a_i|\\otimes \\langle b_j|$ (I omit the $\\otimes$ for notational clarity).  The partial trace is then defined to be\n",
    "\n",
    "$$\\mathrm{Tr}_b(K):= \\sum_{ik\\ell}K_{i\\ell k\\ell}|a_i\\rangle\\langle a_k|$$\n",
    "\n",
    "which is now a linear operator on $\\mathcal H_a$ alone, with coefficients $$\\bigg(\\mathrm{Tr}_b(K)\\bigg)_{ik} = \\sum_\\ell K_{i\\ell k\\ell}$$\n",
    "\n",
    "Source: https://physics.stackexchange.com/questions/616061/where-does-the-expression-mathrmtrk-sum-j-1n-langle-psi-jk-psi-j/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Test how outer product works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a0 =  1 +  2j\n",
    "a1 =  3 +  5j\n",
    "b0 =  7 + 11j\n",
    "b1 = 13 + 17j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = np.array([1 +  2j,  3 +  5j])\n",
    "b = np.array([7 + 11j, 13 + 17j])\n",
    "np.outer(a.conj(), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(1 -  2j) * (7 + 11j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(1 -  2j) * (13 + 17j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Test bit shift (unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_E     = 10 # #eigenstates, ~100 - ~1000.\n",
    "num_sites = 6  # #n consecutive spins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Source: https://stackoverflow.com/questions/147713/how-do-i-manipulate-bits-in-python\n",
    "def get_bit(value, n):\n",
    "    return ((value >> n & 1) != 0)\n",
    "\n",
    "def set_bit(value, n):\n",
    "    return value | (1 << n)\n",
    "\n",
    "def clear_bit(value, n):\n",
    "    return value & ~(1 << n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(get_bit(32, 4))\n",
    "print(get_bit(32, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def drop_sites(L, sites):\n",
    "    \"\"\"Select sites to drop.\n",
    "    This is only for demonstration. The mechanics is probably wrong.\"\"\"\n",
    "\n",
    "    idx = []\n",
    "    # `i` is the index along an eigenvector.\n",
    "    # `site` is the site to be dropped / summed over while constructing a reduced density matrix.\n",
    "    for i in range(2**L):\n",
    "        # Spins are numbered from left to right.\n",
    "        # But bits are numbered from right to left.\n",
    "        # Hence we need to flip the binary representation.\n",
    "        for site in sites:\n",
    "            print('Check site {} for removal.'.format(site))\n",
    "            print('{:0{}b}'.format(i, L))\n",
    "            if site == 0:\n",
    "                print('^')\n",
    "            else:\n",
    "                print(' ' * (site) + '^')\n",
    "            if get_bit(i, L - site - 1) == 1:\n",
    "                idx.append(i)\n",
    "\n",
    "    return idx\n",
    "\n",
    "print(drop_sites(2, [0]))\n",
    "print('='*25)\n",
    "print(drop_sites(2, [1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Compare implementations of partial trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Tr_B = build_partial_trace_matrix(2, [0], [1], [])\n",
    "print(Tr_B.shape)\n",
    "print(Tr_B.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Hamiltonian sample.\n",
    "L  = 8\n",
    "W1 = 0.5\n",
    "W2 = 8\n",
    "J  = 1\n",
    "periodic = False\n",
    "num_Hs = 10\n",
    "\n",
    "H1 = build_H(L, W1, J, periodic=False)\n",
    "E1, V1 = ED(H1.toarray())\n",
    "\n",
    "print('2^L: {}'.format(2**L))\n",
    "print('#eigenvalues: {}'.format(len(E1)))\n",
    "print('E.shape: {}'.format(E1.shape))\n",
    "print('V.shape: {}'.format(V1.shape))\n",
    "\n",
    "H2 = build_H(L, W2, J, periodic=False)\n",
    "E2, V2 = ED(H2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rho = get_rho(V1[:,0])\n",
    "print(rho.shape)\n",
    "print(np.max(rho))\n",
    "max_idx = np.unravel_index(rho.argmax(), rho.shape) # 2D index of np.argmax().\n",
    "print(max_idx)\n",
    "\n",
    "pos = max_idx[0] # Which 3x3 sub-matrix along the diagonal to print.\n",
    "print(rho[pos-1:pos+2, pos-1:pos+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Compare matrix-generated rho_A vs tensor-generated rho_A.\n",
    "A_sites  = [1,2,3,4,5,6] # Keep n consecutive\n",
    "B1_sites = [0]\n",
    "B2_sites = [7]\n",
    "rho_A_mat1 = partial_trace_matrix(L, A_sites, B1_sites, B2_sites, V1[:,0])\n",
    "rho_A_mat2 = partial_trace_matrix(L, A_sites, B1_sites, B2_sites, V2[:,0])\n",
    "print('Theoretical size of reduced density matrix rho_A: {}'.format(2**len(A_sites)))\n",
    "print('Shape of computed rho_A: {}'.format(rho_A_mat1.shape))\n",
    "print('Size of computed rho_A: {}'.format(rho_A_mat1.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rho_A_ten1 = partial_trace_tensor(L, A_sites, B1_sites, B2_sites, V1[:,0])\n",
    "rho_A_ten2 = partial_trace_tensor(L, A_sites, B1_sites, B2_sites, V2[:,0])\n",
    "print('Theoretical size of reduced density matrix rho_A: {}'.format(2**len(A_sites)))\n",
    "print('Shape of computed rho_A: {}'.format(rho_A_ten1.shape))\n",
    "print('Size of computed rho_A: {}'.format(rho_A_ten1.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rho_A_kev1 = partial_trace_kevin(L, A_sites, B1_sites, B2_sites, V1[:,0])\n",
    "rho_A_kev2 = partial_trace_kevin(L, A_sites, B1_sites, B2_sites, V2[:,0])\n",
    "print('Theoretical size of reduced density matrix rho_A: {}'.format(2**len(A_sites)))\n",
    "print('Shape of computed rho_A: {}'.format(rho_A_kev1.shape))\n",
    "print('Size of computed rho_A: {}'.format(rho_A_kev1.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rho_A_jon1 = partial_trace_jonas(L, A_sites, B1_sites, B2_sites, V1[:,0])\n",
    "rho_A_jon2 = partial_trace_jonas(L, A_sites, B1_sites, B2_sites, V2[:,0])\n",
    "print('Theoretical size of reduced density matrix rho_A: {}'.format(2**len(A_sites)))\n",
    "print('Shape of computed rho_A: {}'.format(rho_A_jon1.shape))\n",
    "print('Size of computed rho_A: {}'.format(rho_A_jon1.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check Kevin's implementation and mine.\n",
    "assert np.allclose(rho_A_ten1, rho_A_kev1)\n",
    "assert np.allclose(rho_A_ten2, rho_A_kev2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "rho_A_ten1 = partial_trace_tensor(L, A_sites, B1_sites, B2_sites, V1[:,0])\n",
    "rho_A_ten2 = partial_trace_tensor(L, A_sites, B1_sites, B2_sites, V2[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "rho_A_kev1 = partial_trace_kevin(L, A_sites, B1_sites, B2_sites, V1[:,0])\n",
    "rho_A_kev2 = partial_trace_kevin(L, A_sites, B1_sites, B2_sites, V2[:,0])\n",
    "# Kevin is faster by 10-20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(fig_w/dpi,fig_h/dpi*2), dpi=dpi, squeeze=False)\n",
    "\n",
    "im1 = axes[0, 0].imshow(np.abs(rho_A_mat1))\n",
    "im2 = axes[0, 1].imshow(np.abs(rho_A_ten1))\n",
    "im3 = axes[1, 0].imshow(np.abs(rho_A_mat2))\n",
    "im4 = axes[1, 1].imshow(np.abs(rho_A_ten2))\n",
    "\n",
    "axes[0, 0].set_title('$W=0.5 \\quad \\\\rho_A$ computed using matrix (seems wrong)')\n",
    "axes[0, 1].set_title('$W=0.5 \\quad \\\\rho_A$ computed using tensor')\n",
    "axes[1, 0].set_title('$W=8.0 \\quad \\\\rho_A$ computed using matrix (almost correct)')\n",
    "axes[1, 1].set_title('$W=8.0 \\quad \\\\rho_A$ computed using tensor')\n",
    "\n",
    "# plt.colorbar(im1, ax=axes[0, 0])#.set_label('Entropy')\n",
    "# plt.colorbar(im2, ax=axes[0, 1])#.set_label('Entropy')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(fig_w/dpi,fig_h/dpi*2), dpi=dpi, squeeze=False)\n",
    "\n",
    "im1 = axes[0, 0].imshow(np.abs(rho_A_kev1))\n",
    "im2 = axes[0, 1].imshow(np.abs(rho_A_ten1))\n",
    "im3 = axes[1, 0].imshow(np.abs(rho_A_kev2))\n",
    "im4 = axes[1, 1].imshow(np.abs(rho_A_ten2))\n",
    "\n",
    "axes[0, 0].set_title('$W=0.5 \\quad \\\\rho_A$ computed using Kevin')\n",
    "axes[0, 1].set_title('$W=0.5 \\quad \\\\rho_A$ computed using tensor')\n",
    "axes[1, 0].set_title('$W=8.0 \\quad \\\\rho_A$ computed using Kevin')\n",
    "axes[1, 1].set_title('$W=8.0 \\quad \\\\rho_A$ computed using tensor')\n",
    "\n",
    "# plt.colorbar(im1, ax=axes[0, 0])#.set_label('Entropy')\n",
    "# plt.colorbar(im2, ax=axes[0, 1])#.set_label('Entropy')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(fig_w/dpi,fig_h/dpi*2), dpi=dpi, squeeze=False)\n",
    "\n",
    "im1 = axes[0, 0].imshow(np.abs(rho_A_jon1))\n",
    "im2 = axes[0, 1].imshow(np.abs(rho_A_ten1))\n",
    "im3 = axes[1, 0].imshow(np.abs(rho_A_jon2))\n",
    "im4 = axes[1, 1].imshow(np.abs(rho_A_ten2))\n",
    "\n",
    "axes[0, 0].set_title('$W=0.5 \\quad \\\\rho_A$ computed using Jonas')\n",
    "axes[0, 1].set_title('$W=0.5 \\quad \\\\rho_A$ computed using tensor')\n",
    "axes[1, 0].set_title('$W=8.0 \\quad \\\\rho_A$ computed using Jonas')\n",
    "axes[1, 1].set_title('$W=8.0 \\quad \\\\rho_A$ computed using tensor')\n",
    "\n",
    "# plt.colorbar(im1, ax=axes[0, 0])#.set_label('Entropy')\n",
    "# plt.colorbar(im2, ax=axes[0, 1])#.set_label('Entropy')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Visualize reduced density matrices for different W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num = 4\n",
    "num_plots = num * num # Number of rho_A to display.\n",
    "base_samples = num_plots // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test drawing images of reduced density matrix.\n",
    "J  = 1                      # Always = 1\n",
    "Ls = [10]                   # System size L.\n",
    "ps = [False]                # Periodic or not.\n",
    "num_EV = 1                  # Number of eigenvalues near zero to save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Training data.\n",
    "Ws_main = [0.5, 8]          # Disorder strength W.\n",
    "Hs = [base_samples]         # Number of samples per L per W.\n",
    "et = []                     # Execution time.\n",
    "rho_As_dict = {}\n",
    "\n",
    "for L, num_Hs in zip(Ls, Hs):\n",
    "    for p in ps:\n",
    "        start_time = time.time()\n",
    "        rho_As = batch_gen_rho_data_main(L, Ws_main, J, p, num_Hs, num_EV, max_n=10, save_data=False)\n",
    "        for n, rho_A in rho_As.items():\n",
    "            if n not in rho_As_dict:\n",
    "                rho_As_dict[n] = []\n",
    "            rho_As_dict[n] = rho_As_dict[n] + rho_A\n",
    "        exec_time = time.time() - start_time\n",
    "        et.append(exec_time)\n",
    "        print('Computed: L={:02d} | periodic={: <5}.'.format(L, str(p)))\n",
    "        print('Execution took {: 8.2f}s or {: 6.2f}min.'.format(exec_time, exec_time/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Random data.\n",
    "Ws_rand = np.random.uniform(0.1, high=9.9, size=(2 * base_samples,)) # Disorder strength W.\n",
    "Hs = [1]                    # Number of samples per L per W.\n",
    "et = []                     # Execution time.\n",
    "rho_As_rand = {}\n",
    "\n",
    "for L, num_Hs in zip(Ls, Hs):\n",
    "    for p in ps:\n",
    "        start_time = time.time()\n",
    "        rho_As = batch_gen_rho_data_rand(L, Ws_rand, J, p, num_Hs, num_EV, max_n=10, save_data=False)\n",
    "        for n, rho_A in rho_As.items():\n",
    "            if n not in rho_As_rand:\n",
    "                rho_As_rand[n] = []\n",
    "            rho_As_rand[n] = rho_As_rand[n] + rho_A\n",
    "        exec_time = time.time() - start_time\n",
    "        et.append(exec_time)\n",
    "        print('Computed: L={:02d} | periodic={: <5}.'.format(L, str(p)))\n",
    "        print('Execution took {: 8.2f}s or {: 6.2f}min.'.format(exec_time, exec_time/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n = 6\n",
    "# sample_idx = np.random.randint(0, len(rho_As_dict[n]), size=num*num)\n",
    "sample_idx = np.arange(num*num)\n",
    "\n",
    "fig, axes = plt.subplots(num, num, figsize=(fig_w/dpi,fig_h/dpi*2), dpi=dpi, squeeze=False)\n",
    "fig.suptitle('Visualize training $\\\\rho_A$ ($W=0.5,W=8.0$)', fontsize=16)\n",
    "\n",
    "for i, idx in enumerate(sample_idx):\n",
    "    axes[i%num,i//num].imshow(np.abs(rho_As_dict[n][idx][0]))\n",
    "    axes[i%num,i//num].annotate('W={:3.1f}'.format(rho_As_dict[n][idx][1]), (0.5,0.5), xycoords='axes fraction', ha='center', color='w', fontsize=14)\n",
    "\n",
    "for axe in axes:\n",
    "    for ax in axe:\n",
    "        # ax.legend(loc='best')\n",
    "        ax.xaxis.set_ticklabels([])\n",
    "        ax.yaxis.set_ticklabels([])\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n = 6\n",
    "# sample_idx = np.random.randint(0, len(rho_As_rand[n]), size=num*num)\n",
    "sample_idx = np.arange(num*num)\n",
    "\n",
    "fig, axes = plt.subplots(num, num, figsize=(fig_w/dpi,fig_h/dpi*2), dpi=dpi, squeeze=False)\n",
    "fig.suptitle('Visualize random $\\\\rho_A$ ($W \\in [0.1,9.9]$)', fontsize=16)\n",
    "\n",
    "for i, idx in enumerate(sample_idx):\n",
    "    axes[i%num,i//num].imshow(np.abs(rho_As_rand[n][idx][0]))\n",
    "    axes[i%num,i//num].annotate('W={:3.1f}'.format(rho_As_rand[n][idx][1]), (0.5,0.5), xycoords='axes fraction', ha='center', color='w', fontsize=14)\n",
    "\n",
    "for axe in axes:\n",
    "    for ax in axe:\n",
    "        # ax.legend(loc='best')\n",
    "        ax.xaxis.set_ticklabels([])\n",
    "        ax.yaxis.set_ticklabels([])\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch generate data (execution part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Estimated runtime to generate 10,000 samples per W:')\n",
    "for i in range(7):\n",
    "    print('For L = {:2d}, est runtime {:3d} min or {: 2.2f} hrs.'.format(8+i, 7 * 2**i, 7 * 2**i / 60))\n",
    "print(' ')\n",
    "\n",
    "print('Estimated runtime to generate 100,000 samples per W:')\n",
    "for i in range(7):\n",
    "    print('For L = {:2d}, est runtime {:4d} min or {:5.2f} hrs.'.format(8+i, 70 * 2**i, 70 * 2**i / 60))\n",
    "print(' ')\n",
    "\n",
    "print('It will therefore be safer to break down generation to segments of 1-hr:')\n",
    "for i in range(7):\n",
    "    print('For L = {:2d}, separate execution into {:2d} batches with {:6d} samples each.'.format(8+i, 2**i, 100000 // 2**i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Batch generate reduced density matrix.\n",
    "\n",
    "k = 5\n",
    "batches = 10\n",
    "batch_resume = 1\n",
    "base_sample = 10000 // k // batches # Divide by k (num_EV)\n",
    "rand_sample = 100           # Samples per random W.\n",
    "Ws_main = [0.5, 8]          # Disorder strength W.\n",
    "Ws_rand = np.random.uniform(0.1, high=7.9, size=(2 * base_sample // rand_sample,))\n",
    "J  = 1                      # Always = 1\n",
    "Ls = list(range(8,13,1))    # System sizes L.\n",
    "ps = [False, True]          # Periodic or not.\n",
    "et = []                     # Execution time.\n",
    "Hs_main = [base_sample]*len(Ls) # Number of samples per L per W.\n",
    "Hs_rand = [rand_sample]*len(Ls) # Number of samples per L per W.\n",
    "num_EVs = [k]               # Number of eigenvalues near zero to save.\n",
    "\n",
    "for i in range(batches):\n",
    "\n",
    "    if i < batch_resume:\n",
    "        tqdm.write('{} | Processing batch {:03d} of {:d}:'.format(dt(), i+1, batches)) #, flush=True)\n",
    "        tqdm.write('{} | Batch {:03d} skipped.'.format(dt(), i+1, batches)) #, flush=True)\n",
    "        tqdm.write(' ') #, flush=True)\n",
    "        continue\n",
    "\n",
    "    tqdm.write('='*60) #, flush=True)\n",
    "    tqdm.write('{} | Processing batch {:03d} of {:d}:'.format(dt(), i+1, batches)) #, flush=True)\n",
    "    tqdm.write(' ') #, flush=True)\n",
    "    for L, num_Hs_m, num_Hs_r in zip(Ls, Hs_main, Hs_rand):\n",
    "\n",
    "        tqdm.write('='*40) #, flush=True)\n",
    "        for num_EV in num_EVs:\n",
    "            for p in ps:\n",
    "                start_time = time.time()\n",
    "\n",
    "                tqdm.write('{} | Generating training data for L={:02d} | num_EV={} | periodic={: <5}...'.format(dt(), L, num_EV, str(p))) #, flush=True)\n",
    "                batch_gen_rho_data_main(L, Ws_main, J, p, num_Hs_m, num_EV, max_n=6, clamp_zero=1e-32, save_data=True)\n",
    "                tqdm.write('{} | Generating random data for L={:02d} | num_EV={} | periodic={: <5}...'.format(dt(), L, num_EV, str(p))) #, flush=True)\n",
    "                batch_gen_rho_data_rand(L, Ws_rand, J, p, num_Hs_r, num_EV, max_n=6, clamp_zero=1e-32, save_data=True)\n",
    "\n",
    "                exec_time = time.time() - start_time\n",
    "                et.append(exec_time)\n",
    "                tqdm.write('{} | Computed: L={:02d} | num_EV={} | periodic={: <5}.'.format(dt(), L, num_EV, str(p))) #, flush=True)\n",
    "                tqdm.write('{} | Execution took {: 8.2f}s or {: 6.2f}min.'.format(dt(), exec_time, exec_time/60)) #, flush=True)\n",
    "                tqdm.write(' ') #, flush=True)\n",
    "\n",
    "        tqdm.write('='*40) #, flush=True)\n",
    "        tqdm.write(' ') #, flush=True)\n",
    "\n",
    "    tqdm.write('{} | Batch {:03d} of {:d} completed.'.format(dt(), i+1, batches)) #, flush=True)\n",
    "    tqdm.write('='*60) #, flush=True)\n",
    "    tqdm.write(' ') #, flush=True)\n",
    "\n",
    "    if check_shutdown_signal():\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit",
   "language": "python",
   "name": "python37264bit22d8f94fb4124d8cb7bc86dc616da5cb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
